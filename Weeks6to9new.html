<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Weeks6to9new.knit</title>

<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>



<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div id="header">




</div>


<div id="correlation" class="section level1" number="1">
<h1><span class="header-section-number">1</span> Correlation</h1>
<div id="correlation-and-causation" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Correlation and Causation</h2>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="figures/cellphonesxkcd.png" alt="https://xkcd.com/925/" width="60%" height="60%" />
<p class="caption">
Figure 1.1: <a href="https://xkcd.com/925/" class="uri">https://xkcd.com/925/</a>
</p>
</div>
<p><img src="figures/corr1.png" width="40%" height="40%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="simple-regression-introduction" class="section level1" number="2">
<h1><span class="header-section-number">2</span> Simple regression: Introduction</h1>
<div id="motivation" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Motivation</h2>
<p><strong>Predicting the Price of a used car</strong></p>
<p><img src="figures/motivation1.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/motivation2.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="simple-linear-regression" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Simple linear regression</h2>
<p><strong>Simple linear regression (population)</strong>
<span class="math display">\[Y=\beta_0+\beta_1 x+\epsilon\]</span>
In our example:
<span class="math display">\[Price=\beta_0+\beta_1 Age+\epsilon\]</span></p>
<p><strong>Simple linear regression (sample)</strong>
<span class="math display">\[\hat{y}=b_0+b_1 x\]</span>
where the coefficient <span class="math inline">\(\beta_0\)</span> (and its estimate <span class="math inline">\(b_0\)</span> or <span class="math inline">\(\hat{\beta}_0\)</span> ) refers to the <span class="math inline">\(y\)</span>-intercept or simply the intercept or the constant of the regression line, and the coefficient <span class="math inline">\(\beta_1\)</span> (and its estimate <span class="math inline">\(b_1\)</span> or <span class="math inline">\(\hat{\beta}_1\)</span> ) refers to the slope of the regression line.</p>
</div>
<div id="least-squares-criterion" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Least-Squares criterion</h2>
<ul>
<li><p>The <strong>least-squares criterion</strong> is that the line that best fits a set of data points is the one having the smallest possible sum of squared errors. The `errors’ are the vertical distances of the data points to the line.</p></li>
<li><p>We need to use the data to estimate the values of the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, i.e. to fit a straight line to the set of points <span class="math inline">\(\{(x_i , y_i )\}\)</span>.
There are many straight lines we could use, so we need some idea of which is best.
Clearly, a bad straight line model would be one that had many large errors, and conversely, a good straight line model will have, on average, small errors.
We quantify this by the sum of squares of the errors:
<span class="math display">\[Q(\beta_0,\beta_1)=\sum_{i=1}^n \epsilon_i^2=\sum_{i=1}^n[y_i-(\beta_0 + \beta_1 x_i)]^2\]</span>
then the “line of best fit” will correspond to the line with values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimises <span class="math inline">\(Q(\beta_0,\beta_1)\)</span>.</p></li>
<li><p>The regression line is the line that fits a set of data points according to the least squares criterion.</p></li>
<li><p>The regression equation is the equation of the regression line.</p></li>
<li><p>The regression equation for a set of <span class="math inline">\(n\)</span> data points is
<span class="math inline">\(\hat{y}=b_0+b_1\;x\)</span>, where
<span class="math display">\[b_1=\frac{S_{xy}}{S_{xx}}=\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i-\bar{x})^2}
\;\;\text{and}\;\; b_0=\bar{y}-b_1\; \bar{x}\]</span></p></li>
<li><p><span class="math inline">\(y\)</span> is the dependent variable (or response variable) and <span class="math inline">\(x\)</span> is the independent variable (predictor variable or explanatory variable).</p></li>
<li><p><span class="math inline">\(b_0\)</span> is called the <strong>y-intercept</strong> and <span class="math inline">\(b_1\)</span> is called the <strong>slope</strong>.</p></li>
</ul>
<p><img src="figures/leastsq2.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><strong>SSE and the standard error</strong></p>
<p>This least square regression line minimizes the error sum of squares
<span class="math display">\[SSE=\sum e^2_i =\sum (y_i-\hat{y}_i)^2\]</span>
The standard error of the estimate,
<span class="math inline">\(s_e=\sqrt{SSE/(n-2)}\)</span>, indicates how much, on average, the observed values of the response variable differ from the predicated values of the response variable.</p>
<p><img src="figures/leastsq1.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-used-cars-cont." class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Example: used cars (cont.)</h2>
<p>The table below displays data on Age (in years) and Price (in hundreds of dollars) for a sample of cars of a particular make and model.(Weiss, 2012)</p>
<table>
<thead>
<tr class="header">
<th align="center">Price (<span class="math inline">\(y\)</span>)</th>
<th align="center">Age (<span class="math inline">\(x\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">85</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">103</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td align="center">70</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">82</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">89</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">98</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">66</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">95</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td align="center">169</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">70</td>
<td align="center">7</td>
</tr>
<tr class="odd">
<td align="center">48</td>
<td align="center">7</td>
</tr>
</tbody>
</table>
<ul>
<li><p>For our example, <em>age</em> is the predictor variable and <em>price</em> is the response variable.</p></li>
<li><p>The regression equation is <span class="math inline">\(\hat{y}=195.47-20.26\;x\)</span>, where the slope <span class="math inline">\(b_1=-20.26\)</span> and the intercept <span class="math inline">\(b_0=195.47\)</span></p></li>
<li><p>Prediction: for <span class="math inline">\(x = 4\)</span>, that is we would like to predict the price of a
4-year-old car,
<span class="math display">\[\hat{y}=195.47-20.26 {\color{blue}(4)}= 114.43 \;\;\text{or}\;\; \$ 11443\]</span></p></li>
</ul>
<p><img src="figures/leastsq5.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p>Back to our used cars example, we want to find the “best line”
through the data points, which can be used to predict prices of used cars based on their age.</p>
<p>First we need to enter the data in R.</p>
<pre class="r"><code>Price&lt;-c(85, 103,  70,  82,  89,  98,  66,  95, 169,  70,  48)
Age&lt;- c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7)
carSales&lt;-data.frame(Price,Age)
str(carSales)</code></pre>
<pre><code>## &#39;data.frame&#39;:    11 obs. of  2 variables:
##  $ Price: num  85 103 70 82 89 98 66 95 169 70 ...
##  $ Age  : num  5 4 6 5 5 5 6 6 2 7 ...</code></pre>
<pre class="r"><code>cor(Age, Price, method = &quot;pearson&quot;)</code></pre>
<pre><code>## [1] -0.9237821</code></pre>
<p>Scatterplot: Age vs. Price</p>
<pre class="r"><code>library(ggplot2)</code></pre>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 4.3.2</code></pre>
<pre class="r"><code>ggplot(carSales, aes(x=Age, y=Price)) + geom_point()</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-9-1.png" width="50%" height="50%" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Remove the confidence interval
ggplot(carSales, aes(x=Age, y=Price)) + 
  geom_point()+
  geom_smooth(method=lm, formula= y~x, se=FALSE)</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-10-1.png" width="50%" height="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="prediction" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Prediction</h2>
<pre class="r"><code># simple linear regression
reg&lt;-lm(Price~Age)
print(reg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Price ~ Age)
## 
## Coefficients:
## (Intercept)          Age  
##      195.47       -20.26</code></pre>
<p>To predict the price of a 4-year-old car (<span class="math inline">\(x=4\)</span>):
<span class="math display">\[\hat{y}=195.47-20.26(4)=114.43\]</span></p>
<p><img src="figures/leastsq3.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="simple-regression-coefficient-of-determination" class="section level1" number="3">
<h1><span class="header-section-number">3</span> Simple Regression: Coefficient of Determination</h1>
<div id="extrapolation" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Extrapolation</h2>
<ul>
<li><p>Within the range of the observed values of the predictor variable, we can reasonably use the regression equation to make predictions for the response variable.</p></li>
<li><p>However, to do so outside the range, which is called <strong>Extrapolation</strong>, may not be reasonable because the linear relationship between the predictor and response variables may not hold here.</p></li>
<li><p>To predict the price of an 11-year old car, <span class="math inline">\(\hat{y}=195.47-20.26 (11)=-27.39\)</span>
or $ 2739, this result is unrealistic as no one is going to pay us $2739 to take away their 11-year old car.</p></li>
</ul>
<p><img src="figures/extrap.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="outliers-and-influential-observations" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Outliers and influential observations</h2>
<ul>
<li><p>Recall that an <strong>outlier</strong> is an observation that lies outside the overall pattern of the data. In the context of regression, an outlier is a data point that lies far from the regression line, relative to the other data points.</p></li>
<li><p>An <strong>influential observation</strong> is a data point whose removal causes the regression equation (and line) to change considerably.</p></li>
<li><p>From the scatterplot, it seems that the data point (2,169) might be an influential observation.
Removing that data point and recalculating the regression equation yields <span class="math inline">\(\hat{y}=160.33-14.24\;x\)</span>.</p></li>
</ul>
<p><img src="figures/outliersreg.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="coefficient-of-determination" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Coefficient of determination</h2>
<p><img src="figures/cod1.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/cod2.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/cod3.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/cod4.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/cod5.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<ul>
<li><p>The total variation in the observed values of the response variable, <span class="math inline">\(SST=\sum(y_i-\bar{y})^2\)</span>, can be partitioned into two components:</p>
<ul>
<li>The variation in the observed values of the response variable explained by the regression: <span class="math inline">\(SSR=\sum(\hat{y}_i-\bar{y})^2\)</span></li>
<li>The variation in the observed values of the response variable not explained by the regression: <span class="math inline">\(SSE=\sum(y_i-\hat{y}_i)^2\)</span></li>
</ul></li>
<li><p>The coefficient of determination, <span class="math inline">\(R^2\)</span> (or <span class="math inline">\(R\)</span>-square), is the proportion of the variation in the observed values of the response variable explained by the regression, which is given by
<span class="math display">\[R^2=\frac{SSR}{SST}=\frac{SST-SSE}{SST}=1-\frac{SSE}{SST}\]</span>
where <span class="math inline">\(SST=SSR+SSE\)</span>. <span class="math inline">\(R^2\)</span> is a descriptive measure of the utility of the regression equation for making prediction.</p></li>
<li><p>The coefficient of determination <span class="math inline">\(R^2\)</span> always lies between 0 and 1. A value of <span class="math inline">\(R^2\)</span> near 0 suggests that the regression equation is not very useful for making predictions, whereas a value of <span class="math inline">\(R^2\)</span> near 1 suggests that the regression equation is quite useful for making predictions.</p></li>
<li><p>For a simple linear regression (one independent variable) ONLY, <span class="math inline">\(R^2\)</span> is the square of Pearson correlation coefficient, <span class="math inline">\(r\)</span>.</p></li>
<li><p><span class="math inline">\(\text{Adjusted}\;R^2\)</span> is a modification of <span class="math inline">\(R^2\)</span> which takes into account the number of independent variables, say <span class="math inline">\(k\)</span>. In a simple linear regression <span class="math inline">\(k=1\)</span>. Adjusted-<span class="math inline">\(R^2\)</span> increases only when a significant related independent variable is added to the model. Adjusted-<span class="math inline">\(R^2\)</span> has a crucial role in the process of model building. Adjusted-<span class="math inline">\(R^2\)</span> is given by
<span class="math display">\[\text{Adjusted-}R^2=1-(1-R^2)\frac{n-1}{n-k-1}\]</span></p></li>
</ul>
</div>
<div id="notation-used-in-regression" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Notation used in regression</h2>
<table>
<thead>
<tr class="header">
<th align="center">Quantity</th>
<th align="center">Defining formula</th>
<th align="left">Computing formula</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(S_{xx}\)</span></td>
<td align="center"><span class="math inline">\(\sum (x_i-\bar{x})^2\)</span></td>
<td align="left"><span class="math inline">\(\sum x^2_i - n \bar{x}^2\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(S_{xy}\)</span></td>
<td align="center"><span class="math inline">\(\sum (x_i-\bar{x})(y_i-\bar{y})\)</span></td>
<td align="left"><span class="math inline">\(\sum x_i y_i - n \bar{x}\bar{y}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(S_{yy}\)</span></td>
<td align="center"><span class="math inline">\(\sum (y_i-\bar{y})^2\)</span></td>
<td align="left"><span class="math inline">\(\sum y^2_i - n \bar{y}^2\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(\bar{x}=\frac{\sum x_i}{n}\)</span> and <span class="math inline">\(\bar{y}=\frac{\sum y_i}{n}\)</span>. And,</p>
<p><span class="math display">\[SST=S_{yy},\;\;\; SSR=\frac{S^2_{xy}}{S_{xx}},\;\;\; SSE=S_{yy}-\frac{S^2_{xy}}{S_{xx}} \]</span>
and <span class="math inline">\(SST=SSR+SSE\)</span>.</p>
</div>
<div id="pearson-correlation-coefficient" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Pearson correlation coefficient</h2>
<p><strong>Pearson correlation coefficient</strong> (<span class="math inline">\(r\)</span>) is a measure of the strength and the direction of <strong>a linear relationship</strong> between two variables in the sample,</p>
<p><span class="math display">\[r=\frac{\sum(x_{i} -\bar{x})(y_{i} -\bar{y}) }{\sqrt{\sum (x_{i} -\bar{x})^{2}  \sum (y_{i} -\bar{y})^{2}  } } \]</span></p>
<p>where <span class="math inline">\(r\)</span> always lies between -1 and 1. Values of <span class="math inline">\(r\)</span> near -1 or 1 indicate a strong linear relationship between the variables whereas values of <span class="math inline">\(r\)</span> near 0 indicate a weak linear relationship between variables. If <span class="math inline">\(r\)</span> is zero the variables are linearly uncorrelated, that is there is no linear relationship between the two variables.</p>
<p><img src="figures/rr1.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/rr2.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/corr0.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="hypothesis-testing-for-the-population-correlation-coefficient-rho" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Hypothesis testing for the population correlation coefficient <span class="math inline">\(\rho\)</span></h2>
<p>Hypothesis testing for the population correlation coefficient <span class="math inline">\(\rho\)</span>.</p>
<p>Assumptions:</p>
<ul>
<li>The sample of paired <span class="math inline">\((x, y)\)</span> data is a random sample.</li>
<li>The pairs of <span class="math inline">\((x, y)\)</span> data have a bivariate normal distribution.</li>
</ul>
<p>The null hypothesis</p>
<p><span class="math inline">\(H_0: \rho = 0\)</span> (no significant correlation)</p>
<p>against one of the alternative hypotheses:</p>
<ul>
<li><p><span class="math inline">\(H_1: \rho \neq 0\)</span> (significant correlation) ``Two-tailed test’’</p></li>
<li><p><span class="math inline">\(H_1: \rho &lt; 0\)</span> (significant negative correlation) ``Left-tailed test’’</p></li>
<li><p><span class="math inline">\(H_1: \rho &gt; 0\)</span> (significant positive correlation) ``Right-tailed test’’</p></li>
</ul>
<p>Compute the value of the test statistic:
<span class="math display">\[t=\frac{r\; \sqrt{n-2} }{\sqrt{1-r^{2} } }\sim T_{(n-2)} \;\; \text{with}\;\; df = n-2. \]</span></p>
<p>where <span class="math inline">\(n\)</span> is the sample size.</p>
<p>The critical value(s) for this test can be found from T distribution table ( <span class="math inline">\(\pm t_{\alpha/2}\)</span> for a two-tailed test, <span class="math inline">\(- t_{\alpha}\)</span> for a left-tailed test and <span class="math inline">\(t_{\alpha}\)</span> for a right-tailed test).</p>
<p><img src="figures/hypottest.PNG" width="80%" height="80%" style="display: block; margin: auto;" /></p>
<ul>
<li>If the value of the test statistic falls in the rejection region, then reject <span class="math inline">\(H_0\)</span>; otherwise, do not reject <span class="math inline">\(H_0\)</span>.</li>
<li>Statistical packages report <strong>p-values</strong> rather than critical values which can be used in testing the null hypothesis <span class="math inline">\(H_0\)</span>.</li>
</ul>
</div>
<div id="correlation-and-linear-transformation" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Correlation and linear transformation</h2>
<ul>
<li><p>Suppose we have a linear transformation of the two variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, say <span class="math inline">\(x_1=a x+b\)</span> and <span class="math inline">\(y_1=cy+d\)</span> where <span class="math inline">\(a&gt;0\)</span> and <span class="math inline">\(c&gt;0\)</span>. Then the Pearson correlation coefficient between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(y_1\)</span> is equal to Pearson correlation coefficient between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p></li>
<li><p>For our example, suppose we convert cars’ prices from dollars to pounds (say <span class="math inline">\(\$1=\)</span> , so <span class="math inline">\(y_1=0.75 y\)</span>), and we left the age of the cars unchanged. Then we will find that the correlation between the age of the car and its price in pounds is equal to the one we obtained before (i.e. the correlation between the age and the price in dollars).</p></li>
<li><p>A special linear transformation is to standardize one or both variables. That is obtaining the values <span class="math inline">\(z_x=(x-\bar{x})/s_x\)</span> and <span class="math inline">\(z_y=(y-\bar{y})/s_y\)</span>. Then the correlation between <span class="math inline">\(z_x\)</span> and <span class="math inline">\(z_y\)</span> is equal to the correlation between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>.</p></li>
</ul>
</div>
<div id="spearmans-rho-correlation-coefficient-r_s" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Spearman’s rho correlation coefficient (<span class="math inline">\(r_s\)</span>)</h2>
<ul>
<li><p>When the normality assumption for the Pearson correlation coefficient <span class="math inline">\(r\)</span> cannot be met, or when one or both variables may be ordinal, then we should consider nonparametric methods such as Spearman’s rho and Kendall’s tau correlation coefficients.</p></li>
<li><p>Spearman’s rho correlation coefficient, <span class="math inline">\(r_s\)</span>,can be obtained by first rank the <span class="math inline">\(x\)</span> values (and <span class="math inline">\(y\)</span> values) among themselves, and then we compute the Pearson correlation coefficient of the rank pairs. Similarly <span class="math inline">\(-1\leq r_s \leq 1\)</span>, the values of <span class="math inline">\(r_s\)</span> range from -1 to +1 inclusive.</p></li>
<li><p>Spearman’s rho correlation coefficient can be used to describe the strength of the linear relationship as well as the nonlinear relationship.</p></li>
</ul>
</div>
<div id="kendalls-tau-tau-correlation-coefficient" class="section level2" number="3.9">
<h2><span class="header-section-number">3.9</span> Kendall’s tau (<span class="math inline">\(\tau\)</span>) correlation coefficient</h2>
<ul>
<li><p>Kendall’s tau, <span class="math inline">\(\tau\)</span>, measures the concordance of the relationship between two variables, and <span class="math inline">\(-1\leq \tau \leq 1\)</span>.</p></li>
<li><p>Any pair of observations <span class="math inline">\((x_i, y_i)\)</span> and <span class="math inline">\((x_j, y_j)\)</span> are said to be concordant if both <span class="math inline">\(x_i &gt; x_j\)</span> and <span class="math inline">\(y_i &gt; y_j\)</span> or if both <span class="math inline">\(x_i &lt; x_j\)</span> and <span class="math inline">\(y_i &lt; y_j\)</span>. And they are said to be discordant, if <span class="math inline">\(x_i &gt; x_j\)</span> and <span class="math inline">\(y_i &lt; y_j\)</span> or if <span class="math inline">\(x_i &lt; x_j\)</span> and <span class="math inline">\(y_i &gt; y_j\)</span>. We will have <span class="math inline">\(n(n-1)/2\)</span> of pairs to compare.</p></li>
<li><p>The Kendall’s tau (<span class="math inline">\(\tau\)</span>) correlation coefficient is defined as:
<span class="math display">\[\tau=\frac{\text{number of concordant pairs} - \text{number of discordant pairs}}{n(n-1)/2}\]</span></p></li>
</ul>
</div>
<div id="example-used-cars" class="section level2" number="3.10">
<h2><span class="header-section-number">3.10</span> Example: used cars</h2>
<p>The table below displays data on Age (in years) and Price (in hundreds of dollars) for a sample of cars of a particular make and model (Weiss, 2012).</p>
<table>
<thead>
<tr class="header">
<th align="center">Price (<span class="math inline">\(y\)</span>)</th>
<th align="center">Age (<span class="math inline">\(x\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">85</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">103</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td align="center">70</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">82</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">89</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">98</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">66</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">95</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td align="center">169</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">70</td>
<td align="center">7</td>
</tr>
<tr class="odd">
<td align="center">48</td>
<td align="center">7</td>
</tr>
</tbody>
</table>
<ul>
<li><p>The Pearson correlation coefficient,
<span class="math display">\[r=\frac{\sum x_i y_i - (\sum x_i) (\sum y_i)/n }{\sqrt{ [\sum x^2_{i} -(\sum x_i)^2/n] [\sum y^2_{i} -(\sum y_i)^2/n] } } \]</span>
<span class="math display">\[r=\frac{4732-(58)(975)/11}{\sqrt{(326-58^2/11)( 96129-975^2/11)}}=-0.924\]</span>
the value of <span class="math inline">\(r=-0.924\)</span> suggests a strong negative linear correlation between age and price.</p></li>
<li><p>Test the hypothesis <span class="math inline">\(H_0: \rho = 0\)</span> (no linear correlation) against <span class="math inline">\(H_1: \rho &lt; 0\)</span> (negative correlation) at significant level <span class="math inline">\(\alpha=0.05\)</span>.</p></li>
</ul>
<p>Compute the value of the test statistic:
<span class="math display">\[t=\frac{r\; \sqrt{n-2} }{\sqrt{1-r^{2} } }=\frac{-0.924\sqrt{11-2}}{\sqrt{1-(-0.924)^2}}=-7.249  \]</span></p>
<p>Since <span class="math inline">\(t=-7.249&lt;-1.833\)</span>, reject <span class="math inline">\(H_0\)</span>.</p>
<p><img src="figures/correx1.PNG" width="35%" height="35%" style="display: block; margin: auto;" /></p>
<p><strong>Using R:</strong></p>
<p>First we need to enter the data in R.</p>
<pre class="r"><code>Price&lt;-c(85, 103,  70,  82,  89,  98,  66,  95, 169,  70,  48)
Age&lt;- c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7)
carSales&lt;-data.frame(Price,Age)
str(carSales)</code></pre>
<pre><code>## &#39;data.frame&#39;:    11 obs. of  2 variables:
##  $ Price: num  85 103 70 82 89 98 66 95 169 70 ...
##  $ Age  : num  5 4 6 5 5 5 6 6 2 7 ...</code></pre>
<p>Now let us plot <code>age</code> against <code>price</code>, i.e. a scatterplot.</p>
<pre class="r"><code>plot(Price ~ Age, pch=16, col=2)</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-26-1.png" width="50%" height="50%" style="display: block; margin: auto;" /></p>
<p>or we can use ggplot2 for a much nicer plot.</p>
<pre class="r"><code>library(ggplot2)
# Basic scatter plot
ggplot(carSales, aes(x=Age, y=Price)) + geom_point()</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-27-1.png" width="50%" height="50%" style="display: block; margin: auto;" /></p>
<p>From this plot it seems that there is a negative linear relationship between age and price. There are several tools that can help us to measure this relationship more precisely.</p>
<pre class="r"><code>cor.test(Age, Price,
         alternative = &quot;less&quot;,
         method = &quot;pearson&quot;, conf.level = 0.95)</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  Age and Price
## t = -7.2374, df = 9, p-value = 2.441e-05
## alternative hypothesis: true correlation is less than 0
## 95 percent confidence interval:
##  -1.0000000 -0.7749819
## sample estimates:
##        cor 
## -0.9237821</code></pre>
<p>Suppose now we scale both variables (standardized)</p>
<pre class="r"><code>cor.test(scale(Age), scale(Price),
         alternative = &quot;less&quot;,
         method = &quot;pearson&quot;, conf.level = 0.95)</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  scale(Age) and scale(Price)
## t = -7.2374, df = 9, p-value = 2.441e-05
## alternative hypothesis: true correlation is less than 0
## 95 percent confidence interval:
##  -1.0000000 -0.7749819
## sample estimates:
##        cor 
## -0.9237821</code></pre>
<p>We notice that corr(age, price in pounds) <span class="math inline">\(=\)</span> corr(age, price in dollars).</p>
<p><span class="math inline">\(~\)</span></p>
<p>We can also obtain Spearman’s rho and Kendall’s tau as follows.</p>
<pre class="r"><code>cor.test(Age, Price,
         alternative = &quot;less&quot;,
         method = &quot;spearman&quot;, conf.level = 0.95)</code></pre>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  Age and Price
## S = 403.26, p-value = 0.0007267
## alternative hypothesis: true rho is less than 0
## sample estimates:
##        rho 
## -0.8330014</code></pre>
<pre class="r"><code>cor.test(Age, Price,
         alternative = &quot;less&quot;,
         method = &quot;kendall&quot;, conf.level = 0.95)</code></pre>
<pre><code>## 
##  Kendall&#39;s rank correlation tau
## 
## data:  Age and Price
## z = -2.9311, p-value = 0.001689
## alternative hypothesis: true tau is less than 0
## sample estimates:
##        tau 
## -0.7302967</code></pre>
<p><span class="math inline">\(~\)</span></p>
<p>As the p-values for all three tests (Pearson, Spearman, Kendall) less than <span class="math inline">\(\alpha=0.05\)</span>, we reject the null hypothesis of no correlation between the age and the price, at the 5% significance level.</p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Now what do you think about correlation and causation?</strong></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-31"></span>
<img src="figures/correlationxkcd.png" alt="https://xkcd.com/552/" width="60%" height="60%" />
<p class="caption">
Figure 3.1: <a href="https://xkcd.com/552/" class="uri">https://xkcd.com/552/</a>
</p>
</div>
<!--
https://xkcd.com/925/

https://xkcd.com/552/

-->
</div>
</div>
<div id="simple-regression-introduction-1" class="section level1" number="4">
<h1><span class="header-section-number">4</span> Simple regression: Introduction</h1>
<div id="motivation-1" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> Motivation</h2>
<p><strong>Predicting the Price of a used car</strong></p>
<p><img src="figures/motivation1.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/motivation2.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="simple-linear-regression-1" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> Simple linear regression</h2>
<p><strong>Simple linear regression (population)</strong>
<span class="math display">\[Y=\beta_0+\beta_1 x+\epsilon\]</span>
In our example:
<span class="math display">\[Price=\beta_0+\beta_1 Age+\epsilon\]</span></p>
<p><strong>Simple linear regression (sample)</strong>
<span class="math display">\[\hat{y}=b_0+b_1 x\]</span>
where the coefficient <span class="math inline">\(\beta_0\)</span> (and its estimate <span class="math inline">\(b_0\)</span> or <span class="math inline">\(\hat{\beta}_0\)</span> ) refers to the <span class="math inline">\(y\)</span>-intercept or simply the intercept or the constant of the regression line, and the coefficient <span class="math inline">\(\beta_1\)</span> (and its estimate <span class="math inline">\(b_1\)</span> or <span class="math inline">\(\hat{\beta}_1\)</span> ) refers to the slope of the regression line.</p>
</div>
<div id="least-squares-criterion-1" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> Least-Squares criterion</h2>
<ul>
<li><p>The <strong>least-squares criterion</strong> is that the line that best fits a set of data points is the one having the smallest possible sum of squared errors. The `errors’ are the vertical distances of the data points to the line.</p></li>
<li><p>We need to use the data to estimate the values of the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, i.e. to fit a straight line to the set of points <span class="math inline">\(\{(x_i , y_i )\}\)</span>.
There are many straight lines we could use, so we need some idea of which is best.
Clearly, a bad straight line model would be one that had many large errors, and conversely, a good straight line model will have, on average, small errors.
We quantify this by the sum of squares of the errors:
<span class="math display">\[Q(\beta_0,\beta_1)=\sum_{i=1}^n \epsilon_i^2=\sum_{i=1}^n[y_i-(\beta_0 + \beta_1 x_i)]^2\]</span>
then the “line of best fit” will correspond to the line with values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> that minimises <span class="math inline">\(Q(\beta_0,\beta_1)\)</span>.</p></li>
<li><p>The regression line is the line that fits a set of data points according to the least squares criterion.</p></li>
<li><p>The regression equation is the equation of the regression line.</p></li>
<li><p>The regression equation for a set of <span class="math inline">\(n\)</span> data points is
<span class="math inline">\(\hat{y}=b_0+b_1\;x\)</span>, where
<span class="math display">\[b_1=\frac{S_{xy}}{S_{xx}}=\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i-\bar{x})^2}
\;\;\text{and}\;\; b_0=\bar{y}-b_1\; \bar{x}\]</span></p></li>
<li><p><span class="math inline">\(y\)</span> is the dependent variable (or response variable) and <span class="math inline">\(x\)</span> is the independent variable (predictor variable or explanatory variable).</p></li>
<li><p><span class="math inline">\(b_0\)</span> is called the <strong>y-intercept</strong> and <span class="math inline">\(b_1\)</span> is called the <strong>slope</strong>.</p></li>
</ul>
<p><img src="figures/leastsq2.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><strong>SSE and the standard error</strong></p>
<p>This least square regression line minimizes the error sum of squares
<span class="math display">\[SSE=\sum e^2_i =\sum (y_i-\hat{y}_i)^2\]</span>
The standard error of the estimate,
<span class="math inline">\(s_e=\sqrt{SSE/(n-2)}\)</span>, indicates how much, on average, the observed values of the response variable differ from the predicated values of the response variable.</p>
<p><img src="figures/leastsq1.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-used-cars-cont.-1" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> Example: used cars (cont.)</h2>
<p>The table below displays data on Age (in years) and Price (in hundreds of dollars) for a sample of cars of a particular make and model.(Weiss, 2012)</p>
<table>
<thead>
<tr class="header">
<th align="center">Price (<span class="math inline">\(y\)</span>)</th>
<th align="center">Age (<span class="math inline">\(x\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">85</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">103</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td align="center">70</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">82</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">89</td>
<td align="center">5</td>
</tr>
<tr class="even">
<td align="center">98</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="center">66</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">95</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td align="center">169</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">70</td>
<td align="center">7</td>
</tr>
<tr class="odd">
<td align="center">48</td>
<td align="center">7</td>
</tr>
</tbody>
</table>
<ul>
<li><p>For our example, <em>age</em> is the predictor variable and <em>price</em> is the response variable.</p></li>
<li><p>The regression equation is <span class="math inline">\(\hat{y}=195.47-20.26\;x\)</span>, where the slope <span class="math inline">\(b_1=-20.26\)</span> and the intercept <span class="math inline">\(b_0=195.47\)</span></p></li>
<li><p>Prediction: for <span class="math inline">\(x = 4\)</span>, that is we would like to predict the price of a
4-year-old car,
<span class="math display">\[\hat{y}=195.47-20.26 {\color{blue}(4)}= 114.43 \;\;\text{or}\;\; \$ 11443\]</span></p></li>
</ul>
<p><img src="figures/leastsq5.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p>Back to our used cars example, we want to find the “best line”
through the data points, which can be used to predict prices of used cars based on their age.</p>
<p>First we need to enter the data in R.</p>
<pre class="r"><code>Price&lt;-c(85, 103,  70,  82,  89,  98,  66,  95, 169,  70,  48)
Age&lt;- c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7)
carSales&lt;-data.frame(Price,Age)
str(carSales)</code></pre>
<pre><code>## &#39;data.frame&#39;:    11 obs. of  2 variables:
##  $ Price: num  85 103 70 82 89 98 66 95 169 70 ...
##  $ Age  : num  5 4 6 5 5 5 6 6 2 7 ...</code></pre>
<pre class="r"><code>cor(Age, Price, method = &quot;pearson&quot;)</code></pre>
<pre><code>## [1] -0.9237821</code></pre>
<p>Scatterplot: Age vs. Price</p>
<pre class="r"><code>library(ggplot2)
ggplot(carSales, aes(x=Age, y=Price)) + geom_point()</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-38-1.png" width="50%" height="50%" style="display: block; margin: auto;" /></p>
<pre class="r"><code># Remove the confidence interval
ggplot(carSales, aes(x=Age, y=Price)) + 
  geom_point()+
  geom_smooth(method=lm, formula= y~x, se=FALSE)</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-39-1.png" width="50%" height="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="prediction-1" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> Prediction</h2>
<pre class="r"><code># simple linear regression
reg&lt;-lm(Price~Age)
print(reg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Price ~ Age)
## 
## Coefficients:
## (Intercept)          Age  
##      195.47       -20.26</code></pre>
<p>To predict the price of a 4-year-old car (<span class="math inline">\(x=4\)</span>):
<span class="math display">\[\hat{y}=195.47-20.26(4)=114.43\]</span></p>
<p><img src="figures/leastsq3.PNG" width="60%" height="60%" style="display: block; margin: auto;" />
# Simple Regression: Coefficient of Determination</p>
</div>
<div id="extrapolation-1" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> Extrapolation</h2>
<ul>
<li><p>Within the range of the observed values of the predictor variable, we can reasonably use the regression equation to make predictions for the response variable.</p></li>
<li><p>However, to do so outside the range, which is called <strong>Extrapolation</strong>, may not be reasonable because the linear relationship between the predictor and response variables may not hold here.</p></li>
<li><p>To predict the price of an 11-year old car, <span class="math inline">\(\hat{y}=195.47-20.26 (11)=-27.39\)</span>
or $ 2739, this result is unrealistic as no one is going to pay us $2739 to take away their 11-year old car.</p></li>
</ul>
<p><img src="figures/extrap.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="outliers-and-influential-observations-1" class="section level2" number="4.7">
<h2><span class="header-section-number">4.7</span> Outliers and influential observations</h2>
<ul>
<li><p>Recall that an <strong>outlier</strong> is an observation that lies outside the overall pattern of the data. In the context of regression, an outlier is a data point that lies far from the regression line, relative to the other data points.</p></li>
<li><p>An <strong>influential observation</strong> is a data point whose removal causes the regression equation (and line) to change considerably.</p></li>
<li><p>From the scatterplot, it seems that the data point (2,169) might be an influential observation.
Removing that data point and recalculating the regression equation yields <span class="math inline">\(\hat{y}=160.33-14.24\;x\)</span>.</p></li>
</ul>
<p><img src="figures/outliersreg.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="coefficient-of-determination-1" class="section level2" number="4.8">
<h2><span class="header-section-number">4.8</span> Coefficient of determination</h2>
<p><img src="figures/cod1.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/cod2.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/cod3.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/cod4.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/cod5.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<ul>
<li><p>The total variation in the observed values of the response variable, <span class="math inline">\(SST=\sum(y_i-\bar{y})^2\)</span>, can be partitioned into two components:</p>
<ul>
<li>The variation in the observed values of the response variable explained by the regression: <span class="math inline">\(SSR=\sum(\hat{y}_i-\bar{y})^2\)</span></li>
<li>The variation in the observed values of the response variable not explained by the regression: <span class="math inline">\(SSE=\sum(y_i-\hat{y}_i)^2\)</span></li>
</ul></li>
<li><p>The coefficient of determination, <span class="math inline">\(R^2\)</span> (or <span class="math inline">\(R\)</span>-square), is the proportion of the variation in the observed values of the response variable explained by the regression, which is given by
<span class="math display">\[R^2=\frac{SSR}{SST}=\frac{SST-SSE}{SST}=1-\frac{SSE}{SST}\]</span>
where <span class="math inline">\(SST=SSR+SSE\)</span>. <span class="math inline">\(R^2\)</span> is a descriptive measure of the utility of the regression equation for making prediction.</p></li>
<li><p>The coefficient of determination <span class="math inline">\(R^2\)</span> always lies between 0 and 1. A value of <span class="math inline">\(R^2\)</span> near 0 suggests that the regression equation is not very useful for making predictions, whereas a value of <span class="math inline">\(R^2\)</span> near 1 suggests that the regression equation is quite useful for making predictions.</p></li>
<li><p>For a simple linear regression (one independent variable) ONLY, <span class="math inline">\(R^2\)</span> is the square of Pearson correlation coefficient, <span class="math inline">\(r\)</span>.</p></li>
<li><p><span class="math inline">\(\text{Adjusted}\;R^2\)</span> is a modification of <span class="math inline">\(R^2\)</span> which takes into account the number of independent variables, say <span class="math inline">\(k\)</span>. In a simple linear regression <span class="math inline">\(k=1\)</span>. Adjusted-<span class="math inline">\(R^2\)</span> increases only when a significant related independent variable is added to the model. Adjusted-<span class="math inline">\(R^2\)</span> has a crucial role in the process of model building. Adjusted-<span class="math inline">\(R^2\)</span> is given by
<span class="math display">\[\text{Adjusted-}R^2=1-(1-R^2)\frac{n-1}{n-k-1}\]</span></p></li>
</ul>
</div>
<div id="notation-used-in-regression-1" class="section level2" number="4.9">
<h2><span class="header-section-number">4.9</span> Notation used in regression</h2>
<table>
<thead>
<tr class="header">
<th align="center">Quantity</th>
<th align="center">Defining formula</th>
<th align="left">Computing formula</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><span class="math inline">\(S_{xx}\)</span></td>
<td align="center"><span class="math inline">\(\sum (x_i-\bar{x})^2\)</span></td>
<td align="left"><span class="math inline">\(\sum x^2_i - n \bar{x}^2\)</span></td>
</tr>
<tr class="even">
<td align="center"><span class="math inline">\(S_{xy}\)</span></td>
<td align="center"><span class="math inline">\(\sum (x_i-\bar{x})(y_i-\bar{y})\)</span></td>
<td align="left"><span class="math inline">\(\sum x_i y_i - n \bar{x}\bar{y}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><span class="math inline">\(S_{yy}\)</span></td>
<td align="center"><span class="math inline">\(\sum (y_i-\bar{y})^2\)</span></td>
<td align="left"><span class="math inline">\(\sum y^2_i - n \bar{y}^2\)</span></td>
</tr>
</tbody>
</table>
<p>where <span class="math inline">\(\bar{x}=\frac{\sum x_i}{n}\)</span> and <span class="math inline">\(\bar{y}=\frac{\sum y_i}{n}\)</span>. And,</p>
<p><span class="math display">\[SST=S_{yy},\;\;\; SSR=\frac{S^2_{xy}}{S_{xx}},\;\;\; SSE=S_{yy}-\frac{S^2_{xy}}{S_{xx}} \]</span>
and <span class="math inline">\(SST=SSR+SSE\)</span>.</p>
</div>
</div>
<div id="simple-linear-regression-assumptions" class="section level1" number="5">
<h1><span class="header-section-number">5</span> Simple Linear Regression: Assumptions</h1>
<p>Recall that the simple linear regression model for <span class="math inline">\(Y\)</span> on <span class="math inline">\(x\)</span> is
<span class="math display">\[Y=\beta_0+\beta_1 x+\epsilon\]</span>
where</p>
<p><span class="math inline">\(Y\)</span> : the dependent or response variable</p>
<p><span class="math inline">\(x\)</span> : the independent or predictor variable, assumed known</p>
<p><span class="math inline">\(\beta_0,\beta_1\)</span> : the regression parameters, the intercept and slope of the regression line</p>
<p><span class="math inline">\(\epsilon\)</span> : the random regression error around the line.</p>
<p>and the regression equation for a set of <span class="math inline">\(n\)</span> data points is
<span class="math inline">\(\hat{y}=b_0+b_1\;x\)</span>, where
<span class="math display">\[b_1=\frac{S_{xy}}{S_{xx}}=\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i-\bar{x})^2}\]</span>
and
<span class="math display">\[b_0=\bar{y}-b_1\; \bar{x}\]</span>
where <span class="math inline">\(b_0\)</span> is called the <strong>y-intercept</strong> and <span class="math inline">\(b_1\)</span> is called the <strong>slope</strong>.</p>
<p>The <strong>residual standard error</strong> <span class="math inline">\(s_e\)</span> can be defined as</p>
<p><span class="math display">\[s_e=\sqrt{\frac{SSE}{n-2}}=\sqrt{\frac{\sum(y_i-\hat{y}_i)^2}{n-2}} \]</span>
<span class="math inline">\(s_e\)</span> indicates how much, on average, the observed values of the response variable differ from the predicated values of the response variable.
<span class="math inline">\(~\)</span></p>
<div id="simple-linear-regression-assumptions-slr" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Simple Linear Regression Assumptions (SLR)</h2>
<p>We have a collection of <span class="math inline">\(n\)</span> pairs of observations <span class="math inline">\(\{(x_i,y_i)\}\)</span>, and the idea is to use them to estimate the unknown parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
<span class="math display">\[\epsilon_i=Y_i-(\beta_0+\beta_1\;x_i)\;,\;\;i=1,2,\ldots,n\]</span></p>
<p>We need to make the following key assumptions on the errors:</p>
<p>A. <span class="math inline">\(E(\epsilon_i)=0\)</span> (errors have mean zero and do not depend on <span class="math inline">\(x\)</span>)</p>
<p>B. <span class="math inline">\(Var(\epsilon_i)=\sigma^2\)</span> (errors have a constant variance, homoscedastic, and do not depend on <span class="math inline">\(x\)</span>)</p>
<p>C. <span class="math inline">\(\epsilon_1, \epsilon_2,\ldots \epsilon_n\)</span> are independent.</p>
<p>D. <span class="math inline">\(\epsilon_i \mbox{ are all i.i.d. } N(0, \;\sigma^2)\)</span>, meaning that the errors are independent and identically distributed as Normal with mean zero and constant variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The above assumptions, and conditioning on <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, imply:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Linearity: <span class="math inline">\(E(Y_i|X_i)=\beta_0+\beta_1\;x_i\)</span></p></li>
<li><p>Homogenity or homoscedasticity: <span class="math inline">\(Var(Y_i|X_i)=\sigma^2\)</span></p></li>
<li><p>Independence: <span class="math inline">\(Y_1,Y_2,\ldots,Y_n\)</span> are all independent given <span class="math inline">\(X_i\)</span>.</p></li>
<li><p>Normality: <span class="math inline">\(Y_i|X_i\sim N(\beta_0+\beta_1x_i,\;\sigma^2)\)</span></p></li>
</ol>
<p><img src="figures/ass1.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-used-cars-cont.-2" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Example: used cars (cont.)</h2>
<p><img src="figures/ass2.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p>We can see that for each age, the mean price of all cars of that age lies on the regression line <span class="math inline">\(E(Y|x)=\beta_0+\beta_1x\)</span>. And, the prices of all cars of that age are assumed to be normally distributed with mean equal to <span class="math inline">\(\beta_0+\beta_1 x\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. For example, the prices of all 4-year-old cars must be normally distributed with mean <span class="math inline">\(\beta_0+\beta_1 (4)\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>We used the least square method to find the best fit for this data set, and residuals can be obtained as
<span class="math inline">\(e_i=y_i-\hat{y_i}= y_i-(195.47 -20.26x_i)\)</span>.</p>
<p><img src="figures/ass3.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="residual-analysis" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Residual Analysis</h2>
<p>The easiest way to check the simple linear regression assumptions is by constructing
a scatterplot of the residuals (<span class="math inline">\(e_i=y_i-\hat{y_i}\)</span>) against the fitted values (<span class="math inline">\(\hat{y_i}\)</span>) or against <span class="math inline">\(x\)</span>. If the model is a good fit, then the <strong>residual plot</strong> should show an even and random scatter of the residuals.</p>
<p><img src="figures/ass4.PNG" width="40%" height="40%" style="display: block; margin: auto;" /></p>
<div id="linearity" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Linearity</h3>
<p>The regression needs to be linear in the parameters.</p>
<p><span class="math display">\[Y=\beta_0+\beta_1\;x+\epsilon\]</span>
<span class="math display">\[E(Y_i|X_i)=\beta_0+\beta_1\;x_i  \equiv E(\epsilon_i|X_i)=E(\epsilon_i)=0\]</span></p>
<p><img src="figures/ass5.PNG" width="15%" height="15%" style="display: block; margin: auto;" /></p>
<p>The residual plot below shows that a linear regression model is not appropriate for this data.</p>
<p><img src="figures/ass6.PNG" width="40%" height="40%" style="display: block; margin: auto;" /></p>
</div>
<div id="constant-error-variance-homoscedasticity" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Constant error variance (homoscedasticity)</h3>
<p>The plot shows the spread of the residuals is increasing as the fitted values (or <span class="math inline">\(x\)</span>) increases, which indicates that we have Heteroskedasticity (non-constant variance). The standard errors are biased when heteroskedasticity is present, but the regression coefficients will still be unbiased.</p>
<p><img src="figures/ass7.PNG" width="70%" height="70%" style="display: block; margin: auto;" /></p>
<p><strong>How to detect?</strong></p>
<ul>
<li><p>Residuals plot (fitted vs residuals)</p></li>
<li><p>Goldfeld–Quandt test</p></li>
<li><p>Breusch-Pagan test</p></li>
</ul>
<p><strong>How to fix?</strong></p>
<ul>
<li><p>White’s standard errors</p></li>
<li><p>Weighted least squares model</p></li>
<li><p>Taking the log</p></li>
</ul>
</div>
<div id="independent-errors-terms-no-autocorrelation" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Independent errors terms (no autocorrelation)</h3>
<p>The problem of autocorrelation is most likely to occur in time series data, however, it can also occur in cross-sectional data, e.g. if the model is incorrectly specified. When autocorrelation is present, the regression coefficients will still be unbiased, however, the standard errors and test statitics are no longer valid.</p>
<p><strong>An example of no autocorrelation</strong></p>
<p><img src="figures/ass10.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><strong>An example of positive autocorrelation</strong></p>
<p><img src="figures/ass8.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><strong>An example of negative autocorrelation</strong></p>
<p><img src="figures/ass9.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><strong>How to detect?</strong></p>
<ul>
<li><p>Residuals plot</p></li>
<li><p>Durbin-Watson test</p></li>
<li><p>Breusch-Godfrey test</p></li>
</ul>
<p><strong>How to fix?</strong></p>
<ul>
<li><p>Investigate omitted variables (e.g. trend, business cycle)</p></li>
<li><p>Use advanced models (e.g. AR model)</p></li>
</ul>
</div>
<div id="normality-of-the-errors" class="section level3" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> Normality of the errors</h3>
<p>We need the errors to be normally distributed. Normality is only required for the sampling distributions, hypothesis testing and confidence intervals.</p>
<p><strong>How to detect?</strong></p>
<ul>
<li><p>Histogram of residuals</p></li>
<li><p>Q-Q plot of residuals</p></li>
<li><p>Kolmogorov–Smirnov test</p></li>
<li><p>Shapiro–Wilk test</p></li>
</ul>
<p><strong>How to fix?</strong></p>
<ul>
<li><p>Change the functional form (e.g. taking the log)</p></li>
<li><p>Larger sample if possible</p></li>
</ul>
</div>
</div>
<div id="example-infant-mortality-and-gdp" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Example: Infant mortality and GDP</h2>
<p>Let us investigate the relationship between infant mortality and the wealth of a country. We will use data on 207 countries of the world gathered by the UN in 1998 (the ‘UN’ data set is available from the R package ‘car’). The data set contains two variables: the infant mortality rate in deaths per 1000 live births, and the GDP per capita in US dollars. There are some missing data values for some countries, so we will remove the missing data before we fit our model.</p>
<pre class="r"><code># install.packages(&quot;carData&quot;)
library(carData)
data(UN)
options(scipen=999)
# Remove missing data
newUN&lt;-na.omit(UN) 
str(newUN)</code></pre>
<pre><code>## &#39;data.frame&#39;:    193 obs. of  7 variables:
##  $ region         : Factor w/ 8 levels &quot;Africa&quot;,&quot;Asia&quot;,..: 2 4 1 1 5 2 3 8 4 2 ...
##  $ group          : Factor w/ 3 levels &quot;oecd&quot;,&quot;other&quot;,..: 2 2 3 3 2 2 2 1 1 2 ...
##  $ fertility      : num  5.97 1.52 2.14 5.13 2.17 ...
##  $ ppgdp          : num  499 3677 4473 4322 9162 ...
##  $ lifeExpF       : num  49.5 80.4 75 53.2 79.9 ...
##  $ pctUrban       : num  23 53 67 59 93 64 47 89 68 52 ...
##  $ infantMortality: num  124.5 16.6 21.5 96.2 12.3 ...
##  - attr(*, &quot;na.action&quot;)= &#39;omit&#39; Named int [1:20] 4 6 21 35 38 54 67 75 77 78 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:20] &quot;American Samoa&quot; &quot;Anguilla&quot; &quot;Bermuda&quot; &quot;Cayman Islands&quot; ...</code></pre>
<pre class="r"><code>fit&lt;- lm(infantMortality ~ ppgdp, data=newUN)
summary(fit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = infantMortality ~ ppgdp, data = newUN)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -31.48 -18.65  -8.59  10.86  83.59 
## 
## Coefficients:
##               Estimate Std. Error t value             Pr(&gt;|t|)    
## (Intercept) 41.3780016  2.2157454  18.675 &lt; 0.0000000000000002 ***
## ppgdp       -0.0008656  0.0001041  -8.312   0.0000000000000173 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 25.13 on 191 degrees of freedom
## Multiple R-squared:  0.2656, Adjusted R-squared:  0.2618 
## F-statistic: 69.08 on 1 and 191 DF,  p-value: 0.0000000000000173</code></pre>
<pre class="r"><code>plot(newUN$infantMortality ~ newUN$ppgdp, xlab=&quot;GDP per Capita&quot;, ylab=&quot;Infant mortality (per 1000 births)&quot;, pch=16, col=&quot;cornflowerblue&quot;)
abline(fit,col=&quot;red&quot;)</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-59-1.png" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p>We can see from the scatterplot that the relationship between the two variables is not linear. There is a concentration of data points at small values of GDP (many poor countries) and a concentration of data points at small values of infant mortality (many countries with very low mortality). This suggests a skewness to both variables which would not conform to the normality assumption. Indeed, the regression line (the red line) we construct is a poor fit and only has an <span class="math inline">\(R^2\)</span> of 0.266.</p>
<p>From the residual plot below we can see a clear evidence of structure to the residuals suggesting the linear relationship is a poor description of the data, and substantial changes in spread suggesting the assumption of homogeneous variance is not appropriate.</p>
<pre class="r"><code># diagnostic plots 
plot(fit,which=1,pch=16,col=&quot;cornflowerblue&quot;)</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-60-1.png" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p>So we can apply a transformation to one or both variables, e.g. taking the log or adding a quadratic form. Notice that this will not affect (violet) the linearity assumption as the regression will still be linear in the parameters. So if we take the logs of both variables gives us the scatterplot of the transformed data set, below, which appears to show a more promising linear structure. The quality of the regression is now improved, with an <span class="math inline">\(R^2\)</span> value of 0.766, which is still a little weak due to the rather large spread in the data.</p>
<pre class="r"><code>fit1&lt;- lm(log(infantMortality) ~ log(ppgdp), data=newUN)
summary(fit1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(infantMortality) ~ log(ppgdp), data = newUN)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.16789 -0.36738 -0.02351  0.24544  2.43503 
## 
## Coefficients:
##             Estimate Std. Error t value            Pr(&gt;|t|)    
## (Intercept)  8.10377    0.21087   38.43 &lt;0.0000000000000002 ***
## log(ppgdp)  -0.61680    0.02465  -25.02 &lt;0.0000000000000002 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.5281 on 191 degrees of freedom
## Multiple R-squared:  0.7662, Adjusted R-squared:  0.765 
## F-statistic: 625.9 on 1 and 191 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<pre class="r"><code>plot(log(newUN$infantMortality) ~ log(newUN$ppgdp), xlab=&quot;GDP per Capita&quot;, ylab=&quot;Infant mortality (per 1000 births)&quot;, pch=16, col=&quot;cornflowerblue&quot;)
abline(fit1,col=&quot;red&quot;)</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-61-1.png" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p>So we check the residuals again, as we can see from the residuals plot below that the log transformation has corrected many of the problems with with residual plot and the residuals now much closer to the expected random scatter.</p>
<pre class="r"><code># diagnostic plots 
plot(fit1,which=1,pch=16,col=&quot;cornflowerblue&quot;)</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-62-1.png" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p>Now let us check the Normality of the errors by creating a histogram and normal QQ plot for the residuals, before and after the log transformation. The normal quantile (QQ) plot shows the sample quantiles of the residuals against the theoretical quantiles that we would expect if these values were drawn from a Normal distribution. If the Normal assumption holds, then we would see an approximate straight-line relationship on the Normal quantile plot.</p>
<pre class="r"><code>par(mfrow=c(2,2))
# before the log  transformation.
plot(fit, which = 2,pch=16, col=&quot;cornflowerblue&quot;)
hist(resid(fit),col=&quot;cornflowerblue&quot;,main=&quot;&quot;)
# after the log  transformation.
plot(fit1, which = 2, pch=16, col=&quot;hotpink3&quot;)  
hist(resid(fit1),col=&quot;hotpink3&quot;,main=&quot;&quot;)</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-63-1.png" width="90%" height="90%" style="display: block; margin: auto;" /></p>
<p>The normal quantile plot and the histogram of residuals (before the log transformation) shows strong departure from the expectation of an approximate straight line, with curvature in the tails which reflects the skewness of the data. Finally, the normal quantile plot and the histogram of residuals suggest that residuals are much closer to Normality after the transformation, with some minor deviations in the tails.</p>
</div>
</div>
<div id="simple-linear-regression-inference" class="section level1" number="6">
<h1><span class="header-section-number">6</span> Simple Linear Regression: Inference</h1>
<div id="simple-linear-regression-assumptions-1" class="section level2" number="6.1">
<h2><span class="header-section-number">6.1</span> Simple Linear Regression Assumptions</h2>
<ul>
<li><p>Linearity of the relationship between the dependent and independent variables</p></li>
<li><p>Independence of the errors (no autocorrelation)</p></li>
<li><p>Constant variance of the errors (homoscedasticity)</p></li>
<li><p>Normality of the error distribution.</p></li>
</ul>
</div>
<div id="simple-linear-regression-2" class="section level2" number="6.2">
<h2><span class="header-section-number">6.2</span> Simple Linear Regression</h2>
<p>The simple linear regression model for <span class="math inline">\(Y\)</span> on <span class="math inline">\(x\)</span> is
<span class="math display">\[Y=\beta_0+\beta_1 x+\epsilon\]</span>
where</p>
<p><span class="math inline">\(Y\)</span> : the dependent or response variable</p>
<p><span class="math inline">\(x\)</span> : the independent or predictor variable, assumed known</p>
<p><span class="math inline">\(\beta_0,\beta_1\)</span> : the regression parameters, the intercept and slope of the regression line</p>
<p><span class="math inline">\(\epsilon\)</span> : the random regression error around the line.</p>
</div>
<div id="the-simple-linear-regression-equation" class="section level2" number="6.3">
<h2><span class="header-section-number">6.3</span> The simple linear regression equation</h2>
<ul>
<li>The regression equation for a set of <span class="math inline">\(n\)</span> data points is
<span class="math inline">\(\hat{y}=b_0+b_1\;x\)</span>, where
<span class="math display">\[b_1=\frac{S_{xy}}{S_{xx}}=\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i-\bar{x})^2}\]</span>
and
<span class="math display">\[b_0=\bar{y}-b_1\; \bar{x}\]</span></li>
<li><span class="math inline">\(y\)</span> is the dependent variable (or response variable) and <span class="math inline">\(x\)</span> is the independent variable (predictor variable or explanatory variable).</li>
<li><span class="math inline">\(b_0\)</span> is called the <strong>y-intercept</strong> and <span class="math inline">\(b_1\)</span> is called the <strong>slope</strong>.</li>
</ul>
</div>
<div id="residual-standard-error-s_e" class="section level2" number="6.4">
<h2><span class="header-section-number">6.4</span> Residual standard error, <span class="math inline">\(s_e\)</span></h2>
<p>The residual standard error, <span class="math inline">\(s_e\)</span>, is defined by
<span class="math display">\[s_e=\sqrt{\frac{SSE}{n-2}}\]</span>
where <span class="math inline">\(SSE\)</span> is the error sum of squares (also known as the residual sum of squares, RSS) which can be defined as
<span class="math display">\[SSE=\sum e^2_i=\sum(y_i-\hat{y}_i)^2=S_{yy}-\frac{S^2_{xy}}{S_{xx}}\]</span>
<span class="math inline">\(s_e\)</span> indicates how much, on average, the observed values of the response variable differ from the predicated values of the response variable.
Under the simple linear regression assumptions, <span class="math inline">\(s_e\)</span> is an unbiased estimate for the error standard deviation <span class="math inline">\(\sigma\)</span>.</p>
</div>
<div id="properties-of-regression-coefficients" class="section level2" number="6.5">
<h2><span class="header-section-number">6.5</span> Properties of Regression Coefficients</h2>
<p>Under the simple linear regression assumptions, the least square estimates <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are unbiased for the <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, respectively, i.e.</p>
<p><span class="math inline">\(E[b_0]=\beta_0\)</span> and <span class="math inline">\(E[b_1]=\beta_1\)</span>.</p>
<p>The variances of the least squares estimators in simple linear regression are:</p>
<p><span class="math display">\[Var[b_0]=\sigma^2_{b_0}=\sigma^2\left(\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}\right)\]</span></p>
<p><span class="math display">\[Var[b_1]=\sigma^2_{b_1}=\frac{\sigma^2}{S_{xx}}\]</span>
<span class="math display">\[Cov[b_0,b_1]=\sigma_{b_0,b_1}=-\sigma^2\frac{\bar{x}}{S_{xx}}\]</span></p>
<p>We use <span class="math inline">\(s_e\)</span> to estimate the error standard deviation <span class="math inline">\(\sigma\)</span>:</p>
<p><span class="math display">\[s^2_{b_0}=s_e^2\left(\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}\right)\]</span></p>
<p><span class="math display">\[s^2_{b_1}=\frac{s_e^2}{S_{xx}}\]</span></p>
<p><span class="math display">\[s_{b_0,b_1}=-s_e^2\frac{\bar{x}}{S_{xx}}\]</span></p>
</div>
<div id="sampling-distribution-of-the-least-square-estimators" class="section level2" number="6.6">
<h2><span class="header-section-number">6.6</span> Sampling distribution of the least square estimators</h2>
<p>For the Normal error simple linear regression model:
<span class="math display">\[b_0\sim N(\beta_0,\sigma^2_{b_0}) \rightarrow
\frac{b_0-\beta_0}{\sigma_{b_0}}\sim N(0,1)\]</span>
and
<span class="math display">\[b_1\sim N(\beta_1,\sigma^2_{b_1}) \rightarrow
\frac{b_1-\beta_1}{\sigma_{b_1}}\sim N(0,1)\]</span></p>
<p>We use <span class="math inline">\(s_e\)</span> to estimate the error standard deviation <span class="math inline">\(\sigma\)</span>:
<span class="math display">\[\frac{b_0-\beta_0}{s_{b_0}}\sim t_{n-2}\]</span>
and
<span class="math display">\[\frac{b_1-\beta_1}{s_{b_1}}\sim t_{n-2}\]</span></p>
</div>
<div id="degrees-of-freedom" class="section level2" number="6.7">
<h2><span class="header-section-number">6.7</span> Degrees of Freedom</h2>
<ul>
<li><p>In statistics, degrees of freedom are the number of independent pieces of information that go into the estimate of a particular parameter.</p></li>
<li><p>Typically, the degrees of freedom of an estimate of a parameter are equal to the number of independent observations that go into the estimate, minus the number of parameters that are estimated as intermediate steps in the estimation of the parameter itself.</p></li>
<li><p>The sample variance has <span class="math inline">\(n - 1\)</span> degrees of freedom, since it is computed from n pieces of data, minus the 1 parameter estimated as intermediate step, the sample mean. Similarly, having estimated the sample mean we only have <span class="math inline">\(n - 1\)</span> independent pieces of data left, as if we are given the sample mean and any <span class="math inline">\(n - 1\)</span> of the observations then we can determine the value of remaining observation exactly.</p></li>
</ul>
<p><span class="math display">\[s^2=\frac{\sum(x_i-\bar{x})^2}{n-1}\]</span></p>
<ul>
<li>In linear regression, the degrees of freedom of the residuals is <span class="math inline">\(df=n-k^*\)</span>, where <span class="math inline">\(k^*\)</span>
is the numbers of estimated parameters (including the intercept). So for the simple linear regression, we are estimating <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, thus <span class="math inline">\(df=n-2\)</span>.</li>
</ul>
</div>
<div id="inference-for-the-intercept-beta_0" class="section level2" number="6.8">
<h2><span class="header-section-number">6.8</span> Inference for the intercept <span class="math inline">\(\beta_0\)</span></h2>
<ul>
<li><p>Hypotheses:<span class="math display">\[H_0:\beta_0=0\;\; \text{against}\;\; H_1:\beta_0\neq 0\]</span></p></li>
<li><p>Test statistic: <span class="math display">\[t=\frac{b_0}{s_{b_0}}\]</span>
has a t-distribution with <span class="math inline">\(df=n-2\)</span>, where <span class="math inline">\(s_{b_0}\)</span> is the standard error of <span class="math inline">\(b_0\)</span>, and given by
<span class="math display">\[s_{b_0}=s_e\sqrt{\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}}\]</span>
and</p></li>
</ul>
<p><span class="math display">\[s_e=\sqrt{\frac{SSE}{n-2}}=\sqrt{\frac{\sum(y_i-\hat{y}_i)^2}{n-2}}\]</span>
We reject <span class="math inline">\(H_0\)</span> at level <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(|t|&gt;t_{\alpha/2}\)</span> with <span class="math inline">\(df=n-2\)</span>.</p>
<ul>
<li>100(1-<span class="math inline">\(\alpha\)</span>)% confidence interval for <span class="math inline">\(\beta_0\)</span>,</li>
</ul>
<p><span class="math display">\[b_0 \pm t_{\alpha/2} .\;s_{b_0}\]</span>
where <span class="math inline">\(t_{\alpha/2}\)</span> is critical value obtained from the t‐distribution table with <span class="math inline">\(df=n-2\)</span>.</p>
</div>
<div id="inference-for-the-slope-beta_1" class="section level2" number="6.9">
<h2><span class="header-section-number">6.9</span> Inference for the slope <span class="math inline">\(\beta_1\)</span></h2>
<ul>
<li><p>Hypotheses:
<span class="math display">\[H_0:\beta_1=0\;\; \text{against}\;\; H_1:\beta_1\neq 0\]</span></p></li>
<li><p>Test statistic: <span class="math display">\[t=\frac{b_1}{s_{b_1}}\]</span>
has a t-distribution with <span class="math inline">\(df=n-2\)</span>, where <span class="math inline">\(s_{b_1}\)</span> is the standard error of <span class="math inline">\(b_1\)</span>,and given by</p></li>
</ul>
<p><span class="math display">\[s_{b_1}=\frac{s_e}{\sqrt{S_{xx}}}\]</span></p>
<p>and</p>
<p><span class="math display">\[s_e=\sqrt{\frac{SSE}{n-2}}=\sqrt{\frac{\sum(y_i-\hat{y}_i)^2}{n-2}}\]</span>
We reject <span class="math inline">\(H_0\)</span> at level <span class="math inline">\(\alpha\)</span> if <span class="math inline">\(|t|&gt;t_{\alpha/2}\)</span> with <span class="math inline">\(df=n-2\)</span>.</p>
<ul>
<li>100(1-<span class="math inline">\(\alpha\)</span>)% confidence interval for <span class="math inline">\(\beta_1\)</span>,</li>
</ul>
<p><span class="math display">\[b_1 \pm t_{\alpha/2} \;s_{b_1}\]</span>
where <span class="math inline">\(t_{\alpha/2}\)</span> is critical value obtained from the t‐distribution table with <span class="math inline">\(df=n-2\)</span>.</p>
<p><img src="figures/Ttest2.PNG" width="30%" height="30%" style="display: block; margin: auto;" /></p>
</div>
<div id="how-useful-is-the-regression-model" class="section level2" number="6.10">
<h2><span class="header-section-number">6.10</span> How useful is the regression model?</h2>
<p><strong>Goodness of fit test</strong></p>
<ul>
<li><p>We test the null hypothesis <span class="math inline">\(H_0:\beta_1=0\)</span> against <span class="math inline">\(H_1:\beta_1\neq 0\)</span>, the F-statistic
<span class="math display">\[F=\frac{MSR}{MSE}=\frac{SSR}{SSE/(n-2)}\]</span>
has F-distribution with degrees of freedom <span class="math inline">\(df_1=1\)</span> and <span class="math inline">\(df_2=n-2\)</span>.</p></li>
<li><p>We reject <span class="math inline">\(H_0\)</span>, at level <span class="math inline">\(\alpha\)</span>, if <span class="math inline">\(F&gt;F_{\alpha}(df_1,df_2)\)</span>.</p></li>
<li><p>For a simple linear regression ONLY, F-test is equivalent to t-test for <span class="math inline">\(\beta_1\)</span>.</p></li>
</ul>
<p><img src="figures/Ftest.PNG" width="40%" height="40%" style="display: block; margin: auto;" /></p>
</div>
<div id="example-used-cars-cont.-3" class="section level2" number="6.11">
<h2><span class="header-section-number">6.11</span> Example: used cars (cont.)</h2>
<pre class="r"><code>Price&lt;-c(85, 103,  70,  82,  89,  98,  66,  95, 169,  70,  48)
Age&lt;- c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7)
carSales&lt;-data.frame(Price,Age)
str(carSales)</code></pre>
<pre><code>## &#39;data.frame&#39;:    11 obs. of  2 variables:
##  $ Price: num  85 103 70 82 89 98 66 95 169 70 ...
##  $ Age  : num  5 4 6 5 5 5 6 6 2 7 ...</code></pre>
<pre class="r"><code># simple linear regression
reg&lt;-lm(Price~Age)
summary(reg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Price ~ Age)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.162  -8.531  -5.162   8.946  21.099 
## 
## Coefficients:
##             Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept)   195.47      15.24  12.826 0.000000436 ***
## Age           -20.26       2.80  -7.237 0.000048819 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.58 on 9 degrees of freedom
## Multiple R-squared:  0.8534, Adjusted R-squared:  0.8371 
## F-statistic: 52.38 on 1 and 9 DF,  p-value: 0.00004882</code></pre>
<p><span class="math inline">\(~\)</span></p>
<pre class="r"><code># To obtain the confidence intervals 
confint(reg, level=0.95)</code></pre>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) 160.99243 229.94451
## Age         -26.59419 -13.92833</code></pre>
</div>
<div id="r-output" class="section level2" number="6.12">
<h2><span class="header-section-number">6.12</span> R output</h2>
<p><img src="figures/regRoutput.PNG" width="80%" height="80%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="simple-linear-regression-confidence-and-prediction-intervals" class="section level1" number="7">
<h1><span class="header-section-number">7</span> Simple Linear Regression: Confidence and Prediction intervals</h1>
<p>Earlier we have introduced the simple linear regression as a basic statistical model for the relationship between two random variables. We used the least square method for estimating the regression parameters.</p>
<p>Recall that the simple linear regression model for <span class="math inline">\(Y\)</span> on <span class="math inline">\(x\)</span> is
<span class="math display">\[Y=\beta_0+\beta_1 x+\epsilon\]</span>
where</p>
<p><span class="math inline">\(Y\)</span> : the dependent or response variable</p>
<p><span class="math inline">\(x\)</span> : the independent or predictor variable, assumed known</p>
<p><span class="math inline">\(\beta_0,\beta_1\)</span> : the regression parameters, the intercept and slope of the regression line</p>
<p><span class="math inline">\(\epsilon\)</span> : the random regression error around the line.</p>
<p>and the regression equation for a set of <span class="math inline">\(n\)</span> data points is
<span class="math inline">\(\hat{y}=b_0+b_1\;x\)</span>, where
<span class="math display">\[b_1=\frac{S_{xy}}{S_{xx}}=\frac{\sum (x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i-\bar{x})^2}\]</span>
and
<span class="math display">\[b_0=\bar{y}-b_1\; \bar{x}\]</span>
where <span class="math inline">\(b_0\)</span> is called the <strong>y-intercept</strong> and <span class="math inline">\(b_1\)</span> is called the <strong>slope</strong>.</p>
<p><span class="math inline">\(~\)</span></p>
<p><strong>Under the simple linear regression assumptions</strong>, the residual standard error <span class="math inline">\(s_e\)</span> is an unbiased estimate for the error standard deviation <span class="math inline">\(\sigma\)</span>, where</p>
<p><span class="math display">\[s_e=\sqrt{\frac{SSE}{n-2}}=\sqrt{\frac{\sum(y_i-\hat{y}_i)^2}{n-2}} \]</span>
<span class="math inline">\(s_e\)</span> indicates how much, on average, the observed values of the response variable differ from the predicated values of the response variable.</p>
<p><span class="math inline">\(~\)</span></p>
<p>Below we will see how we can use these least square estimates for prediction. First, we will consider the inference for the conditional mean of the response variable <span class="math inline">\(y\)</span> given a particular value of the independent variable <span class="math inline">\(x\)</span>, let us call this particular value <span class="math inline">\(x^*\)</span>. Next we will see how to predicting the value of the response variable <span class="math inline">\(Y\)</span> for a given value of the independent variable <span class="math inline">\(x^*\)</span>. These confidence and predictive intervals, to be valid, the usual four simple regression assumptions must hold.</p>
<div id="inference-for-the-regression-line-eleftyxright" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Inference for the regression line <span class="math inline">\(E\left[Y|x^*\right]\)</span></h2>
<p>Suppose we are interested in the value of the regression line at a new point <span class="math inline">\(x^*\)</span>. Let’s denote the unknown true value of the regression line at <span class="math inline">\(x=x^*\)</span> as <span class="math inline">\(\mu^*\)</span>. From the form of the regression line equation we have</p>
<p><span class="math display">\[\mu^*=\mu_{Y|x^*}=E\left[Y|x^*\right]=\beta_0+\beta_1 x^*\]</span></p>
<p>but <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown. We can use the least square regression equation
to estimate the unknown true value of the regression line, so we have</p>
<p><span class="math display">\[\hat{\mu}^*=b_0+b_1 x^*=\hat{y}^*\]</span></p>
<p>This is simply a point estimate for the regression line. However, in statistics, point estimate is often not enough, and we need to express our uncertainty about this point estimate, and one way to do so is via confidence interval.</p>
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> confidence interval for the conditional mean <span class="math inline">\(\mu^*\)</span> is
<span class="math display">\[\hat{y}^*\pm t_{\alpha/2}\;\cdot s_e\;\sqrt{\frac{1}{n}+\frac{(x^*-\bar{x})^2}{S_{xx}}}\]</span>
where
<span class="math inline">\(S_{xx}=\sum_{i=1}^{n} (x_i-\bar{x})^2\)</span>,
and <span class="math inline">\(t_{\alpha/2}\)</span> is the <span class="math inline">\(\alpha/2\)</span> critical value from the t-distribution with <span class="math inline">\(df=n-2\)</span>.</p>
<p><img src="figures/Ttest2.PNG" width="35%" height="35%" style="display: block; margin: auto;" /></p>
</div>
<div id="inference-for-the-response-variable-y-for-a-given-xx" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Inference for the response variable <span class="math inline">\(Y\)</span> for a given <span class="math inline">\(x=x^*\)</span></h2>
<p>Suppose now we are interested in predicting the value of <span class="math inline">\(Y^*\)</span> if we have a new observation at <span class="math inline">\(x^*\)</span>.</p>
<p>At <span class="math inline">\(x=x^*\)</span>, the value of <span class="math inline">\(Y^*\)</span> is unknown and given by
<span class="math display">\[Y^*=\beta_0+\beta_1 x^*+\epsilon\]</span>
where but <span class="math inline">\(\beta_0\)</span>, <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\epsilon\)</span> are unknown. We will use <span class="math inline">\(\hat{y}^*=b_0+b_1\;x^*\)</span>
as a basis for our prediction.</p>
<p><span class="math inline">\(~\)</span></p>
<p>A <span class="math inline">\(100(1-\alpha)\%\)</span> prediction interval for <span class="math inline">\(Y^*\)</span> at <span class="math inline">\(x=x^*\)</span> is</p>
<p><span class="math display">\[\hat{y}^* \pm t_{\alpha/2}\;\cdot s_e\;\sqrt{1+\frac{1}{n}+\frac{(x^*-\bar{x})^2}{S_{xx}}}\]</span>
The extra ’1’ under the square root sign, we have here to account for the extra variability of a single observation about the mean.</p>
<p>Note: we construct a confidence interval for a parameter of the population, which is the conditional mean in this case, while we construct a prediction interval for a single value.</p>
</div>
<div id="example-used-cars-cont.-4" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Example: used cars (cont.)</h2>
<p><strong>Estimate the mean price of all 3-year-old cars, <span class="math inline">\(E[Y|x=3]\)</span>:</strong></p>
<p><span class="math display">\[\hat{\mu}^*=195.47-20.26 (3)= 134.69=\hat{y}^*\]</span></p>
<p><img src="figures/predex.PNG" width="40%" height="40%" style="display: block; margin: auto;" /></p>
<p>A 95% confidence interval for the mean price of all 3-year-old cars is
<span class="math display">\[\hat{y}^*\pm t_{\alpha/2}\;\times se\sqrt{\frac{1}{n}+\frac{(x^*-\bar{x})^2}{S_{xx}}}\]</span>
<span class="math display">\[[195.47-20.26(3)]\pm 2.262\times12.58\sqrt{\frac{1}{11}+\frac{(3-5.273)^2}{(11-1)\times2.018}}\]</span>
<span class="math display">\[134.69\pm 16.76\]</span>
that is
<span class="math display">\[117.93&lt;\mu^*&lt;151.45\]</span></p>
<p><strong>Predict the price of a 3-year-old car, <span class="math inline">\(Y|x=3\)</span></strong>:
<span class="math display">\[\hat{y}^*=195.47-20.26 (3)= 134.69\]</span></p>
<p>A 95% predictive interval for the price of a 3-year-old car is</p>
<p><span class="math display">\[\hat{y}^*\pm t_{\alpha/2}\;\times se\sqrt{1+\frac{1}{n}+\frac{(x^*-\bar{x})^2}{S_{xx}}}\]</span>
<span class="math display">\[[195.47-20.26(3)]\pm 2.262\times12.58\sqrt{1+\frac{1}{11}+\frac{(3-5.273)^2}{(11-1)*\times2.018}}\]</span>
<span class="math display">\[134.69\pm 33.025\]</span>
that is
<span class="math display">\[101.67&lt;Y^*&lt;167.72\]</span></p>
<p>where <span class="math inline">\(S_{xx}=\sum_{i=1}^{n} (x_i-\bar{x})^2=(n-1) Var(x)\)</span>.</p>
</div>
<div id="regression-in-r" class="section level2" number="7.4">
<h2><span class="header-section-number">7.4</span> Regression in R</h2>
<pre class="r"><code># Build linear model 
Price&lt;-c(85, 103,  70,  82,  89,  98,  66,  95, 169,  70,  48)
Age&lt;- c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7)
carSales&lt;-data.frame(Price=Price,Age=Age)

reg &lt;- lm(Price~Age,data=carSales)
summary(reg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Price ~ Age, data = carSales)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.162  -8.531  -5.162   8.946  21.099 
## 
## Coefficients:
##             Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept)   195.47      15.24  12.826 0.000000436 ***
## Age           -20.26       2.80  -7.237 0.000048819 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.58 on 9 degrees of freedom
## Multiple R-squared:  0.8534, Adjusted R-squared:  0.8371 
## F-statistic: 52.38 on 1 and 9 DF,  p-value: 0.00004882</code></pre>
<pre class="r"><code>mean(Age)</code></pre>
<pre><code>## [1] 5.272727</code></pre>
<pre class="r"><code>var(Age)</code></pre>
<pre><code>## [1] 2.018182</code></pre>
<pre class="r"><code>qt(0.975,9)</code></pre>
<pre><code>## [1] 2.262157</code></pre>
<pre class="r"><code>newage&lt;- data.frame(Age = 3)
predict(reg, newdata = newage, interval = &quot;confidence&quot;)</code></pre>
<pre><code>##        fit      lwr      upr
## 1 134.6847 117.9293 151.4401</code></pre>
<pre class="r"><code>predict(reg, newdata = newage, interval = &quot;prediction&quot;)</code></pre>
<pre><code>##        fit      lwr      upr
## 1 134.6847 101.6672 167.7022</code></pre>
<p><span class="math inline">\(~\)</span></p>
<p>We can plot the confidence and prediction intervals as follows:</p>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-72-1.png" width="80%" height="80%" style="display: block; margin: auto;" /></p>
<p><img src="figures/predex2.PNG" width="80%" height="80%" style="display: block; margin: auto;" /></p>
<!--
-->
</div>
</div>
<div id="multiple-linear-regression-introduction" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Multiple Linear Regression: Introduction</h1>
<div id="multiple-linear-regression-model" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Multiple linear regression model</h2>
<p>In simple linear regression, we have one dependent variable (<span class="math inline">\(y\)</span>) and one independent variable (<span class="math inline">\(x\)</span>). In multiple linear regression, we have
one dependent variable (<span class="math inline">\(y\)</span>) and several independent variables (<span class="math inline">\(x_1,x_2, \ldots,x_k\)</span>).</p>
<ul>
<li><p>The multiple linear regression model, for the <strong>population</strong>, can be expressed as
<span class="math display">\[Y=\beta_0+\beta_1 x_1 +\beta_2 x_2+\ldots+\beta_kx_k+ \epsilon\]</span>
where <span class="math inline">\(\epsilon\)</span> is the error term.</p></li>
<li><p>The corresponding least square estimate, from the <strong>sample</strong>, of this multiple linear regression model is given by
<span class="math display">\[\hat{y}=b_0+b_1 x_1+b_2 x_2+\ldots+b_k x_k\]</span></p></li>
<li><p>The coefficient <span class="math inline">\(b_0\)</span> (or <span class="math inline">\(\beta_0\)</span>) represents the <span class="math inline">\(y\)</span>-intercept, that is, the value of <span class="math inline">\(y\)</span> when <span class="math inline">\(x_1=x_2= \ldots=x_k=0\)</span>. The coefficient <span class="math inline">\(b_i\)</span> (or <span class="math inline">\(\beta_i\)</span>) <span class="math inline">\((i=1, \ldots, k)\)</span> is the partial slope of <span class="math inline">\(x_i\)</span>, holding all other <span class="math inline">\(x\)</span>’s fixed. So <span class="math inline">\(b_i\)</span> (or <span class="math inline">\(\beta_i\)</span>) tells us the change in <span class="math inline">\(y\)</span> for a unit increase in <span class="math inline">\(x_i\)</span>, holding all other <span class="math inline">\(x\)</span>’s fixed.</p></li>
</ul>
</div>
<div id="example-used-cars-cont.-5" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Example: used cars (cont.)</h2>
<p>The table below displays data on Age, Miles and Price for a sample of cars of a particular make and model.</p>
<table>
<thead>
<tr class="header">
<th align="center">Price (<span class="math inline">\(y\)</span>)</th>
<th align="center">Age (<span class="math inline">\(x_1\)</span>)</th>
<th align="center">Miles (<span class="math inline">\(x_2\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">85</td>
<td align="center">5</td>
<td align="center">57</td>
</tr>
<tr class="even">
<td align="center">103</td>
<td align="center">4</td>
<td align="center">40</td>
</tr>
<tr class="odd">
<td align="center">70</td>
<td align="center">6</td>
<td align="center">77</td>
</tr>
<tr class="even">
<td align="center">82</td>
<td align="center">5</td>
<td align="center">60</td>
</tr>
<tr class="odd">
<td align="center">89</td>
<td align="center">5</td>
<td align="center">49</td>
</tr>
<tr class="even">
<td align="center">98</td>
<td align="center">5</td>
<td align="center">47</td>
</tr>
<tr class="odd">
<td align="center">66</td>
<td align="center">6</td>
<td align="center">58</td>
</tr>
<tr class="even">
<td align="center">95</td>
<td align="center">6</td>
<td align="center">39</td>
</tr>
<tr class="odd">
<td align="center">169</td>
<td align="center">2</td>
<td align="center">8</td>
</tr>
<tr class="even">
<td align="center">70</td>
<td align="center">7</td>
<td align="center">69</td>
</tr>
<tr class="odd">
<td align="center">48</td>
<td align="center">7</td>
<td align="center">89</td>
</tr>
</tbody>
</table>
<p><img src="figures/3d1.png" width="40%" height="40%" style="display: block; margin: auto;" /></p>
<pre><code>## Warning in par(usr): argument 1 does not name a graphical parameter

## Warning in par(usr): argument 1 does not name a graphical parameter

## Warning in par(usr): argument 1 does not name a graphical parameter</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-75-1.png" width="70%" height="70%" style="display: block; margin: auto;" /></p>
<p>The scatterplot and the correlation matrix show a fairly negative relationship between the price of the car and both independent variables (age and miles). It is desirable to have a relationship between each independent variable and the dependent variable.
However, the scatterplot also shows a positive relationship between the age and the miles, which isundesirable as it will cause the issue of Multicollinearity.</p>
</div>
<div id="coefficient-of-determination-r2-and-adjusted-r2" class="section level2" number="8.3">
<h2><span class="header-section-number">8.3</span> Coefficient of determination, <span class="math inline">\(R^2\)</span> and adjusted <span class="math inline">\(R^2\)</span></h2>
<ul>
<li><p>Recall that, <span class="math inline">\(R^2\)</span> is a measure of the proportion of the total variation in the observed values of the response variable that is explained by the multiple linear regression in the <span class="math inline">\(k\)</span> predictor variables <span class="math inline">\(x_1, x_2, \ldots, x_k\)</span>.</p></li>
<li><p><span class="math inline">\(R^2\)</span> will increase when an additional predictor variable is added to the model. One should not simply select a model with many predictor variables because it has the highest <span class="math inline">\(R^2\)</span> value, it is often good to have a model with high <span class="math inline">\(R^2\)</span> value but only few x’s included.</p></li>
<li><p>Adjusted <span class="math inline">\(R^2\)</span> is a modification of <span class="math inline">\(R^2\)</span> that takes into account the number of predictor variables.
<span class="math display">\[\mbox{Adjusted-}R^2=1-(1-R^2)\frac{n-1}{n-k-1}\]</span></p></li>
</ul>
</div>
<div id="the-residual-standard-error-s_e" class="section level2" number="8.4">
<h2><span class="header-section-number">8.4</span> The residual standard error, <span class="math inline">\(s_e\)</span></h2>
<ul>
<li>Recall that,
<span class="math display">\[\text{Residual} = \text{Observed value} - \text{Predicted value.}\]</span></li>
</ul>
<p><span class="math display">\[e_i=y_i-\hat{y}_i\]</span></p>
<ul>
<li><p>In a multiple linear regression with <span class="math inline">\(k\)</span> predictors, the standard error of the estimate, <span class="math inline">\(s_e\)</span>, is defined by
<span class="math display">\[s_e=\sqrt{\frac{SSE}{n-(k+1)}}\;\;\;\;\; \text{where}\;\;SSE=\sum (y_i-\hat{y}_i)^2\]</span></p></li>
<li><p>The standard error of the estimate, <span class="math inline">\(s_e\)</span>, indicates how much, on average, the observed values of the
response variable differ from the predicated values of the
response variable. The <span class="math inline">\(s_e\)</span> is the estimate of the common standard deviation <span class="math inline">\(\sigma\)</span>.</p></li>
</ul>
</div>
<div id="inferences-about-a-particular-predictor-variable" class="section level2" number="8.5">
<h2><span class="header-section-number">8.5</span> Inferences about a particular predictor variable</h2>
<ul>
<li><p>To test whether a particular predictor variable, say <span class="math inline">\(x_i\)</span>, is useful for predicting <span class="math inline">\(y\)</span> we test the null hypothesis <span class="math inline">\(H_0:\beta_i=0\)</span> against <span class="math inline">\(H_1:\beta_i\neq 0\)</span>.</p></li>
<li><p>The test statistic
<span class="math display">\[t=\frac{b_i}{s_{b_i}}\]</span>
has a <span class="math inline">\(t\)</span>-distribution with degrees of freedom <span class="math inline">\(df=n-(k+1)\)</span>. So we reject <span class="math inline">\(H_0\)</span>, at level <span class="math inline">\(\alpha\)</span>, if <span class="math inline">\(|t|&gt;t_{\alpha/2}.\)</span></p></li>
<li><p>Rejection of the null hypothesis indicates that <span class="math inline">\(x_i\)</span> is useful as a predictor for <span class="math inline">\(y\)</span>. However, failing to reject the null hypothesis suggests that <span class="math inline">\(x_i\)</span> may not be useful as a predictor of <span class="math inline">\(y\)</span>, so we may want to consider removing this variable from the regression analysis.</p></li>
<li><p>100(1-<span class="math inline">\(\alpha\)</span>)% confidence interval for <span class="math inline">\(\beta_i\)</span> is
<span class="math display">\[b_i \pm t_{\alpha/2} . s_{b_i}\]</span>
where <span class="math inline">\(s_{b_i}\)</span> is the standard error of <span class="math inline">\(b_i\)</span>.</p></li>
</ul>
<p><img src="figures/Ttest2.png" width="30%" height="30%" style="display: block; margin: auto;" /></p>
</div>
<div id="how-useful-is-the-multiple-regression-model" class="section level2" number="8.6">
<h2><span class="header-section-number">8.6</span> How useful is the multiple regression model?</h2>
<p><strong>Goodness of fit test</strong></p>
<p>To test how useful is this model, we test the null hypothesis</p>
<p><span class="math inline">\(H_0: \beta_1=\beta_2=\ldots =\beta_k=0\)</span>, against</p>
<p><span class="math inline">\(H_1: \text{at least one of the} \;\beta_i \text{&#39;s is not zero}\)</span>.
- The <span class="math inline">\(F\)</span>-statistic
<span class="math display">\[F=\frac{MSR}{MSE}=\frac{SSR/k}{SSE/(n-k-1)}\]</span>
with degrees of freedom <span class="math inline">\(df_1=k\)</span> and <span class="math inline">\(df_2=n-(k+1)\)</span>.</p>
<p>We reject <span class="math inline">\(H_0\)</span>, at level <span class="math inline">\(\alpha\)</span>, if <span class="math inline">\(F&gt;F_{\alpha}(df_1,df_2)\)</span>.</p>
<p><img src="figures/Ftest.png" width="40%" height="40%" style="display: block; margin: auto;" /></p>
</div>
<div id="used-cars-example-continued" class="section level2" number="8.7">
<h2><span class="header-section-number">8.7</span> Used cars example continued</h2>
<p>Multiple regression equation: <span class="math inline">\(\hat{y}=183.04-9.50 x_1- 0.82 x_2\)</span></p>
<p><img src="figures/3d2.png" width="40%" height="40%" style="display: block; margin: auto;" /></p>
<p>The predicted price for a 4-year-old car that has driven 45 thousands miles is
<span class="math display">\[\hat{y}=183.04-9.50 (4)- 0.82 (45)=108.14\]</span>
(as units of $100 were used, this means $10814)</p>
<p><strong>Extrapolation:</strong> we need to look at the region (all combined values) not only the range of the observed values of each predictor variable separately.</p>
</div>
<div id="regression-in-r-1" class="section level2" number="8.8">
<h2><span class="header-section-number">8.8</span> Regression in R</h2>
<pre class="r"><code>Price&lt;-c(85, 103,  70,  82,  89,  98,  66,  95, 169,  70,  48)
Age&lt;- c(5, 4, 6, 5, 5, 5, 6, 6, 2, 7, 7)
Miles&lt;-c(57,40,77,60,49,47,58,39,8,69,89)
carSales&lt;-data.frame(Price=Price,Age=Age,Miles=Miles)


# Scatterplot matrix
# Customize upper panel
upper.panel&lt;-function(x, y){
  points(x,y, pch=19, col=4)
  r &lt;- round(cor(x, y), digits=3)
  txt &lt;- paste0(&quot;r = &quot;, r)
  usr &lt;- par(&quot;usr&quot;); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  text(0.5, 0.9, txt)
}
pairs(carSales, lower.panel = NULL, 
      upper.panel = upper.panel)</code></pre>
<pre><code>## Warning in par(usr): argument 1 does not name a graphical parameter

## Warning in par(usr): argument 1 does not name a graphical parameter

## Warning in par(usr): argument 1 does not name a graphical parameter</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-79-1.png" width="672" /></p>
<pre class="r"><code>reg &lt;- lm(Price~Age+Miles,data=carSales)
summary(reg)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Price ~ Age + Miles, data = carSales)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.364  -5.243   1.028   5.926  11.545 
## 
## Coefficients:
##             Estimate Std. Error t value    Pr(&gt;|t|)    
## (Intercept) 183.0352    11.3476  16.130 0.000000219 ***
## Age          -9.5043     3.8742  -2.453      0.0397 *  
## Miles        -0.8215     0.2552  -3.219      0.0123 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.805 on 8 degrees of freedom
## Multiple R-squared:  0.9361, Adjusted R-squared:  0.9201 
## F-statistic: 58.61 on 2 and 8 DF,  p-value: 0.00001666</code></pre>
<pre class="r"><code>confint(reg, level=0.95)</code></pre>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 156.867552 209.2028630
## Age         -18.438166  -0.5703751
## Miles        -1.409991  -0.2329757</code></pre>
<div id="summary" class="section level3" number="8.8.1">
<h3><span class="header-section-number">8.8.1</span> Summary</h3>
<p><img src="figures/Routputmulti.png" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><img src="figures/Routputmultitable.png" width="60%" height="60%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="multiple-linear-regression-assumptions" class="section level2" number="8.9">
<h2><span class="header-section-number">8.9</span> Multiple Linear Regression Assumptions</h2>
<ul>
<li><p><strong>Linearity</strong>: For each set of values, <span class="math inline">\(x_1, x_2, \ldots, x_k\)</span>, of the predictor variables, the conditional mean of the response variable <span class="math inline">\(y\)</span> is <span class="math inline">\(\beta_0+\beta_1 x_1+\beta_2 x_2+ \ldots+ \beta_k x_k\)</span>.</p></li>
<li><p><strong>Equal variance (homoscedasticity)</strong>: The conditional variance of the response variable are the same (equal to <span class="math inline">\(\sigma^2\)</span>) for all sets of values, <span class="math inline">\(x_1, x_2, \ldots, x_k\)</span>, of the predictor variables.</p></li>
<li><p><strong>Independent observations</strong>: The observations of the response variable are independent of one another.</p></li>
<li><p><strong>Normally</strong>: For each set values, <span class="math inline">\(x_1, x_2, \ldots, x_k\)</span>, of the predictor variables, the conditional distribution of the response variable is a normal distribution.</p></li>
<li><p><strong>No Multicollinearity</strong>: Multicollinearity exists when two or more of the predictor variables are highly correlated.</p></li>
</ul>
<div id="multicollinearity" class="section level3" number="8.9.1">
<h3><span class="header-section-number">8.9.1</span> Multicollinearity</h3>
<ul>
<li><p>Multicollinearity refers to a situation when two or more predictor variables in our multiple regression model are highly (linearly) correlated.</p></li>
<li><p>The least square estimates will remain unbiased, but unstable.</p></li>
<li><p>The standard errors (of the affected variables) are likely to be high.</p></li>
<li><p>Overall model fit (e.g. R-square, F, prediction) is not affected.</p></li>
</ul>
</div>
<div id="multicollinearity-detect" class="section level3" number="8.9.2">
<h3><span class="header-section-number">8.9.2</span> Multicollinearity: Detect</h3>
<ul>
<li><p>Scatterplot Matrix</p></li>
<li><p><strong>Variance Inflation Factors</strong>: the Variance Inflation Factors (VIF) for the <span class="math inline">\(i^{th}\)</span> predictor is
<span class="math display">\[VIF_i=\frac{1}{1-R^2_i}\]</span>
where <span class="math inline">\(R^2_i\)</span> is the R-square value obtained by regressing the <span class="math inline">\(i^{th}\)</span> predictor on the other predictor variables.</p></li>
<li><p><span class="math inline">\(VIF=1\)</span> indicates that there is no correlation between <span class="math inline">\(i^{th}\)</span> predictor variable and the other predictor variables.</p></li>
<li><p>As rule of thumb if <span class="math inline">\(VIF&gt;10\)</span> then multicollinearity could be a problem.</p></li>
</ul>
</div>
<div id="multicollinearity-how-to-fix" class="section level3" number="8.9.3">
<h3><span class="header-section-number">8.9.3</span> Multicollinearity: How to fix?</h3>
<p><strong>Ignore:</strong> if the model is going to be used for prediction only.</p>
<p><strong>Remove:</strong> e.g. see if the variables are providing the same information.</p>
<p><strong>Combine:</strong> combining highly correlated variables.</p>
<p><strong>Advanced:</strong> e.g. Principal Components Analysis, Partial Least Squares.</p>
<p><span class="math inline">\(~\)</span></p>
</div>
</div>
<div id="regression-in-r-regression-assumptions" class="section level2" number="8.10">
<h2><span class="header-section-number">8.10</span> Regression in R (regression assumptions)</h2>
<pre class="r"><code>plot(reg, which=1, pch=19, col=4)</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-82-1.png" width="672" /></p>
<pre class="r"><code>plot(reg, which=2, pch=19, col=4)</code></pre>
<p><img src="Weeks6to9new_files/figure-html/unnamed-chunk-82-2.png" width="672" /></p>
<pre class="r"><code># install.packages(&quot;car&quot;)
library(car)
vif(reg)</code></pre>
<pre><code>##      Age    Miles 
## 3.907129 3.907129</code></pre>
<p>The value of <span class="math inline">\(VIF=3.91\)</span> indicates a moderate correlation between the age and the miles in the model, but this is not a major concern.</p>
</div>
<div id="dummy-variables" class="section level2" number="8.11">
<h2><span class="header-section-number">8.11</span> Dummy Variables</h2>
<p>We will consider the case when we have a qualitative (categorical) predictor (also known as a factor) with two or more levels (or possible values).</p>
<p><strong>Qualitative predictors with only two levels</strong></p>
<p>To include a qualitative predictor in our model, we create a dummy variable that takes on two possible
numerical values, e.g. 0 and 1.</p>
<p>Back to our used cars example, suppose we want to add the transmission type to our linear regression model. So let <span class="math inline">\(d\)</span> be a dummy variable represents the car’s transmission type which takes value 1 for manual car and value 0 for automatic car.</p>
<p>Again, <span class="math inline">\(y=Price\)</span> and <span class="math inline">\(x_1=age\)</span>, and let us not include <span class="math inline">\(x_2=miles\)</span> at the moment.
<span class="math display">\[d_i=\left\{\begin{array}{ll}
1&amp; \text{if $i$th car is manual,}\\
0&amp; \text{if $i$th car is automatic}\\
\end{array}\right.\]</span></p>
<p>then we can regress price on age and transmission type as</p>
<p><span class="math display">\[y=\beta_0+\beta_1 x_1+\beta_2 d+\epsilon\]</span></p>
<p>so for manual cars:
<span class="math display">\[y=(\beta_0+\beta_2)+\beta_1 x_1+\epsilon\]</span>
and for automatic cars:
<span class="math display">\[y=\beta_0+\beta_1 x_1+\epsilon\]</span>
or we can write</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1 x_{1i}+ \beta_2 d_i+\epsilon_i =\left\{\begin{array}{ll}
(\beta_0+\beta_2) + \beta_1 x_{1i}+\epsilon_i&amp; \text{if $i$th car is manual,}\\
\beta_0+\beta_1 x_{1i}+\epsilon_i&amp; \text{if $i$th car is automatic}\\
\end{array}\right.\]</span></p>
<p><img src="figures/fig1_dummy.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><strong>Qualitative predictors with more than two levels</strong></p>
<p>Suppose we now have a categorical variable with three levels, e.g. fuel type (petrol, diesel, and hybrid). So in this case we need to create two dummy
variables, <span class="math inline">\(d_1\)</span> and <span class="math inline">\(d_2\)</span>.</p>
<p><span class="math display">\[d_{1i}=\left\{\begin{array}{ll}
1&amp; \text{if $i$th car has a petrol engine,}\\
0&amp; \text{otherwise}\\
\end{array}\right.\]</span></p>
<p><span class="math display">\[d_{2i}=\left\{\begin{array}{ll}
1&amp; \text{if $i$th car has a diesel engine}\\
0&amp; \text{otherwise}\\
\end{array}\right.\]</span></p>
<p>then one can regress price on age and fuel type as
<span class="math display">\[y=\beta_0+\beta_1 x_{1}+\beta_2 d_{1}+\beta_3 d_{2}+\epsilon\]</span></p>
<p>so for petrol cars:
<span class="math display">\[y=(\beta_0+\beta_2)+\beta_1 x_{1}+\epsilon\]</span></p>
<p>for diesel cars:
<span class="math display">\[y=(\beta_0+\beta_3)+\beta_1 x_{1} +\epsilon\]</span></p>
<p>and for hybrid cars
<span class="math display">\[y=\beta_0+\beta_1 x_{1}+\epsilon\]</span>
this last model is often called the baseline model.</p>
<p><span class="math display">\[\begin{align*}y_i&amp;=\beta_0+\beta_1 \;x_{1i}+\beta_2 d_{1i}+\beta_3 d_{2i}+\epsilon_i\\
&amp;=\left\{\begin{array}{ll}
(\beta_0+\beta_2)+\beta_1 x_{1i} +\epsilon_i&amp; \text{if $i$th car has a petrol engine,}\\
(\beta_0+\beta_3)+\beta_1 x_{1i} +\epsilon_i&amp; \text{if $i$th car  has a diesel engine}\\
\beta_0+\beta_1 x_{1i}+\epsilon_i&amp; \text{if $i$th car has a hybrid engine}\\
\end{array}\right.
\end{align*}\]</span></p>
<p><img src="figures/fig2_dummy.PNG" width="60%" height="60%" style="display: block; margin: auto;" /></p>
<p><strong>The interaction effect</strong></p>
<p>In our used car example, we concluded that both age and miles seem to be associated with the price.
<span class="math display">\[Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 +\epsilon\]</span>
<span class="math display">\[Price = \beta_0 + \beta_1 age + \beta_2 miles +\epsilon\]</span>
that is the linear regression model assumed that the average effect on price of a one-unit increase in age is always <span class="math inline">\(\beta_1\)</span> regardless of the number of miles.</p>
<p>One can extend this model to allow for interaction effects, called an interaction term, which is constructed by computing the product of <span class="math inline">\(x_1=age\)</span> and <span class="math inline">\(x_2=miles\)</span>, e.g. older cars associated with additional miles of driving.
<span class="math display">\[Price = \beta_0 + \beta_1 age + \beta_2 miles + \beta_3 (age\times miles)+\epsilon\]</span>
<span class="math display">\[Price = \beta_0 + (\beta_1+\beta_3\times miles) \times age + \beta_2 miles + \epsilon\]</span>
<span class="math display">\[Price = \beta_0 + \tilde{\beta}_1  \times age + \beta_2 miles + \epsilon
\]</span></p>
<p>where <span class="math inline">\(\tilde{\beta}_1 = \beta_1+\beta_3\times miles\)</span>. Since <span class="math inline">\(\tilde{\beta}_1\)</span> changes with <span class="math inline">\(x_2=miles\)</span>, the effect of <span class="math inline">\(x_1=age\)</span> on <span class="math inline">\(Y=Price\)</span> is
no longer constant.</p>
<p>That is adjusting <span class="math inline">\(x_2=miles\)</span> will change the impact of <span class="math inline">\(x_1=age\)</span> on <span class="math inline">\(Y=Price\)</span>.</p>
<p><span class="math inline">\(~\)</span></p>
</div>
</div>
<div id="sec:varselection" class="section level1" number="9">
<h1><span class="header-section-number">9</span> Introduction to Variable Selection</h1>
<div id="subsec:prostatecancerdata" class="section level2" number="9.1">
<h2><span class="header-section-number">9.1</span> Motivating Example – Prostate Cancer Data</h2>
<p>Prostate cancer is the most common cancer in men in the UK. Fortunately, cure rates are high. One treatment option is surgery (called a radical prostatectomy) which aims to remove the whole prostate and the prostate cancer cells inside it. Prostate-specific antigen (PSA) is a protein made by both normal and cancerous prostate cells. It can be elevated in cases of prostate cancer and other prostate problems.</p>
<p>Throughout this chapter, we will use as a running example a data set that came come from a study that examined the relationship between the level of PSA and 8 clinical measures (e.g. age, prostate weight) in 97 men with prostate cancer who were about to receive a radical prostatectomy.</p>
<p>The data are arranged into a <span class="math inline">\(n \times p\)</span> array with</p>
<ul>
<li><span class="math inline">\(n=97\)</span> rows corresponding to the men;</li>
<li><span class="math inline">\(p=9\)</span> columns corresponding to the variables: the level of PSA and the 8 clinical measures.</li>
</ul>
<p>The data are available from the <code>ElemStatLearn</code> package. Unfortunately, this package is no longer hosted on CRAN so it must be installed from source. After downloading the file <code>ElemStatLearn_2015.6.26.2.tar.gz</code> from Ultra, save it in a directory of your choice. The package can then be installed in RStudio by going to <code>Tools</code> then <code>Install Packages</code>. In the pop-up box that appears, change the drop-down option for “Install from:” to “Package Archive File (.tar.gz)”. Then navigate to the file <code>ElemStatLearn_2015.6.26.2.tar.gz</code> and click <code>Open</code>. Finally click <code>Install</code>.</p>
<p>Once <code>ElemStatLearn</code> has been installed, we can load the package and data set in the usual way:</p>
<pre class="r"><code>## Load R package:
library(ElemStatLearn)
## Load data into R:
data(prostate)
## Print the first 5 rows:
head(prostate, 5)</code></pre>
<pre><code>##       lcavol  lweight age      lbph svi       lcp gleason pgg45       lpsa
## 1 -0.5798185 2.769459  50 -1.386294   0 -1.386294       6     0 -0.4307829
## 2 -0.9942523 3.319626  58 -1.386294   0 -1.386294       6     0 -0.1625189
## 3 -0.5108256 2.691243  74 -1.386294   0 -1.386294       7    20 -0.1625189
## 4 -1.2039728 3.282789  58 -1.386294   0 -1.386294       6     0 -0.1625189
## 5  0.7514161 3.432373  62 -1.386294   0 -1.386294       6     0  0.3715636
##   train
## 1  TRUE
## 2  TRUE
## 3  TRUE
## 4  TRUE
## 5  TRUE</code></pre>
<p>Our response variable is the log PSA, <code>lpsa</code>, in column 9. Columns 1 to 8 contain the 8 clinical measures which will serve as our predictor variables. Note that the final column, column 10, is not of interest for now and we will remove it:</p>
<pre class="r"><code>ProstateData = prostate[,-10]
## Store the number observations and number of predictors:
n = nrow(ProstateData); p = ncol(ProstateData) - 1</code></pre>
<p>To get a feel for the relationships among the predictor variables and between the predictor variables and the response variable, we can produce a scatterplot matrix, as you saw towards the end of last week:</p>
<pre class="r"><code>pairs(ProstateData)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:prostate-pairs"></span>
<img src="Weeks6to9new_files/figure-html/prostate-pairs-1.png" alt="Scatterplot matrix for prostate cancer data." width="576" />
<p class="caption">
Figure 9.1: Scatterplot matrix for prostate cancer data.
</p>
</div>
<p>This produces the plot in Figure <a href="#fig:prostate-pairs">9.1</a>. Focusing on the bottom row where <code>lpsa</code> is plotted on the <span class="math inline">\(y\)</span>-axis, we see some variables, like <code>lcavol</code> have a strong (positive) linear relationship with <code>lpsa</code> while for other variables, like <code>age</code>, the (positive) linear relationship is much weaker. If we now focus on column 5 where <code>svi</code> is plotted on the <span class="math inline">\(x\)</span>-axis we see that <code>svi</code> can only take two possible values – 0 and 1. Similarly, if we look at column 7 where <code>gleason</code> is plotted on the <span class="math inline">\(x\)</span>-axis, we see that there are only four values for <code>gleason</code> represented in these data – 6, 7, 8 and 9. We can confirm these observations using the <code>unique</code> function which returns the unique elements in a vector:</p>
<pre class="r"><code>## Possible values for svi:
unique(prostate$svi)</code></pre>
<pre><code>## [1] 0 1</code></pre>
<pre class="r"><code>## Possible values for gleason:
unique(prostate$gleason)</code></pre>
<pre><code>## [1] 6 7 8 9</code></pre>
<p>In fact, <code>svi</code> is a categorical variable with two possible values, which we normally refer to as its “levels”, indicating whether or not the seminal vesicles have been invaded by prostate cancer. The variable <code>gleason</code> is an ordered categorical variable which, for cancers, has five possible levels labelled 6, 7, 8, 9 and 10, with larger values indicating more aggressive cancer. Note from the R output above that we do not observe any patient with a gleason score of 10 in this data set.</p>
<p>As discussed in the “Dummy Variables” video in Workshop 5, we can incorporate a categorical predictor variable with <span class="math inline">\(k\)</span> levels by introducing <span class="math inline">\(k-1\)</span> dummy or indicator variables. For <code>svi</code> we can therefore choose “no seminal invasion” as the baseline and introduce an invasion indicator variable <span class="math inline">\(x_{5}\)</span> defined through:
<span class="math display">\[\begin{equation*}
x_{5} =
\begin{cases}
1, \quad &amp;\text{if seminal invasion observed;}\\
0, \quad &amp;\text{otherwise.}
\end{cases}
\end{equation*}\]</span>
If we look at the output of <code>unique(prostate$svi)</code> above, we see that <code>svi</code> has already been encoded as an indicator variable. In principle we could proceed in a similar fashion for <code>gleason</code>, choosing level 6 as the baseline and introducing four indicator variables – one for each of levels 7 to 10. However, <code>gleason</code> is an ordered categorical variable and so, in this case, we would expect <code>lpsa</code> to only increase or only decrease as the <code>gleason</code> level goes up. Indeed, if we look at the <span class="math inline">\((9,7)\)</span>-panel in Figure <a href="#fig:prostate-pairs">9.1</a>, we see that there is a weak (positive) linear relationship between <code>gleason</code> and <code>lpsa</code>. Therefore we can reasonably treat the <code>gleason</code> level as a quantitative variable. This leads to a simpler model because it means we have only a single explanatory variable representing the effect of <code>gleason</code> and not four. We will therefore leave <span class="math inline">\(x_{7}\)</span> as it is. Consequently the first 8 columns of <code>ProstateData</code> can be used as our predictor variables with no pre-processing.</p>
<p>As seen in the previous part of the course, we can fit a multiple linear regression model using the <code>lm</code> function:</p>
<pre class="r"><code>## Fit linear model:
lsq_fit = lm(lpsa ~ ., data=ProstateData)
## Summarise model fit:
summary(lsq_fit)</code></pre>
<pre><code>## 
## Call:
## lm(formula = lpsa ~ ., data = ProstateData)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.76644 -0.35510 -0.00328  0.38087  1.55770 
## 
## Coefficients:
##              Estimate Std. Error t value      Pr(&gt;|t|)    
## (Intercept)  0.181561   1.320568   0.137       0.89096    
## lcavol       0.564341   0.087833   6.425 0.00000000655 ***
## lweight      0.622020   0.200897   3.096       0.00263 ** 
## age         -0.021248   0.011084  -1.917       0.05848 .  
## lbph         0.096713   0.057913   1.670       0.09848 .  
## svi          0.761673   0.241176   3.158       0.00218 ** 
## lcp         -0.106051   0.089868  -1.180       0.24115    
## gleason      0.049228   0.155341   0.317       0.75207    
## pgg45        0.004458   0.004365   1.021       0.31000    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.6995 on 88 degrees of freedom
## Multiple R-squared:  0.6634, Adjusted R-squared:  0.6328 
## F-statistic: 21.68 on 8 and 88 DF,  p-value: &lt; 0.00000000000000022</code></pre>
<p>Notice that the syntax <code>lpsa ~ .</code> means that <code>lpsa</code> is the response variable and it is regressed on all other columns in the <code>ProstateData</code> data frame, i.e. all other columns are taken as predictor variables. Examining the output of the <code>summary</code> function we see that the coefficient of determination is given by <span class="math inline">\(R^2=0.6634\)</span>. We interpret this to mean that 66.34% of the variation in log PSA can be explained by regression on the eight predictors. This leaves a relatively large proportion of the variation unexplained, i.e. attributed to random error. Inspecting the coefficient table, we see that, conditional on inclusion of all other predictor variables, the <span class="math inline">\(t\)</span>-tests:
<span class="math display">\[\begin{equation*}
H_0: \beta_i = 0 \quad \text{versus} \quad H_1: \beta_i \ne 0
\end{equation*}\]</span>
for <code>age</code>, <code>lbph</code>, <code>lcp</code>, <code>gleason</code> and <code>pgg45</code> all have large <span class="math inline">\(p\)</span>-values. This suggests that if we consider them one a time, each of these predictors contributes little to a model that already contains the other seven predictor variables. Therefore we are unlikely to need to include all of them.</p>
<p>But why might we want to get rid of some predictor variables? There are a number of reasons:</p>
<ul>
<li><strong>To improve predictive performance</strong>: Suppose the model is fitted using least squares, which is the technique introduced in the first half of this module. Typically when we have a large number of predictors (<span class="math inline">\(p\)</span>) relative to the number of observations (<span class="math inline">\(n\)</span>) we can’t estimate their coefficients very precisely using least squares and so the variance of the least squares estimator is large. Recall that the variance of the least squares estimator tells us how much our estimates would vary if we could repeatedly take samples from the population regression model and recompute the least squares estimates. When the variance is large this means our estimates of the regression coefficients would vary widely. Consequently, so too would our predictions from the fitted model using future data that were not used in model-fitting. This corresponds to poor predictive performance.</li>
<li><strong>To improve model interpretability</strong>: A model with fewer predictors is easier to interpret and use for generating predictions using future data. It can therefore be helpful to eliminate predictor variables which are not associated with the response given the other predictors in the model.</li>
</ul>
<p>There are classic methods for deciding on how to eliminate sets of predictors, for example, by applying the <em>extra sum of squares principle</em>. In this module we will consider a method called <strong>best-subset selection</strong> which belongs to a general class of techniques called <em>variable selection</em> or <em>feature selection</em> methods.</p>
</div>
<div id="subsec:bss" class="section level2" number="9.2">
<h2><span class="header-section-number">9.2</span> Best-Subset Selection</h2>
<p>The main idea behind variable selection methods is that if we can identify a “good” subset of <span class="math inline">\(p^{\ast}&lt;p\)</span> predictor variables, then we can learn the effects of our (reduced set of) predictor variables more precisely. This reduces the variance in our parameter estimates and can therefore improve predictive performance.</p>
<p>As its name suggests, <strong>best subset selection</strong> involves using least squares to fit a linear regression model to each possible subset of the <span class="math inline">\(p\)</span> explanatory variables. So we would fit the so-called <em>null model</em> with no predictors (i.e. just an intercept), all <span class="math inline">\(p\)</span> models with a single predictor, all <span class="math inline">\(p(p-1)/2\)</span> models with two predictors, and so on. It turns out that if we continue in this fashion there are <span class="math inline">\(2^p\)</span> possible models. We compare all of them to decide which one is “best”. The full procedure is as follows:</p>
<ol style="list-style-type: decimal">
<li>Fit the null model, <span class="math inline">\(\mathcal{M}_0\)</span>, which contains no predictor variables, and is simply <span class="math inline">\(\hat{y} = \bar{y}\)</span>.</li>
<li>For <span class="math inline">\(k=1,2,\ldots,p\)</span>:
<ol style="list-style-type: lower-alpha">
<li>Fit all <span class="math inline">\(\binom{p}{k}\)</span> models that contain exactly <span class="math inline">\(k\)</span> predictor variables.</li>
<li>Select the model amongst these which has the smallest residual sum of squares <span class="math inline">\(SSE\)</span>, or equivalently, the largest coefficient of determination <span class="math inline">\(R^2\)</span>. Call this model <span class="math inline">\(\mathcal{M}_k\)</span>.</li>
</ol></li>
<li>Select a single “best” model from amongst <span class="math inline">\(\mathcal{M}_0, \mathcal{M}_1, \ldots, \mathcal{M}_p\)</span>.</li>
</ol>
<p>To compare linear models which contain the same number of predictor variables, we can simply use the coefficient of determination <span class="math inline">\(R^2\)</span>, selecting the model which maximises this statistic. This is what we do in step 2(b) above. However, in step 3, when comparing models with different numbers of predictor variables, if we were to pick the one which maximised <span class="math inline">\(R^2\)</span>, we would always select <span class="math inline">\(\mathcal{M}_p\)</span>, which contains all the predictors. This is because <span class="math inline">\(R^2\)</span> cannot decrease as predictor variables are added to the model. To get around this problem, a few alternative statistics have been proposed to compare models with different numbers of predictor variables. They all work by trading off terms which reward good model-fit (typically based on <span class="math inline">\(R^2\)</span> or <span class="math inline">\(SSE\)</span>) with terms that penalise model complexity. For a model with <span class="math inline">\(k\)</span> predictors we can compute, for example:</p>
<ul>
<li><strong>Adjusted <span class="math inline">\(R^2\)</span>:</strong> defined by<br />
<span class="math display">\[\begin{equation*}
R^2_{\text{adj}} = 1 - (1 - R^2) \frac{n - 1}{n - k - 1}
\end{equation*}\]</span>
which adjusts <span class="math inline">\(R^2\)</span> to penalise model complexity (i.e. large <span class="math inline">\(k\)</span>). We would choose the model for which <span class="math inline">\(R^2_{\text{adj}}\)</span> is largest.</li>
<li><strong>Mallow’s <span class="math inline">\(C_p\)</span> statistic:</strong> equal to <span class="math inline">\(SSE / n\)</span> plus a penalty for model complexity. We would choose the model for which <span class="math inline">\(C_p\)</span> is smallest.</li>
<li><strong>Bayes Information Criterion (BIC):</strong> equal to <span class="math inline">\(SSE / n\)</span> plus a (different) penalty for model complexity. The penalty for BIC tends to penalise models with lots of explanatory variables more heavily than the penalty for Mallow’s <span class="math inline">\(C_p\)</span> statistic and so often favours simpler models. We would choose the model for which BIC is smallest.</li>
</ul>
<p>Another option is to use <span class="math inline">\(k\)</span>-fold cross-validation to pick the number of predictors. The general method is introduced in Section <a href="#subsec:kfoldcrossvalidation">11.3</a> and its use in this context is explored in computer labs.</p>
<p>In practice, the different statistics often suggest different models are “best” and so it is usually a good idea to consider more than one measure and look for some sort of consensus.</p>
<div id="subsubsec:prostatecancerdatabss" class="section level3" number="9.2.1">
<h3><span class="header-section-number">9.2.1</span> Example: Prostate Cancer Data Continued</h3>
<p>Consider again the prostate cancer data which we examined in Section <a href="#subsec:prostatecancerdata">9.1</a>. We are going to use R to apply the best subset selection approach. To do this we will use the <code>regsubsets</code> function from the <code>leaps</code> package. We begin by loading the package:</p>
<pre class="r"><code>library(leaps)</code></pre>
<pre><code>## Warning: package &#39;leaps&#39; was built under R version 4.3.2</code></pre>
<p>Then we apply the <code>regsubsets</code> function. The syntax is almost identical to that for the <code>lm</code> function except we should also specify that we want to use best subset regression by setting the <code>method</code> argument equal to <code>"exhaustive"</code>. Also, we need to use the <code>nvmax</code> argument to specify the size of the largest subset we wish to consider. In general this should be the number of predictors, <span class="math inline">\(p\)</span>.</p>
<pre class="r"><code>bss = regsubsets(lpsa ~ ., data=ProstateData, method=&quot;exhaustive&quot;, nvmax=p)</code></pre>
<p>By applying the <code>summary</code> function to the returned object, we can see which models were identified as <span class="math inline">\(\mathcal{M}_1, \ldots, \mathcal{M}_8\)</span>:</p>
<pre class="r"><code>(bss_summary = summary(bss))</code></pre>
<pre><code>## Subset selection object
## Call: regsubsets.formula(lpsa ~ ., data = ProstateData, method = &quot;exhaustive&quot;, 
##     nvmax = p)
## 8 Variables  (and intercept)
##         Forced in Forced out
## lcavol      FALSE      FALSE
## lweight     FALSE      FALSE
## age         FALSE      FALSE
## lbph        FALSE      FALSE
## svi         FALSE      FALSE
## lcp         FALSE      FALSE
## gleason     FALSE      FALSE
## pgg45       FALSE      FALSE
## 1 subsets of each size up to 8
## Selection Algorithm: exhaustive
##          lcavol lweight age lbph svi lcp gleason pgg45
## 1  ( 1 ) &quot;*&quot;    &quot; &quot;     &quot; &quot; &quot; &quot;  &quot; &quot; &quot; &quot; &quot; &quot;     &quot; &quot;  
## 2  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot; &quot; &quot; &quot;  &quot; &quot; &quot; &quot; &quot; &quot;     &quot; &quot;  
## 3  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot; &quot; &quot; &quot;  &quot;*&quot; &quot; &quot; &quot; &quot;     &quot; &quot;  
## 4  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot; &quot; &quot;*&quot;  &quot;*&quot; &quot; &quot; &quot; &quot;     &quot; &quot;  
## 5  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot; &quot;*&quot;  &quot;*&quot; &quot; &quot; &quot; &quot;     &quot; &quot;  
## 6  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot; &quot;*&quot;  &quot;*&quot; &quot; &quot; &quot; &quot;     &quot;*&quot;  
## 7  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot; &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot; &quot;     &quot;*&quot;  
## 8  ( 1 ) &quot;*&quot;    &quot;*&quot;     &quot;*&quot; &quot;*&quot;  &quot;*&quot; &quot;*&quot; &quot;*&quot;     &quot;*&quot;</code></pre>
<p>In the above output the asterisk indicates that a variable is included in the model. So, for example, the best model with one predictor variable, <span class="math inline">\(\mathcal{M}_1\)</span>, uses <code>lcavol</code>; the best model with two predictor variables, <span class="math inline">\(\mathcal{M}_2\)</span>, uses <code>lcavol</code> and <code>lweight</code>; and so on.</p>
<p>In addition to the output printed to the screen, the <code>summary</code> function also computes a number of statistics to help us choose the best overall model. This includes the three discussed above: adjusted <span class="math inline">\(R^2\)</span>, Mallow’s <span class="math inline">\(C_p\)</span> statistic and the BIC which can be accessed as follows:</p>
<pre class="r"><code>## Adjusted Rsq:
bss_summary$adjr2</code></pre>
<pre><code>## [1] 0.5345839 0.5868977 0.6242063 0.6280585 0.6335279 0.6349654 0.6365002
## [8] 0.6327886</code></pre>
<pre class="r"><code>## Mallow&#39;s Cp statistic:
bss_summary$cp</code></pre>
<pre><code>## [1] 27.406210 14.747299  6.173546  6.185065  5.816804  6.466493  7.100428
## [8]  9.000000</code></pre>
<pre class="r"><code>## BIC:
bss_summary$bic</code></pre>
<pre><code>## [1] -66.05416 -74.07188 -79.71614 -77.18955 -75.11192 -71.99028 -68.90809
## [8] -64.44401</code></pre>
<p>The optimal value of <span class="math inline">\(k\)</span> in each case is therefore:</p>
<pre class="r"><code>(best_adjr2 = which.max(bss_summary$adjr2))</code></pre>
<pre><code>## [1] 7</code></pre>
<pre class="r"><code>(best_cp = which.min(bss_summary$cp))</code></pre>
<pre><code>## [1] 5</code></pre>
<pre class="r"><code>(best_bic = which.min(bss_summary$bic))</code></pre>
<pre><code>## [1] 3</code></pre>
<p>To help us decide on the “best” model, we can assess graphically how each statistic varies with the number of predictor variables <span class="math inline">\(k\)</span> via:</p>
<pre class="r"><code>## Create multi-panel plotting device:
par(mfrow=c(1,3))
## Produce plots, highlighting optimal value of k:
plot(1:8, bss_summary$adjr2, xlab=&quot;Number of predictors&quot;, ylab=&quot;Adjusted Rsq&quot;, 
     type=&quot;b&quot;)
points(best_adjr2, bss_summary$adjr2[best_adjr2], col=&quot;red&quot;, pch=16)
plot(1:8, bss_summary$cp, xlab=&quot;Number of predictors&quot;, ylab=&quot;Cp&quot;, type=&quot;b&quot;)
points(best_cp, bss_summary$cp[best_cp], col=&quot;red&quot;, pch=16)
plot(1:8, bss_summary$bic, xlab=&quot;Number of predictors&quot;, ylab=&quot;BIC&quot;, type=&quot;b&quot;)
points(best_bic, bss_summary$bic[best_bic], col=&quot;red&quot;, pch=16)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bss"></span>
<img src="Weeks6to9new_files/figure-html/bss-1.png" alt="Best subset selection for the prostate cancer data." width="\linewidth" />
<p class="caption">
Figure 9.2: Best subset selection for the prostate cancer data.
</p>
</div>
<p>which generates the plot in Figure <a href="#fig:bss">9.2</a>.</p>
<p>Unfortunately, as very often occurs in practice, the different methods select different models. Adjusted <span class="math inline">\(R^2\)</span> suggests <span class="math inline">\(\mathcal{M}_7\)</span>, whilst Mallow’s <span class="math inline">\(C_p\)</span> statistic and the BIC suggest <span class="math inline">\(\mathcal{M}_5\)</span> and <span class="math inline">\(\mathcal{M}_3\)</span>, respectively. However, if we examine the plot for adjusted <span class="math inline">\(R^2\)</span>, it suggests there is little difference between models <span class="math inline">\(\mathcal{M}_3, \ldots, \mathcal{M}_8\)</span>, whilst the plot for Mallow’s <span class="math inline">\(C_p\)</span> statistic suggests there is little difference between models <span class="math inline">\(\mathcal{M}_3, \ldots, \mathcal{M}_7\)</span>. Therefore we might regard the best model as <span class="math inline">\(\mathcal{M}_3\)</span> (which includes <code>lcavol</code>, <code>lweight</code> and <code>svi</code>).</p>
<p>In order to obtain the least squares estimates of the coefficients for one of the models, say, <span class="math inline">\(\mathcal{M}_3\)</span>, we can use the <code>coef</code> function:</p>
<pre class="r"><code>coef(bss, 3)</code></pre>
<pre><code>## (Intercept)      lcavol     lweight         svi 
##  -0.7771566   0.5258519   0.6617699   0.6656666</code></pre>
<p>where the second argument refers to the number of explanatory variables <span class="math inline">\(k\)</span> in the model of interest <span class="math inline">\(\mathcal{M}_k\)</span>.</p>
<p>Although conceptually appealing, the main problem with best-subset selection is that the number of models to be considered grows very fast with the number of predictor variables <span class="math inline">\(p\)</span>. For example, for the prostate cancer data we had <span class="math inline">\(p=8\)</span> predictor variables leading to <span class="math inline">\(2^p = 2^8 = 256\)</span> possible models. If <span class="math inline">\(p=16\)</span>, we would have needed to consider <span class="math inline">\(65 \, 536\)</span> models, and if <span class="math inline">\(p=32\)</span>, then there would have been over four billion! As a result, best subset selection becomes computationally infeasible for values of <span class="math inline">\(p\)</span> greater than around 40. You will study other techniques that are more appropriate when <span class="math inline">\(p\)</span> is large (or large relative to <span class="math inline">\(n\)</span>) in the Machine Learning module.</p>
</div>
</div>
</div>
<div id="assessing-predictive-error" class="section level1" number="10">
<h1><span class="header-section-number">10</span> Assessing Predictive Error</h1>
<p>A popular approach for assessing and comparing supervised learning techniques, which is particularly popular in the machine learning literature, is to base the judgement on their predictive performance. In other words on the extent to which the predicted value of an “output” variable for a particular individual / item matches the value we actually observe. In this week’s lecture-workshops we will consider application of these ideas in the context of linear regression models. Next week, we will consider their application in the context of classification methods where we have a categorical, rather than quantitative, “output” variable, e.g. disease status.</p>
<p>In judging the predictive performance of a supervised learning method, we need to distinguish between two kinds of error:</p>
<ul>
<li><strong>Test error:</strong> the average error that results from predicting the response for an observation that was not used in model-fitting. This is sometimes called <strong>out-of-sample validation</strong>. The data used in model-fitting are called <strong>training data</strong>. The data used to assess the predictive performance are called <strong>validation data</strong> or <strong>test data</strong>.</li>
<li><strong>Training error:</strong> the average error that results from predicting the response for an observation that was used in model-fitting. This is called <strong>in-sample validation</strong> and uses only training data.</li>
</ul>
<p>It is generally easier to perform in-sample validation because we use the same data to fit and validate the model. In a regression context, common measures of the training error include the residual sum of squares <span class="math inline">\(SSE\)</span> or, equivalently, the coefficient of determination <span class="math inline">\(R^2\)</span>. However, the most commonly presented measure of training error is the average <strong>mean squared error (MSE)</strong> over the training data:
<span class="math display">\[\begin{equation*}
MSE = \frac{1}{n} \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \frac{1}{n} SSE
\end{equation*}\]</span>
where the fitted value <span class="math inline">\(\hat{y}_i\)</span> is given by
<span class="math display">\[\begin{equation*}
\hat{y}_i = \hat{\beta}_0 +  \hat{\beta}_1 x_{i1} + \ldots + \hat{\beta}_p x_{ip}, \qquad \text{for $i=1,\ldots,n$.}
\end{equation*}\]</span>
For example, we can compute the training error associated with the 3-predictor model identified in Section <a href="#subsubsec:prostatecancerdatabss">9.2.1</a> as follows:</p>
<pre class="r"><code>## Fit the three-predictor model:
lsq_fit_3 = lm(lpsa ~ lcavol + lweight + svi, data=prostate)
## Compute fitted values:
yhat = predict(lsq_fit_3, prostate)
head(yhat)</code></pre>
<pre><code>##         1         2         3         4         5         6 
## 0.7506893 0.8968425 0.7352084 0.7621830 1.8894181 0.8075323</code></pre>
<pre class="r"><code>## Compute training error:
training_error = mean((prostate$lpsa - yhat)^2)
training_error</code></pre>
<pre><code>## [1] 0.480087</code></pre>
<p>Although easy to compute, we’re typically not very interested in the training error. What we’re really interested in is the test error because this measures how well the method performs on previously unseen data and is therefore a more faithful characterisation of the model’s predictive performance. Unfortunately, the training error is typically an underestimate of the test error, essentially due to its double-use of the data for constructing <em>and</em> testing the model. The test error can be estimated in a number of ways. In the context of linear regression, the methods we considered in the previous section for comparing models with different numbers of predictors – adjusted <span class="math inline">\(R^2\)</span>, Mallow’s <span class="math inline">\(C_p\)</span> statistic and the BIC – can be regarded as measures of test error; they work by making a mathematical adjustment to the training error rate so that it estimates the test error rate. However, a more common method for estimating the test error, which works more generally for all regression and classification methods, is a class of resampling methods called <strong>cross-validation</strong>. This is the subject of the following section.</p>
</div>
<div id="sec:crossvalidation" class="section level1" number="11">
<h1><span class="header-section-number">11</span> Cross-Validation</h1>
<div id="the-validation-set-approach" class="section level2" number="11.1">
<h2><span class="header-section-number">11.1</span> The Validation Set Approach</h2>
<p>Before introducing the cross-validation approach, we will consider a simpler out-of-sample validation method called the <strong>validation set approach</strong>. The idea is to split the data into two: the training data and the validation (or test) data. The training data is used to estimate the model parameters to give the fitted model. Then the test data is used to compute the test error. For ease of explanation, suppose we are fitting a simple linear regression model:
<span class="math display">\[\begin{equation*}
Y = \beta_0 +  \beta_1 x + \epsilon.
\end{equation*}\]</span>
Suppose further that our training data comprise <span class="math inline">\((x_1, y_1), \ldots, (x_{n_1}, y_{n_1})\)</span> where <span class="math inline">\(n_1 &lt; n\)</span> and that our test data comprise <span class="math inline">\((x_{n_1+1}, y_{n_1+1}), \ldots, (x_n, y_{n})\)</span>. By fitting the model using the training data, we obtain estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> of the regression coefficients, and then make predictions over the test set using
<span class="math display">\[\begin{equation*}
\hat{y}_i = \hat{\beta}_0 +  \hat{\beta}_1 x_{i}, \qquad \text{for $i=n_1+1,\ldots,n$.}
\end{equation*}\]</span>
Finally, we estimate the test mean-squared-error (<span class="math inline">\(MSE\)</span>) with
<span class="math display">\[\begin{equation*}
VS = \frac{1}{n-n_1} \sum_{i=n_1+1}^n (y_i - \hat{y}_i)^2
\end{equation*}\]</span>
in which <span class="math inline">\(n-n_1\)</span> is the number of observations in the test set.</p>
<p>For our prostate cancer data, for example, we could sample our training and test sets as follows:</p>
<pre class="r"><code>## Set the seed to make the analysis reproducible
set.seed(1)
## Randomly sample a set of indices for the training data
train_set = sample(c(TRUE, FALSE), n, replace=TRUE)
## Print first 9 elements
head(train_set, 9)</code></pre>
<pre><code>## [1]  TRUE FALSE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE FALSE</code></pre>
<pre class="r"><code>## Deduce indices of test data
test_set = !train_set</code></pre>
<p>So the indices for the training data are the positions where we have <code>TRUE</code> in <code>train_set</code> and the indices for the test data are the positions where we have a <code>FALSE</code> in <code>train_set</code> and hence a <code>TRUE</code> in <code>test_set</code>:</p>
<pre class="r"><code>head(which(train_set==TRUE)) ## or, more concisely: head(which(train_set))</code></pre>
<pre><code>## [1] 1 3 4 6 7 8</code></pre>
<pre class="r"><code>head(which(test_set==TRUE))  ## or, more concisely: head(which(test_set))</code></pre>
<pre><code>## [1]  2  5  9 10 16 17</code></pre>
<p>We now estimate the test error by fitting the model to the training data and calculating the fitted values for the test data. We begin by considering the full model with 8 predictors:</p>
<pre class="r"><code>## Fit full model using training data
lsq_train = lm(lpsa ~ ., data=ProstateData[train_set,])
## Compute fitted values for test data:
yhat_test = predict(lsq_train, ProstateData[test_set,])
head(yhat_test)</code></pre>
<pre><code>##         2         5         9        10        16        17 
## 0.4616526 1.4987918 0.8332913 1.0875066 1.7511600 1.1634700</code></pre>
<pre class="r"><code>## Compute test error
test_error = mean((ProstateData[test_set,]$lpsa - yhat_test)^2)
test_error</code></pre>
<pre><code>## [1] 0.6992266</code></pre>
<p>Repeating this procedure for the 3-predictor model identified in Section <a href="#subsubsec:prostatecancerdatabss">9.2.1</a> we find</p>
<pre class="r"><code>## Fit 3-predictor model using training data
lsq_train_3 = lm(lpsa ~ lcavol + lweight + svi, data=ProstateData[train_set,])
## Compute fitted values for test data:
yhat_test_3 = predict(lsq_train_3, ProstateData[test_set,])
## Compute test error
test_error_3 = mean((ProstateData[test_set,]$lpsa - yhat_test_3)^2)
test_error_3</code></pre>
<pre><code>## [1] 0.6014814</code></pre>
<p>The test error for the 3-predictor model is lower than that for the full model, indicating better predictive performance. This is as expected since the model comparison criteria we considered previously, like adjusted-<span class="math inline">\(R^2\)</span>, are also intended to measure test error and they suggested the 3-predictor model was better than the full model. Note that we use the same split of the data into training and validation sets for the 3-predictor model and full model to make the comparison as fair as possible.</p>
<p>Although straightforward to implement, the validation set approach suffers from two drawbacks:</p>
<ol style="list-style-type: decimal">
<li>If we repeat this procedure using a different split of the data, we will get different results. For example, consider the 3-predictor model where our previous estimate of the test error was 0.6014814:</li>
</ol>
<pre class="r"><code>## Sample a new set of training and test indices
train_set = sample(c(TRUE, FALSE), n, replace=TRUE)
test_set = !train_set
## Fit 3-predictor model using training data
lsq_train_3 = lm(lpsa ~ lcavol + lweight + svi, data=ProstateData[train_set,])
## Compute fitted values for test data
yhat_test_3 = predict(lsq_train_3, ProstateData[test_set,])
## Compute test error
test_error_3 = mean((ProstateData[test_set,]$lpsa - yhat_test_3)^2)
test_error_3</code></pre>
<pre><code>## [1] 0.572272</code></pre>
<div class="line-block">       Sometimes the test error rate can vary substantially between splits which makes it difficult to draw conclusions.</div>
<ol start="2" style="list-style-type: decimal">
<li>Only a subset of the observations are being used to fit the model which might make its performance appear unduly poor, i.e. the test error is overestimated.</li>
</ol>
<p>We can address these problems using another out-of-sample validation approach called <strong>cross-validation</strong>. We will consider two variants in the following sections.</p>
</div>
<div id="leave-one-out-cross-validation" class="section level2" number="11.2">
<h2><span class="header-section-number">11.2</span> Leave-One-Out Cross-Validation</h2>
<p>Leave-one-out cross-validation (LOOCV) is similar to the validation set approach in that it involves splitting the observations into two parts. However, this splitting procedure is now performed <span class="math inline">\(n\)</span> times. For ease of explanation, suppose we are, again, fitting a simple linear regression model. The first split takes the single observation <span class="math inline">\((x_1, y_1)\)</span> as the test data and the remaining <span class="math inline">\(n-1\)</span> observations as training data: <span class="math inline">\((x_2, y_2), \ldots, (x_n, y_n)\)</span>. After fitting the model using the training data, we make a prediction <span class="math inline">\(\hat{y}_1\)</span> over the test set and then estimate the test error with
<span class="math display">\[\begin{equation*}
MSE_1 = (y_1 - \hat{y}_1)^2.
\end{equation*}\]</span>
Next, we repeat the procedure taking <span class="math inline">\((x_2, y_2)\)</span> as our test data and the remaining <span class="math inline">\(n-1\)</span> observations as our training data: <span class="math inline">\((x_1, y_1), (x_3, y_3), \ldots, (x_n, y_n)\)</span>. This yields our second estimate of the test error as
<span class="math display">\[\begin{equation*}
MSE_2 = (y_2 - \hat{y}_2)^2.
\end{equation*}\]</span>
Repeating this procedure <span class="math inline">\(n\)</span> times generates <span class="math inline">\(n\)</span> estimates of the test error: <span class="math inline">\(MSE_1, MSE_2, \ldots, MSE_n\)</span> which are averaged to produced the overall LOOCV estimate of the test <span class="math inline">\(MSE\)</span>:
<span class="math display">\[\begin{equation*}
CV_{(n)} = \frac{1}{n} \sum_{i=1}^n MSE_i.
\end{equation*}\]</span></p>
<p>For example, in the prostate cancer example, we can write some code to produce the LOOCV estimate of the test <span class="math inline">\(MSE\)</span> for our full model as follows:</p>
<pre class="r"><code>## Create a vector to store the test error estimates:
test_errors = numeric(n)
## Estimate the test MSE based on all n splits of the data:
for(i in 1:n) {
  ## Fit the model using the training data:
  lsq_train = lm(lpsa ~ ., data=ProstateData[-i,])
  ## Compute fitted values over the test set:
  yhat_test = predict(lsq_train, ProstateData[i,])
  ## Compute estimate of MSE
  test_errors[i] = (yhat_test - ProstateData[i,]$lpsa)^2
}
## Average MSE_1, ..., MSE_n to produce the overall estimate of test MSE:
(LOOCV = mean(test_errors))</code></pre>
<pre><code>## [1] 0.5413291</code></pre>
<p>Repeating this for our 3-predictor model yields:</p>
<pre class="r"><code>## Create a vector to store the test error estimates:
test_errors_3 = numeric(n)
## Estimate the test MSE based on all n splits of the data:
for(i in 1:n) {
  ## Fit the model using the training data:
  lsq_train = lm(lpsa ~ lcavol + lweight + svi, data=ProstateData[-i,])
  ## Compute fitted values over the test set:
  yhat_test = predict(lsq_train, ProstateData[i,])
  ## Compute estimate of MSE
  test_errors_3[i] = (yhat_test - ProstateData[i,]$lpsa)^2
}
## Average MSE_1, ..., MSE_n to produce the overall estimate of test MSE:
(LOOCV_3 = mean(test_errors_3))</code></pre>
<pre><code>## [1] 0.5255275</code></pre>
<p>Again, the test error estimated using LOOCV suggests the 3-predictor model is better than the full model.</p>
<p>The main advantage of LOOCV over the validation-set approach is that each set of training data contains <span class="math inline">\(n-1\)</span> observations and so we typically estimate the regression coefficients more precisely than we did in the validation-set approach where our (single) training set was smaller. This means we can typically estimate the test error with less bias, i.e. it tends to be less of an overestimate.</p>
<p>However, there are two major disadvantages with LOOCV:</p>
<ol style="list-style-type: decimal">
<li><p>It can be computationally expensive to implement because we have to fit the model <span class="math inline">\(n\)</span> times. Obviously, this will be a problem if <span class="math inline">\(n\)</span> is large or if each individual model is slow to fit. For some supervised learning methods, the time taken to fit an individual model can take minutes, hours, or even days, which can render LOOCV computationally infeasible. (Linear regression can actually be regarded as an exception here. Although we will not dwell on the mathematical details and did not use it above, there is actually a formula for <span class="math inline">\(CV_{(n)}\)</span> which makes the cost of calculating the LOOCV estimate of the test error the same as that of a single model-fit. However, it does not work for other supervised learning methods and so we will not consider it further.)</p></li>
<li><p>As a method of estimating the test error, LOOCV has a high variance. This is because the <span class="math inline">\(n\)</span> training sets used to generate the <span class="math inline">\(n\)</span> estimates of the test error are almost identical and so these individual estimates are highly positively correlated.</p></li>
</ol>
<p>We can address both of these limitations using another cross-validation method called <span class="math inline">\(k\)</span>-fold cross-validation.</p>
</div>
<div id="subsec:kfoldcrossvalidation" class="section level2" number="11.3">
<h2><span class="header-section-number">11.3</span> <span class="math inline">\(k\)</span>-fold Cross-Validation</h2>
<p>In <span class="math inline">\(k\)</span>-fold cross validation we randomly divide the data into <span class="math inline">\(k\)</span> groups or <strong>folds</strong> of approximately equal size. The last <span class="math inline">\(k-1\)</span> folds are used as the training data then the first fold is used as a test set. We compute the test error rate based on this first fold. The process is then repeated using the second fold as the test set, then the third fold, and so on.</p>
<p>Eventually this procedure gives us <span class="math inline">\(k\)</span> estimates of the test error rate which are averaged to give the overall test error. In linear regression, for example, we would obtain <span class="math inline">\(k\)</span> estimates of the test mean-squared error <span class="math inline">\(MSE_1, \ldots, MSE_k\)</span> and then compute the overall <span class="math inline">\(k\)</span>-fold cross-validation estimate of the test MSE through
<span class="math display">\[\begin{equation*}
CV_{(k)} = \frac{1}{k} \sum_{i=1}^k MSE_i.
\end{equation*}\]</span>
It should be clear that LOOCV is a special case of <span class="math inline">\(k\)</span>-fold cross-validation when <span class="math inline">\(k=n\)</span>. However, typical choices for the number of folds are <span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span> and not <span class="math inline">\(k=n\)</span>. Why is this? First, there are significant computational benefits to choosing <span class="math inline">\(k\)</span> to be a reasonably small number like <span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span>, namely that this only requires fitting the model to the data <span class="math inline">\(5\)</span> or <span class="math inline">\(10\)</span> times, respectively. This can often be computationally feasible in cases when LOOCV is not.</p>
<p>In addition to this computational benefit, there are also mathematical arguments to suggest that <span class="math inline">\(k\)</span>-fold cross validation with <span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span> can lead to more accurate estimates of the test error than LOOCV. As explained in the previous section, the <span class="math inline">\(n\)</span> estimates of the test error in LOOCV tend to be highly correlated because the <span class="math inline">\(n\)</span> training data sets are almost identical. The overlap between training sets is substantially smaller for <span class="math inline">\(k\)</span>-fold cross-validation and so the <span class="math inline">\(k\)</span> estimates of the test error tend to be less positively correlated. The mean of highly correlated quantities has a higher variance than the mean of quantities that are less highly correlated and so the the method of estimating the test error using LOOCV tends to have a higher variance than the method of estimating the test error using <span class="math inline">\(k\)</span>-fold cross-validation. On the other hand, because each training set in <span class="math inline">\(k\)</span>-fold cross-validation contains fewer observations than each training set in LOOCV, for each split of the data, <span class="math inline">\(k\)</span>-fold cross validation can produce slightly biased estimates of the test error.</p>
<p>The choice of <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>-fold cross-validation is therefore said to be a <em>bias-variance-trade-off</em>. There is some empirical evidence to suggest that the choices <span class="math inline">\(k=5\)</span> or <span class="math inline">\(k=10\)</span> yield estimates of the test error that suffer neither from excessively high bias nor from very high variance.</p>
<p>In the computer labs we will write our own R code to perform <span class="math inline">\(k\)</span>-fold cross-validation for linear regression models. We will also use it as an alternative to the statistics proposed in Section <a href="#subsec:bss">9.2</a> for choosing the number of predictors <span class="math inline">\(0,1,\ldots,p\)</span> during best subset selection.</p>
</div>
</div>
<div id="introduction" class="section level1" number="12">
<h1><span class="header-section-number">12</span> Introduction</h1>
<!-- Section \@ref(eq:mean). -->
<div id="key-ideas" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> Key Ideas</h2>
<p>Suppose that individuals (or items) can be divided into <span class="math inline">\(K \ge 2\)</span> groups that can, in principle, be observed, e.g. patients with and without a particular disease. Suppose further that we can take measurements on <span class="math inline">\(p\)</span> variables for each individual. Given a <span class="math inline">\(p\)</span>-variate observation on an individual whose group membership is unknown, the general objective of <strong>classification</strong> is to assign that individual to a group.</p>
<p>Equivalently, for a collection of individuals (or items) suppose that we can take measurements on <span class="math inline">\(p+1\)</span> variables, the last of which is categorical and indicates group membership. Then classification can be regarded as the problem of predicting the value of the categorical variable given the values of the <span class="math inline">\(p\)</span> others. In this framework we would call the categorical variable our <strong>response</strong> variable and the other <span class="math inline">\(p\)</span> variables the <strong>predictor</strong> or <strong>explanatory</strong> variables.</p>
<p>Typically, in classification problems we use a set of “labelled” data (i.e. data where group membership is known) to construct a rule for classifying an “unlabelled” observation (i.e. an observation whose group membership is unknown). Classification can therefore be regarded as a type of <strong>supervised learning</strong> because labelled observations <em>are</em> available to learn about the relationship between observations and group membership. (In the module “Data Exploration, Visualization, and Unsupervised Learning” you will learn about a related method – cluster analysis – in which labelled observations are not available). Therefore, like in multiple linear regression, classification methods (or “classifiers”) are often assessed on the basis of their predictive performance using techniques such as <strong>cross-validation</strong>; see Section .</p>
</div>
<div id="motivating-example---banknote-authentication-data" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> Motivating Example - Banknote Authentication Data</h2>
<p>These data were extracted from 1372 images taken from genuine and forged banknotes. The images were digitised into <span class="math inline">\(400 \times 400\)</span> arrays of pixels and then summarised into four continuously valued summary statistics. For each banknote, the data set records whether the banknote was genuine or forged, along with the four numerical summaries of the image.</p>
<p>Thus, if we were to arrange these data into a <span class="math inline">\(n \times p\)</span> array, we would have</p>
<ul>
<li><span class="math inline">\(n=1372\)</span> rows corresponding to the images;</li>
<li><span class="math inline">\(p=5\)</span> columns corresponding to the variables: the label (genuine / forgery) and the 4 numerical summaries.</li>
</ul>
<p>As there are two possible values for the label for each banknote, <span class="math inline">\(K=2\)</span> here. In this example the purpose of collecting the data was to “train” a classification method to automatically categorise new banknotes whose status (forged or genuine) was not yet known, using information extracted from a digitized image. We shall return to this example in Section
# Model-Based Classification</p>
</div>
<div id="introduction-1" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> Introduction</h2>
<p>In the introduction above we considered two ways of thinking about the classification problem: (i) as a means of partitioning multivariate observations into groups; and (ii) as a means of predicting a categorical response using a collection of predictor variables. The first framework naturally motivates a <strong>discriminant</strong>-based approach to classification whilst the second motivates a <strong>regression</strong>-based approach. We begin this chapter with a brief discussion of both approaches to demonstrate how they are related.</p>
<p>For each individual / item we will assume we can observe values for <span class="math inline">\(Y\)</span> – a categorical random variable that can take one of <span class="math inline">\(K\)</span> possible values – and <span class="math inline">\(X_1,\ldots,X_p\)</span> – <span class="math inline">\(p\)</span> (random) variables which we will assume, for now, are continuous. We focus exclusively on model-based approaches to classification in which our goal will be to come up with a method for computing
<span class="math display">\[\begin{equation*}
\Pr(Y = k | X_1 = x_1, \ldots, X_p = x_p)
\end{equation*}\]</span>
for each possible value, <span class="math inline">\(k = 1,\ldots, K\)</span>, where clearly <span class="math inline">\(\sum_{k=1}^K \Pr(Y = k | X_1 = x_1, \ldots, X_p = x_p) = 1\)</span>. In words, this is</p>
<blockquote>
<p>“What is the probability that <span class="math inline">\(Y\)</span> takes the value <span class="math inline">\(k\)</span> for an individual / item where <span class="math inline">\(X_1\)</span> takes the value <span class="math inline">\(x_1\)</span>, <span class="math inline">\(X_2\)</span> takes the value <span class="math inline">\(x_2\)</span>, and so on up to <span class="math inline">\(X_p\)</span> taking the value <span class="math inline">\(x_p\)</span>”?</p>
</blockquote>
<p>For example, for a collection of patients presenting with a particular complaint, if <span class="math inline">\(Y\)</span> represents whether the patient has a particular disease and <span class="math inline">\(X_1, \ldots, X_p\)</span> represents a set of clinical measurements taken on the patient, we would want to be able to answer the question</p>
<blockquote>
<p>“What is the probability that an individual has the disease if they have clinical measurements of <span class="math inline">\(x_1,\ldots,x_p\)</span>?”</p>
</blockquote>
<p>As such, <span class="math inline">\(Y\)</span> is often regarded as a response variable and <span class="math inline">\(X_1,\ldots,X_p\)</span> as a collection of (random) predictor variables.</p>
</div>
<div id="subsec:regbased" class="section level2" number="12.4">
<h2><span class="header-section-number">12.4</span> Regression-Based Approaches</h2>
<p>The most direct way of specifying the probabilities <span class="math inline">\(\Pr(Y = k | X_1 = x_1, \ldots, X_p = x_p)\)</span> is by constructing a probability model for <span class="math inline">\(Y\)</span> conditional on <span class="math inline">\(X_i\)</span> taking the value <span class="math inline">\(x_i\)</span> for <span class="math inline">\(i=1,\ldots,p\)</span>. If <span class="math inline">\(Y\)</span> could be treated as a continuous variable, we could construct this kind of probability model by using multiple linear regression, assuming a normal distribution for the error terms. This is equivalent to adopting the model
<span class="math display" id="eq:linreg">\[\begin{equation}
Y | X_1 = x_1, \ldots, X_p = x_p \sim \mathrm{N}(\mu, \sigma^2)\tag{12.1}
\end{equation}\]</span>
where the mean <span class="math inline">\(\mu\)</span> depends on the predictor variables <span class="math inline">\(x_1,\ldots,x_p\)</span> through
<span class="math display">\[\begin{equation*}
\mu = \beta_0 + \beta_1 x_{1} + \ldots + \beta_p x_{p}.
\end{equation*}\]</span></p>
<p>However the assumption of a normal distribution in <a href="#eq:linreg">(12.1)</a> will clearly not be appropriate if <span class="math inline">\(Y\)</span> is categorical. Suppose, for simplicity that <span class="math inline">\(K=2\)</span> so that <span class="math inline">\(Y\)</span> admits two possible values, normally labelled as 0 and 1 (we say that <span class="math inline">\(Y\)</span> is <em>binary</em>). In this case, a suitable model for <span class="math inline">\(Y\)</span> is the <strong>Bernoulli distribution</strong>. This is a special case of the Binomial distribution that you studied in “Introduction to Mathematics for Data Science (IMDS)”. A Bernoulli random variable <span class="math inline">\(Y\)</span> takes two possible values – 0 and 1 – and its distribution depends on a single parameter, <span class="math inline">\(\mu\)</span>, called the <em>probability of success</em>, which is a real number that lies between 0 and 1. If <span class="math inline">\(Y\)</span> has a Bernoulli distribution with probability of success <span class="math inline">\(\mu\)</span>, which we write <span class="math inline">\(Y \sim \mathrm{Bern}(\mu)\)</span>, then it has probability mass function
<span class="math display">\[\begin{equation*}
\Pr(Y = 0) = 1 - \mu \quad \text{and} \quad \Pr(Y = 1) = \mu.
\end{equation*}\]</span>
The mean <span class="math inline">\(E(Y)\)</span> of the distribution is defined by
<span class="math display">\[\begin{equation*}
E(Y) = \sum_{y=0}^1 y \Pr(Y = y) = \Pr(Y = 1) = \mu.
\end{equation*}\]</span></p>
<p>Therefore, if our response variable <span class="math inline">\(Y\)</span> is binary, we might simply replace <a href="#eq:linreg">(12.1)</a> with
<span class="math display">\[\begin{equation*}
Y | X_1 = x_1, \ldots, X_p = x_p \sim \mathrm{Bern}(\mu)
\end{equation*}\]</span>
and model the mean <span class="math inline">\(E(Y)=\mu\)</span> as per linear regression where it is expressed as a <em>linear</em> combination of the <em>predictor</em> variables, called the <em>linear predictor</em>. In other words, denoting the linear predictor by <span class="math inline">\(\eta\)</span> we could write
<span class="math display">\[\begin{equation*}
\eta = \beta_0 + \beta_1 x_{1} + \ldots + \beta_p x_{p}
\end{equation*}\]</span>
and take <span class="math inline">\(\mu = \eta\)</span>. However, this still isn’t right because the linear predictor <span class="math inline">\(\eta\)</span> can take any real value and the mean <span class="math inline">\(\mu\)</span> of a Bernoulli random variable is a probability which must lie between 0 and 1. So as a final modification, instead of modelling <span class="math inline">\(\mu = \eta\)</span>, we instead take
<span class="math display">\[\begin{equation*}
g(\mu) = \eta
\end{equation*}\]</span>
where <span class="math inline">\(g\)</span> is a function – called a <em>link function</em> – which maps <span class="math inline">\(\mu\)</span> (which lies between 0 and 1) to a value in the space of values that the linear predictor can take (i.e. all real numbers). If we take <span class="math inline">\(g(\mu) = \log \{ \mu / (1 - \mu) \}\)</span>, we get <em>logistic regression</em>. If we take <span class="math inline">\(g(\mu) = \Phi^{-1}(\mu)\)</span>, where <span class="math inline">\(\Phi(\cdot)\)</span> is the standard normal cumulative distribution function (<code>pnorm</code> in R) and <span class="math inline">\(\Phi^{-1}(\cdot)\)</span> its inverse (<code>qnorm</code> in R), we get <em>probit regression</em>. There are then simple generalisations, e.g. multinomial logistic regression, which extend this approach to the case where the categorical variable has <span class="math inline">\(K&gt;2\)</span> possible values. We consider logistic regression in more detail in Section <a href="#sec:logisticreg">13</a>.</p>
<p>Regression models like these are examples of <strong>generalised linear models</strong> which generalise the linear model to allow the <em>response variable</em> to have a non-normal distribution. All we need to do to get another model from this class is choose a different distribution for <span class="math inline">\(Y | X_1 = x_1, \ldots, X_p = x_p\)</span> and then choose an appropriate link function to connect the mean of the distribution to the linear predictor. These types of model will be explored further in the module “Multilevel Modelling”.</p>
</div>
<div id="discriminant-based-approaches" class="section level2" number="12.5">
<h2><span class="header-section-number">12.5</span> Discriminant-Based Approaches</h2>
<p>Although this regression approach is conceptually appealing, there is another, less direct, way of defining the conditional probabilities <span class="math inline">\(\Pr(Y = k | X_1 = x_1, \ldots, X_p = x_p)\)</span> for each value <span class="math inline">\(k = 1,\ldots,K\)</span>. This involves formulating the model the “other way around” and defining conditional distributions for <span class="math inline">\((X_1 = x_1, \ldots, X_p = x_p | Y = k)\)</span> for each <span class="math inline">\(k=1,\ldots,K\)</span>. This is easiest to understand in the context of an example. Suppose that <span class="math inline">\(Y\)</span> represents whether a patient has a particular disease whilst <span class="math inline">\(X_1, \ldots, X_p\)</span> represents a set of clinical measurements, such as blood pressure and body-mass-index, taken on that patient. Then we would need to construct a probability model for these clinical measurements conditional on a patient having the disease and another conditional on a patient not having the disease. In our model where we are conditioning on the patient having the disease, high values of blood pressure and body-mass-index might be quite likely. In our model where we are conditioning on the patient not having the disease, high values for these variables might be quite unlikely.</p>
<p>Now, for the purposes of illustration, let us suppose that the random variables in <span class="math inline">\(X_1, \ldots, X_p\)</span> are all discrete-valued. Taking a discriminant-based approach, we would define a conditional probability mass function
<span class="math display">\[\begin{equation*}
\Pr(X_1 = x_1, X_2 = x_2, \ldots, X_p = x_p | Y = k)
\end{equation*}\]</span>
for each <span class="math inline">\(k=1,\ldots,K\)</span>, which would tell us the probability of observing <span class="math inline">\(X_i\)</span> taking the value <span class="math inline">\(x_i\)</span> for <span class="math inline">\(i=1,\ldots,p\)</span> if we know <span class="math inline">\(Y=k\)</span>. Recall from the module “IMDS” that Bayes Theorem gives us a rule for “turning around” conditional probabilities. Applying it here gives
<span class="math display">\[\begin{equation}
\Pr(Y = k | X_1 = x_1, \ldots, X_p = x_p) = \frac{\Pr(X_1 = x_1, \ldots, X_p = x_p | Y = k) \Pr(Y = k)}{\sum_{\ell=1}^K \Pr(X_1 = x_1, \ldots, X_p = x_p | Y = \ell) \Pr(Y = \ell)}
\end{equation}\]</span>
for each value <span class="math inline">\(k = 1,\ldots,K\)</span>. This is a type of <strong>Bayes classification</strong> and, in this context, <span class="math inline">\(\Pr(Y = k | X_1 = x_1, \ldots, X_p = x_p)\)</span> is the posterior probability that the categorical variable takes the value <span class="math inline">\(k\)</span> or, equivalently, that the individual / item belongs to the <span class="math inline">\(k\)</span>-th group. We denote this by <span class="math inline">\(p_k(x_1, \ldots, x_p)\)</span>. Note that to use a classification method based on this idea, we need to know the <strong>prior group membership probabilities</strong>, <span class="math inline">\(\Pr(Y = k) = \pi_k\)</span> where <span class="math inline">\(\sum_{k=1}^K \pi_k = 1\)</span>. With this notation, we can express more concisely as
<span class="math display">\[\begin{equation}
p_k(x_1, \ldots, x_p) = \frac{\Pr(X_1 = x_1, \ldots, X_p = x_p | Y = k) \pi_k}{\sum_{\ell=1}^K \Pr(X_1 = x_1, \ldots, X_p = x_p | Y = k) \pi_{\ell}}.
\end{equation}\]</span></p>
<p>If the random variables in <span class="math inline">\(X_1, \ldots, X_p\)</span> are continuous-valued, rather than discrete-valued, there is an equivalent rule which gives us the posterior probabilities that we need, namely
<span class="math display">\[\begin{equation}
p_k(x_1,\ldots,x_p) = \frac{f_k(x_1,\ldots,x_p) \pi_k}{\sum_{\ell=1}^K f_{\ell}(x_1,\ldots,x_p) \pi_{\ell}}
\end{equation}\]</span>
for each value <span class="math inline">\(k = 1,\ldots,K\)</span>. Recall that when we have continuous random variables, we cannot use probability <em>mass</em> functions to specify their distribution because the probability that a continuous random variable takes any particular value is zero. Instead, we use probability <em>density</em> functions which indicate, in relative terms, which values are more or less likely. So to get we have simply replaced each conditional probability <em>mass</em> function <span class="math inline">\(\Pr(X_1 = x_1, \ldots, X_p = x_p | Y = k)\)</span> in with a conditional probability <em>density</em> function denoted by <span class="math inline">\(f_k(x_1,\ldots,x_p)\)</span>.</p>
<p>Classification based on the conditional distribution of predictor variables given a categorical response, <span class="math inline">\((X_1 = x_1, \ldots, X_p = x_p | Y = k)\)</span>, is normally referred to as <strong>discriminant function analysis</strong>. In Section <a href="#sec:discranal">14</a> we consider a special case in which the distribution of <span class="math inline">\((X_1, \ldots, X_p)\)</span>, conditional on <span class="math inline">\(Y = k\)</span>, is assumed to be a multivariate generalisation of the normal distribution called the <strong>multivariate normal distribution</strong>. The resulting methods are called <strong>linear discriminant analysis</strong> or <strong>quadratic discriminant analysis</strong> depending on assumptions we make about the variance parameter of the multivariate normal distribution.</p>
</div>
</div>
<div id="sec:logisticreg" class="section level1" number="13">
<h1><span class="header-section-number">13</span> Logistic Regression</h1>
<div id="the-model" class="section level2" number="13.1">
<h2><span class="header-section-number">13.1</span> The Model</h2>
<p>Suppose that we have just two groups, <span class="math inline">\(K=2\)</span>, and that our categorical variable <span class="math inline">\(Y\)</span> is labelled so that its two possible values are <span class="math inline">\(Y=0\)</span> and <span class="math inline">\(Y=1\)</span>. Logistic regression was introduced in Section <a href="#subsec:regbased">12.4</a> as a means of directly specifying the probability <span class="math inline">\(\Pr(Y = 1 | X_1 = x_1, \ldots, X_p = x_p)\)</span> (and hence, indirectly, <span class="math inline">\(\Pr(Y = 0 | X_1 = x_1, \ldots, X_p = x_p) = 1 - \Pr(Y = 1 | X_1 = x_1, \ldots, X_p = x_p)\)</span>) using regression techniques. Specifically, we construct the linear predictor
<span class="math display">\[\begin{equation*}
\eta = \beta_0 + \beta_1 x_{1} + \ldots + \beta_p x_{p},
\end{equation*}\]</span>
where <span class="math inline">\(\eta\)</span> can take any real value, and assume that
<span class="math display">\[\begin{equation*}
Y | X_1 = x_1, \ldots, X_p = x_p \sim \mathrm{Bern}(\mu),
\end{equation*}\]</span>
where <span class="math inline">\(\mu = \Pr(Y = 1 | X_1 = x_1, \ldots, X_p = x_p)\)</span> lies between 0 and 1. We then reconcile the difference between the values which can be taken by <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\eta\)</span> by relating the mean <span class="math inline">\(\mu\)</span> to the linear predictor <span class="math inline">\(\eta\)</span> through a link function <span class="math inline">\(g\)</span> where <span class="math inline">\(\eta = g(\mu)\)</span>. For logistic regression we use the <em>logit-link</em>
<span class="math display">\[\begin{equation*}
\eta = \log\left( \frac{\mu}{1 - \mu} \right)
\end{equation*}\]</span>
which has this name because the expression on the right-hand-side is called the <strong>logit</strong> function of <span class="math inline">\(\mu\)</span>. Using rules of algebra, it is possible to invert the above equation so that we get an expression for <span class="math inline">\(\mu\)</span> in terms of <span class="math inline">\(\eta\)</span> (instead of an expression for <span class="math inline">\(\eta\)</span> in terms of <span class="math inline">\(\mu\)</span>). This gives
<span class="math display">\[\begin{equation*}
\mu = \frac{e^{\eta}}{1 + e^{\eta}}
\end{equation*}\]</span>
in which the function on the right-hand-side is called the <strong>expit</strong> function (also known as the <strong>logistic</strong> function) of <span class="math inline">\(\eta\)</span>.</p>
In other words, we model
<span class="math display">\[\begin{equation*}
\mu = \Pr(Y = 1 | X_1 = x_1, \ldots, X_p = x_p) = \frac{\exp(\beta_0 + \beta_1 x_{1} + \ldots + \beta_p x_{p})}{1 + \exp(\beta_0 + \beta_1 x_{1} + \ldots + \beta_p x_{p})}.
\end{equation*}\]</span>
For example, if <span class="math inline">\(p=1\)</span> so that we have a single predictor variable, <span class="math inline">\(X_1 = X\)</span>, then our model for the mean reduces to
<span class="math display">\[\begin{equation*}
\mu = \Pr(Y = 1 | X = x) = \frac{e^{\beta_0 + \beta_1 x}}{1 + e^{\beta_0 + \beta_1 x}}.
\end{equation*}\]</span>
For this simple case, a plot of <span class="math inline">\(\mu\)</span> against <span class="math inline">\(x\)</span> is shown in Figure <a href="#fig:logisticregression">13.1</a> for various values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:logisticregression"></span>
<img src="Weeks6to9new_files/figure-html/logisticregression-1.png" alt="A plot of the logistic function $e^{\beta_0 + \beta_1 x} / (1 + e^{\beta_0 + \beta_1 x})$ against $x$ for various values of $\beta_0$ and $\beta_1$." width="768" />
<p class="caption">
Figure 13.1: A plot of the logistic function <span class="math inline">\(e^{\beta_0 + \beta_1 x} / (1 + e^{\beta_0 + \beta_1 x})\)</span> against <span class="math inline">\(x\)</span> for various values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.
</p>
</div>
<p>Notice that for each set of regression coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, we get an <span class="math inline">\(S\)</span>-shaped curve which maps a real number <span class="math inline">\(x\)</span> to a probability <span class="math inline">\(\mu\)</span> that lies between 0 and 1. It can be shown that</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> affects the intercept, i.e. the point at which the curve cuts through the <span class="math inline">\(y\)</span>-axis;</li>
<li><span class="math inline">\(\beta_1\)</span> affects the “direction” of the curve and how quickly it flattens out in each tail:
<ul>
<li>If <span class="math inline">\(\beta_1 &gt; 0\)</span> the curve slopes upwards from left to right; if <span class="math inline">\(\beta_1 &lt; 0\)</span> the curve slopes downwards from left to right; if <span class="math inline">\(\beta_1=0\)</span> the curve is a straight horizontal line.</li>
<li>The larger the value of <span class="math inline">\(\beta_1\)</span> (ignoring its sign), the more quickly the curve flattens out in each tail, and hence the smaller the range of values of <span class="math inline">\(x\)</span> over which the curve is notably different from 0 or 1.</li>
</ul></li>
</ul>
<p>This means that if <span class="math inline">\(\beta_1\)</span> is positive, for example, the probability <span class="math inline">\(\Pr(Y = 1 | X = x) = \mu\)</span> increases as <span class="math inline">\(x\)</span> increases and, for larger values of <span class="math inline">\(\beta_1\)</span>, it rises from 0 to 1 sharply as <span class="math inline">\(x\)</span> ascends over a very narrow range of values. When we have more than one predictor variable, i.e. <span class="math inline">\(p &gt; 1\)</span>, we assess the relationships between the probability <span class="math inline">\(\Pr(Y = 1 | X_1=x_1, \ldots, X_p=x_p) = \mu\)</span> and the predictor variables <span class="math inline">\(X_j\)</span> one-at-a-time by considering what happens when only <span class="math inline">\(X_j\)</span> varies, so that we need only consider the sign and magnitude of the corresponding <span class="math inline">\(\beta_j\)</span>.</p>
<p>Notice that although in this chapter we only consider continuous predictor variables <span class="math inline">\(X_j\)</span>, we can incorporate categorical predictor variables, interaction terms, non-linear transformations of predictors, and so on, in exactly the same way as for multiple linear regression studied previously.</p>
</div>
<div id="fitting-the-model" class="section level2" number="13.2">
<h2><span class="header-section-number">13.2</span> Fitting the Model</h2>
<p>In general, we do not know the values of the regression coefficients <span class="math inline">\(\beta_0,\beta_1,\ldots,\beta_p\)</span> in the logistic regression model. However, if we have appropriate training data <span class="math inline">\((\boldsymbol{x}_1, y_1),\ldots,(\boldsymbol{x}_n, y_n)\)</span> where <span class="math inline">\(\boldsymbol{x}_i = (x_{i1}, \ldots, x_{ip})\)</span> is the set of <span class="math inline">\(p\)</span> predictor variables for individual / item <span class="math inline">\(i\)</span>, we can use them to generate estimates <span class="math inline">\(\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p\)</span> for the regression coefficients. One method – maximum likelihood estimation – will be considered briefly below.</p>
</div>
<div id="prediction-from-the-fitted-model" class="section level2" number="13.3">
<h2><span class="header-section-number">13.3</span> Prediction from the Fitted Model</h2>
<p>Suppose we want to classify a new (partial) observation, i.e. given <span class="math inline">\((x_{\text{new},1}, \ldots, x_{\text{new},p})\)</span> we want to predict the associated value of <span class="math inline">\(Y_{\text{new}}\)</span>. Once we have estimates <span class="math inline">\(\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p\)</span> of the regression coefficients, we can base this prediction on the estimated probability
<span class="math display">\[\begin{equation*}
\hat{\Pr}(Y_{\text{new}} = 1 | X_{\text{new},1}=x_{\text{new},1}, \ldots, X_{\text{new},p}=x_{\text{new},p}) = \frac{\exp(\hat{\beta}_0 + \hat{\beta}_1 x_{\text{new},1} + \ldots + \hat{\beta}_p x_{\text{new},p})}{1 + \exp(\hat{\beta}_0 + \hat{\beta}_1 x_{\text{new},1} + \ldots + \hat{\beta}_p x_{\text{new},p})}
\end{equation*}\]</span>
by constructing a <strong>classification rule</strong> of the form:
<span class="math display" id="eq:classificationrule">\[\begin{equation}
Y_{\text{new}} =
\begin{cases}
1, \quad \text{if $\hat{\Pr}(Y_{\text{new}} = 1 | X_{\text{new},1}=x_{\text{new},1}, \ldots, X_{\text{new},p}=x_{\text{new},p}) &gt; \alpha$,}\\
0, \quad \text{otherwise}.
\end{cases}\tag{13.1}
\end{equation}\]</span>
Most often <span class="math inline">\(\alpha = 0.5\)</span> but if the “cost” of one of the two classification errors (misclassifying a true 0 as a 1 or a true 1 as a 0) is higher than the other, then a different value for <span class="math inline">\(\alpha\)</span> may be chosen to adjust the classification decision in an appropriate way.</p>
</div>
<div id="estimation-by-maximum-likelihood" class="section level2" number="13.4">
<h2><span class="header-section-number">13.4</span> Estimation by Maximum Likelihood</h2>
<p>The standard approach for producing estimates <span class="math inline">\(\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_p\)</span> of the regression coefficients in the logistic regression model is to use a widely used method in statistical inference called <strong>maximum likelihood estimation</strong>. Although we omit the mathematical details, we note that one of the advantages of this method is that maximum likelihood estimators have a number of nice theoretical properties which can be exploited to derive confidence intervals for the regression coefficients, perform hypothesis tests, and so on. In R, all this goes on under the hood when we use the <code>glm</code> function to fit the logistic regression model. When using the <code>glm</code> function, we need to set the argument <code>family="binomial"</code> to indicate that we want to fit a logistic regression model, and not some other kind of generalised linear model.</p>
<!--
## Big data considerations {#subsec:classbigdata}

In a big data setting when $p$ is large relative to $n$, the estimators of the regression coefficients will have large variance, leading to poor predictive performance. Fortunately, all of the approaches discussed in Chapter \ref{cha:regression} for linear regression have analogues in the context of logistic regression. For example, ridge regression and the LASSO work in exactly the same way, adding a penalty of the same form to the loss function which, in this case, is the negative of the log-likelihood function. These models can be fitted using the `glmnet` package in R, specifying `family="binomial"`. Best subset selection can be performed using the `bestglm` or `glmulti` packages. The latter also implements a stepwise search algorithm for cases when best subset selection would not be computationally feasible. We will explore some of these methods further in practical classes.
-->
</div>
<div id="extension-to-k-2" class="section level2" number="13.5">
<h2><span class="header-section-number">13.5</span> Extension to <span class="math inline">\(K &gt; 2\)</span></h2>
<p>When the categorical variable has <span class="math inline">\(K&gt;2\)</span> possible values, it is possible to derive extensions of logistic regression, such as multinomial logistic regression. However, these methods tend not to be used very often because discriminant analysis is much more popular for multiple-class classification. Although they will not be considered further in this module, note that there are R packages available for performing multinomial logistic regression, such as <code>nnet</code> and <code>mlogit</code>.</p>
</div>
<div id="subsec:chapmandata" class="section level2" number="13.6">
<h2><span class="header-section-number">13.6</span> Example: Chapman Data</h2>
<p>The <em>Chapman data</em> arose from a study on heart disease by Dr. John M. Chapman in the mid-twentieth century. The data were taken from the Los Angeles Heart Study and comprise measurements from <span class="math inline">\(n=200\)</span> men on <span class="math inline">\(p=7\)</span> variables. One of these variables is binary, indicating whether or not the patient experienced a coronary incident in the preceding 10 years. There are also six predictor variables, which can be treated as continuous, namely the patient’s age, height and weight and measurements of their cholesterol, systolic and diastolic blood pressure. The idea here is to help in identifying and describing relationships between the predictor variables and incidence of a heart attack.</p>
<p>The data are available from the <code>durhamSLR</code> package in the <code>chapman</code> data set. If you haven’t already done so, please follow the instructions in the Labs for Week 3 to install the <code>durhamSLR</code> package. Once the package is installed, it can be loaded in the usual way, and so we load and inspect the data as follows:</p>
<pre class="r"><code>## Load package:
library(durhamSLR)
## Load data:
data(chapman)
## Check size:
dim(chapman)</code></pre>
<pre><code>## [1] 200   7</code></pre>
<pre class="r"><code>## Print first 3 rows:
head(chapman, 3)</code></pre>
<pre><code>##   age highbp lowbp chol height weight y
## 1  44    124    80  254     70    190 0
## 2  35    110    70  240     73    216 0
## 3  41    114    80  279     68    178 0</code></pre>
<pre class="r"><code>## Tabulate the categorical variable:
table(chapman$y)</code></pre>
<pre><code>## 
##   0   1 
## 174  26</code></pre>
<p>The coding <span class="math inline">\(y=0\)</span> indicates that a coronary incident was not experienced and conversely for <span class="math inline">\(y=1\)</span>. So we see that 26 of the 200 patients experienced a coronary incident.</p>
<p>We can get a feel for the data by producing a pairs plot of the predictor variables, using colour to distinguish between the two groups of patients:</p>
<pre class="r"><code>pairs(chapman[,1:6], col=chapman[,7]+1)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chapmanpairs"></span>
<img src="Weeks6to9new_files/figure-html/chapmanpairs-1.png" alt="Scatterplot matrix for the Chapman data. Patients who did / did not experience a coronary incident appear in red / black. " width="480" />
<p class="caption">
Figure 13.2: Scatterplot matrix for the Chapman data. Patients who did / did not experience a coronary incident appear in red / black.
</p>
</div>
<p>This generates the plot in Figure <a href="#fig:chapmanpairs">13.2</a>. There is arguably some evidence of a higher incidence of heart attacks amongst older patients but, in general, it is difficult to spot relationships with the response. However, it is immediately clear that the two sets of blood pressure measurements are highly correlated and so any regression model is unlikely to require both.</p>
<p>We can fit a logistic regression model using the <code>glm</code> function. The syntax is almost identical to that of the <code>lm</code> function for multiple linear regression, except we need to remember to set the argument <code>family="binomial"</code> so that R knows to perform logistic regression.</p>
<pre class="r"><code>## Fit logistic regression model:
lr_fit = glm(y ~ ., data=chapman, family=&quot;binomial&quot;)
## Summarise the model fit:
summary(lr_fit)</code></pre>
<pre><code>## 
## Call:
## glm(formula = y ~ ., family = &quot;binomial&quot;, data = chapman)
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)  
## (Intercept) -4.517321   7.481215  -0.604   0.5460  
## age          0.045900   0.023535   1.950   0.0511 .
## highbp       0.006856   0.020198   0.339   0.7343  
## lowbp       -0.006937   0.038352  -0.181   0.8565  
## chol         0.006306   0.003632   1.736   0.0825 .
## height      -0.074002   0.106214  -0.697   0.4860  
## weight       0.020142   0.009871   2.041   0.0413 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 154.55  on 199  degrees of freedom
## Residual deviance: 134.85  on 193  degrees of freedom
## AIC: 148.85
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>The maximum likelihood estimates of the regression coefficients are therefore
<span class="math display">\[\begin{alignat*}{4}
\hat{\beta}_0&amp;=-4.517,&amp;\quad \hat{\beta}_1&amp;=0.046,&amp;\quad \hat{\beta}_2&amp;=0.007,&amp;\quad \hat{\beta}_3&amp;=-0.007,\\
\hat{\beta}_4&amp;=0.006,&amp;\quad \hat{\beta}_5&amp;=-0.074,&amp;\quad \hat{\beta}_6&amp;=0.02.&amp;&amp;
\end{alignat*}\]</span>
Consider, for example, the coefficient for the <code>age</code> variable. Because it is positive, this indicates that older patients are generally more likely to experience a coronary incident.</p>
<p>If we examine the table produced by the <code>summary</code> function we see that a number of the variables have very large <span class="math inline">\(p\)</span>-values meaning that, individually, they contribute very little to a model which contains all the other predictors. Like in linear regression, inclusion of more predictors than are necessary can inflate the variance of the parameter estimators leading to a deterioration in predictive performance. There are classical methods that can be used to eliminate predictors based on techniques such as analysis of deviance, which we will not consider. Alternatively, we can appeal to techniques like best subset selection that we studied in the previous chapter in the context of linear regression. We will return to this idea in the computer labs for this week.</p>
<p><span class="math display">\[\begin{align*}
\hat{\Pr}(Y_{\text{new}} = 1 | X_{\text{new},1}=x_{\text{new},1}, \ldots, X_{\text{new},p}=x_{\text{new},p}) &amp;= \frac{\exp(-4.517 + 0.046 \times 51 + \ldots + 0.02 \times 150)}{1 + \exp(-4.517 + 0.046 \times 51 + \ldots + 0.02 \times 150)}\\
&amp;= 0.108.
\end{align*}\]</span></p>
<p>We can perform this prediction in R as follows</p>
<pre class="r"><code>## Set up data frame of predictor variables
x1 = data.frame(age=51, highbp=146, lowbp=72, chol=320, height=74, weight=150)
## Perform prediction
(p1 = predict(lr_fit, x1, type=&quot;response&quot;))</code></pre>
<pre><code>##         1 
## 0.1079599</code></pre>
<p>Notice that when using the <code>predict</code> function, we need to set the argument <code>type="response"</code> to get the predicted probability. The default is a prediction of the linear predictor <span class="math inline">\(\eta\)</span>.</p>
<p>As the predicted probability is less than 0.5, if we apply the “standard” classification rule <a href="#eq:classificationrule">(13.1)</a>, we would classify <span class="math inline">\(Y_{\text{new}} = 0\)</span>:</p>
<pre class="r"><code>(y = as.numeric(ifelse(p1 &gt; 0.5, 1, 0)))</code></pre>
<pre><code>## [1] 0</code></pre>
</div>
</div>
<div id="sec:discranal" class="section level1" number="14">
<h1><span class="header-section-number">14</span> Discriminant Analysis</h1>
<div id="discriminant-functions" class="section level2" number="14.1">
<h2><span class="header-section-number">14.1</span> Discriminant Functions</h2>
<p>Discriminant analysis requires a <strong>discriminant function</strong> for each group, <span class="math inline">\(k=1,\ldots,K\)</span>, which we denote by <span class="math inline">\(Q_k\)</span>. Consider a vector of <span class="math inline">\(p\)</span> predictor variables, which it is convenient to write as
<span class="math display">\[\begin{equation*}
\boldsymbol{x} =
\begin{pmatrix}
x_1\\
x_2\\
\vdots\\
x_p
\end{pmatrix} = (x_1, x_2, \ldots, x_p)^T.
\end{equation*}\]</span>
When we input <span class="math inline">\(\boldsymbol{x}\)</span> into each of the discriminant functions, the output is a real number, so we would get <span class="math inline">\(K\)</span> real numbers <span class="math inline">\(Q_1(\boldsymbol{x}), \ldots, Q_K(\boldsymbol{x})\)</span>. For the different values <span class="math inline">\(\boldsymbol{x} = (x_1, \ldots, x_p)^T\)</span> that <span class="math inline">\(\boldsymbol{X} = (X_1, \ldots, X_p)^T\)</span> can take, one of the <span class="math inline">\(K\)</span> real numbers, say <span class="math inline">\(Q_k(\boldsymbol{x})\)</span>, will be larger than the others. We use this idea to divide up the whole space of values that <span class="math inline">\(\boldsymbol{X}\)</span> can take into <span class="math inline">\(K\)</span> non-overlapping parts, <span class="math inline">\(R_1,R_2,\ldots,R_K\)</span>, (called <strong>allocation regions</strong>) according to the rule
<span class="math display">\[\begin{equation*}
\text{if } Q_k(\boldsymbol{x}) &gt; Q_{\ell}(\boldsymbol{x}), \text{ for all $\ell \ne k$, then $\boldsymbol{x}$ lies in $R_k$}.
\end{equation*}\]</span>
If <span class="math inline">\(\boldsymbol{x}\)</span> lies in <span class="math inline">\(R_k\)</span> then we assign <span class="math inline">\(Y = k\)</span>.</p>
<p>When trained on a particular data set, different discriminant functions will lead to different allocation regions and the allocation regions created by some methods are likely to be more effective than others for correctly predicting the group in which a new observation lies.</p>
<p>The so-called <strong>Bayes classifier</strong> is based on and simply assigns an observation <span class="math inline">\(\boldsymbol{x}\)</span> to the class <span class="math inline">\(k\)</span> for which the posterior probability <span class="math inline">\(p_k(\boldsymbol{x}) = \Pr(Y = k | \boldsymbol{X} = \boldsymbol{x}) = \Pr(Y = k | X_1 = x_1, \ldots, X_p = x_p)\)</span> is largest. It can be shown that this is equivalent to assigning <span class="math inline">\(\boldsymbol{x}\)</span> to class <span class="math inline">\(k\)</span> if and only if
<span class="math display">\[\begin{align*}
f_k(\boldsymbol{x}) \pi_k &gt; f_{\ell}(\boldsymbol{x}) \pi_{\ell}
\end{align*}\]</span>
for all <span class="math inline">\(\ell \ne k\)</span>. Therefore the corresponding discriminant functions are
<span class="math display">\[\begin{equation*}
Q_k(\boldsymbol{x}) = f_k(\boldsymbol{x}) \pi_k, \quad k=1,\ldots,K.
\end{equation*}\]</span>
In the special case when all the prior group probabilities are equal, i.e. <span class="math inline">\(\pi_1 = \pi_2 = \ldots = \pi_K = 1/K\)</span>, the discriminant functions simplify to
<span class="math display">\[\begin{equation*}
Q_k(\boldsymbol{x}) = f_k(\boldsymbol{x}), \quad k=1,\ldots,K.
\end{equation*}\]</span>
In this case the resulting rule for classification is called the <strong>maximum likelihood discriminant rule</strong> because an observation is assigned to the group with the maximum likelihood.</p>
</div>
<div id="the-multivariate-normal-distribution-a-concise-summary" class="section level2" number="14.2">
<h2><span class="header-section-number">14.2</span> The Multivariate Normal Distribution: A Concise Summary</h2>
<p>A <strong>random p-vector</strong> <span class="math inline">\(\boldsymbol{X} = (X_1,\ldots,X_p)^T\)</span> is a <span class="math inline">\(p\)</span>-dimensional vector whose elements, <span class="math inline">\(X_1,\ldots,X_p\)</span>, are all random variables. It is called a continuous random vector if all the <span class="math inline">\(X_j\)</span> are continous and a discrete random vector if all the <span class="math inline">\(X_j\)</span> are discrete. The normal distribution is a model for a continuous random variable <span class="math inline">\(X\)</span>. The multivariate normal distribution provides a generalisation for a continuous random vector <span class="math inline">\(\boldsymbol{X}\)</span> with <span class="math inline">\(p&gt;1\)</span>. Here we provide a very concise summary.</p>
Recall from the first half of this module that if a random variable <span class="math inline">\(X\)</span> has a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, written <span class="math inline">\(X \sim \mathrm{N}(\mu, \sigma^2)\)</span>, then its probability density function is given by
<span class="math display">\[\begin{equation*}
f(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp \left\{ -\frac{1}{2 \sigma^2}(x-\mu)^2 \right\}.
\end{equation*}\]</span>
For example, if <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>, a plot of the probability density function is shown in the left-hand image of Figure . Recall that the probability of <span class="math inline">\(X\)</span> taking a value between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is just the area under the curve between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> as indicated in the right-hand plot of Figure . Therefore <span class="math inline">\(X\)</span> is more likely to take values in regions where the density function is large.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:univariatenormal"></span>
<img src="Weeks6to9new_files/figure-html/univariatenormal-1.png" alt="Left: probability density function for the (univariate) normal distributions when $\mu=0$ and $\sigma^2=1$. Right: the area of the shaded region is $\Pr(a \le X \le b)$." width="672" />
<p class="caption">
Figure 14.1: Left: probability density function for the (univariate) normal distributions when <span class="math inline">\(\mu=0\)</span> and <span class="math inline">\(\sigma^2=1\)</span>. Right: the area of the shaded region is <span class="math inline">\(\Pr(a \le X \le b)\)</span>.
</p>
</div>
<p>If <span class="math inline">\(\boldsymbol{X} = ( X_1, X_2, \ldots, X_p )^T\)</span> is a random <span class="math inline">\(p\)</span>-vector, it does not have a single mean and a single variance. Instead, there is a mean <span class="math inline">\(\mu_j\)</span> for each random variable <span class="math inline">\(X_j\)</span>. These are collected into a length-<span class="math inline">\(p\)</span> column vector <span class="math inline">\(\boldsymbol{\mu} = (\mu_1, \mu_2, \ldots, \mu_p)^T\)</span>. Similarly, each <span class="math inline">\(X_j\)</span> has its own variance <span class="math inline">\(\sigma_{jj}\)</span> and every pair of random variables <span class="math inline">\(X_j\)</span> and <span class="math inline">\(X_k\)</span> has a covariance <span class="math inline">\(\sigma_{jk}\)</span> which provides information about the relationship between <span class="math inline">\(X_j\)</span> and <span class="math inline">\(X_k\)</span>, e.g. its sign indicates whether there is a positive or negative correlation. These variances and covariances are collected into a <span class="math inline">\(p \times p\)</span> covariance matrix
<span class="math display">\[\begin{equation*}
\Sigma =
\begin{pmatrix}
\sigma_{11} &amp;\sigma_{12} &amp;\cdots &amp;\sigma_{1p}\\
\sigma_{21} &amp;\sigma_{22} &amp;\cdots &amp;\sigma_{2p}\\
\vdots &amp;\vdots &amp;\ddots &amp;\vdots\\
\sigma_{p1} &amp;\sigma_{p2} &amp;\cdots &amp;\sigma_{pp}\\
\end{pmatrix}
\end{equation*}\]</span>
which is symmetric (meaning <span class="math inline">\(\sigma_{ij} = \sigma_{ji}\)</span>) and positive definite (the matrix analogue of “positive”).</p>
<p>Now, if a random <span class="math inline">\(p\)</span>-vector <span class="math inline">\(\boldsymbol{X} = ( X_1, X_2, \ldots, X_p )^T\)</span> has a multivariate normal distribution with mean vector <span class="math inline">\(\boldsymbol{\mu}\)</span> and covariance matrix <span class="math inline">\(\Sigma\)</span>, written <span class="math inline">\(\boldsymbol{X} \sim \mathrm{N}_p(\boldsymbol{\mu}, \Sigma)\)</span>, then its probability density function is given by
<span class="math display">\[\begin{equation*}
f(\boldsymbol{x}) = \frac{1}{(2 \pi)^{p/2} | \Sigma |^{1/2}} \exp \left\{ -\frac{1}{2} (\boldsymbol{x} - \boldsymbol{\mu})^T \Sigma^{-1} (\boldsymbol{x} - \boldsymbol{\mu}) \right\}.
\end{equation*}\]</span>
This expression involves matrix algebra, but the important thing to note is that for any value <span class="math inline">\(\boldsymbol{x}\)</span>, <span class="math inline">\(f(\boldsymbol{x})\)</span> is just a real number. For example, if <span class="math inline">\(p=2\)</span> we have a bivariate normal distribution and can generate a 3-dimensional plot to show the probability density function. For two sets of values for the parameters, <span class="math inline">\(\boldsymbol{\mu}\)</span> and <span class="math inline">\(\Sigma\)</span>, density functions are displayed in Figure . Just like in the univariate case, the probability that <span class="math inline">\(X_1\)</span> lies between <span class="math inline">\(a_1\)</span> and <span class="math inline">\(b_1\)</span> and that <span class="math inline">\(X_2\)</span> lies between <span class="math inline">\(a_2\)</span> and <span class="math inline">\(b_2\)</span> is just the volume under the surface over the rectangular region with corners at <span class="math inline">\((a_1, a_2)\)</span>, <span class="math inline">\((a_1, b_2)\)</span>, <span class="math inline">\((b_1, b_2)\)</span> and <span class="math inline">\((b_1, a_2)\)</span>. Again <span class="math inline">\(\boldsymbol{X}=(X_1, X_2)^T\)</span> is more likely to take values in regions where the density function is large.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bivariatenormal"></span>
<img src="Weeks6to9new_files/figure-html/bivariatenormal-1.png" alt="Probability density functions for two bivariate normal distributions. Left: $X_1$ and $X_2$ are uncorrelated; right: the correlation between $X_1$ and $X_2$ is 0.7." width="480" />
<p class="caption">
Figure 14.2: Probability density functions for two bivariate normal distributions. Left: <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are uncorrelated; right: the correlation between <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is 0.7.
</p>
</div>
</div>
<div id="subsec:LDA" class="section level2" number="14.3">
<h2><span class="header-section-number">14.3</span> Linear Discriminant Analysis (LDA)</h2>
<p>LDA classifiers assume that the conditional distributions for the predictor variables <span class="math inline">\(\boldsymbol{X} = ( X_1, X_2, \ldots, X_p )^T\)</span> are multivariate normal, with a group-specific mean vector and a common covariance matrix:
<span class="math display">\[\begin{equation*}
\boldsymbol{X} | Y = k \sim \mathrm{N}_p(\boldsymbol{\mu}_k, \Sigma)
\end{equation*}\]</span>
for <span class="math inline">\(k=1,\ldots,K\)</span>. The Bayes classifier for LDA therefore uses the discriminant functions
<span class="math display">\[\begin{equation*}
Q_k(\boldsymbol{x}) = f_k(\boldsymbol{x}) \pi_k = \frac{1}{(2\pi)^{p/2} |\Sigma|^{1/2}} \exp\left\{-\frac{1}{2}\left(\boldsymbol{x} - \boldsymbol{\mu}_k\right)^T \Sigma^{-1} \left(\boldsymbol{x} - \boldsymbol{\mu}_k \right)\right\} \pi_k
\end{equation*}\]</span>
for groups <span class="math inline">\(k=1,\dots,K\)</span>. We assign <span class="math inline">\(\boldsymbol{x}\)</span> to the group <span class="math inline">\(k\)</span> for which <span class="math inline">\(Q_k(\boldsymbol{x})\)</span> is largest.</p>
<p>After applying some algebraic manipulation, it can be shown that this is equivalent to using
<span class="math display" id="eq:LDA2">\[\begin{equation}
Q_k(\boldsymbol{x}) = \boldsymbol{\mu}_k^T \Sigma^{-1} \boldsymbol{x} - \frac{1}{2} \boldsymbol{\mu}_k^T \Sigma^{-1} \boldsymbol{\mu}_k + \log \pi_k, \quad \text{for $k=1,\ldots,K$}\tag{14.1}
\end{equation}\]</span>
which are <em>linear</em> in <span class="math inline">\(\boldsymbol{x}\)</span>. This is where the word <em>linear</em> in <em>linear discriminant analysis</em> comes from. For those unfamiliar with matrix algebra, this is simplest to see in the case when <span class="math inline">\(p=1\)</span> so that <span class="math inline">\(\boldsymbol{x}=x\)</span>, <span class="math inline">\(\boldsymbol{\mu}_k = \mu_k\)</span> and <span class="math inline">\(\Sigma = \sigma^2\)</span> are just real numbers. In this case <a href="#eq:LDA2">(14.1)</a> simplifies to
<span class="math display">\[\begin{equation*}
Q_k(x) = \frac{\mu_k}{\sigma^2} x - \frac{\mu_k^2}{2 \sigma^2} + \log \pi_k = a_k x + b_k,
\end{equation*}\]</span>
for <span class="math inline">\(k=1,\ldots,K\)</span>, where <span class="math inline">\(a_k = \mu_k / \sigma^2\)</span> and <span class="math inline">\(b_k = \log \pi_k - \mu_k^2 / (2 \sigma^2)\)</span> are constants. This is just the equation of a straight line with intercept and gradient that depend on the group <span class="math inline">\(k\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:LDAillustration0"></span>
<img src="LDA.png" alt="Illustrative example of allocation regions in LDA when $K=3$ and $p=1$." width="70%" />
<p class="caption">
Figure 14.3: Illustrative example of allocation regions in LDA when <span class="math inline">\(K=3\)</span> and <span class="math inline">\(p=1\)</span>.
</p>
</div>
<p>The boundaries between the allocation regions <span class="math inline">\(R_k\)</span> and <span class="math inline">\(R_{\ell}\)</span> are called the <strong>decision boundaries</strong> of the classifier. They can be calculated by finding those values of <span class="math inline">\(\boldsymbol{x}\)</span> that satisfy <span class="math inline">\(Q_k(\boldsymbol{x}) = Q_\ell(\boldsymbol{x})\)</span>. If <span class="math inline">\(p=1\)</span>, the solution is a point on the real line; if <span class="math inline">\(p=2\)</span>, it is a straight line in the <span class="math inline">\((x_1, x_2)\)</span>-plane; if <span class="math inline">\(p=3\)</span>, it is a plane (i.e. a flat surface) in 3-dimensional <span class="math inline">\((x_1, x_2, x_3)\)</span>-space; and so on. For example, if there were <span class="math inline">\(K=3\)</span> groups and <span class="math inline">\(p=1\)</span>, we might get allocation regions like those illustrated in Figure <a href="#fig:LDAillustration0">14.3</a>. If <span class="math inline">\(K=3\)</span> and <span class="math inline">\(p=2\)</span>, we might get allocation regions like those in Figure <a href="#fig:LDAillustration">14.4</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:LDAillustration"></span>
<img src="Weeks6to9new_files/figure-html/LDAillustration-1.png" alt="Illustrative example of allocation regions in LDA when $K=3$ and $p=2$." width="480" />
<p class="caption">
Figure 14.4: Illustrative example of allocation regions in LDA when <span class="math inline">\(K=3\)</span> and <span class="math inline">\(p=2\)</span>.
</p>
</div>
</div>
<div id="quadratic-discriminant-analysis-qda" class="section level2" number="14.4">
<h2><span class="header-section-number">14.4</span> Quadratic Discriminant Analysis (QDA)</h2>
<p>As discussed in the previous section, in LDA we assume that the conditional distributions for the predictor variables <span class="math inline">\(\boldsymbol{X}\)</span> are multivariate normal, with a group-specific mean vector and a common covariance matrix. Quadratic discriminant analysis (QDA) is similar except the different groups are allowed to have different covariance matrices. In other words we assume that
<span class="math display">\[\begin{equation*}
\boldsymbol{X} | Y = k \sim \mathrm{N}_p(\boldsymbol{\mu}_k, \Sigma_k)
\end{equation*}\]</span>
for <span class="math inline">\(k=1,\ldots,K\)</span>. The Bayes classifier for QDA therefore uses the discriminant functions
<span class="math display">\[\begin{equation*}
Q_k(\boldsymbol{x}) = f_k(\boldsymbol{x}) \pi_k = \frac{1}{(2\pi)^{p/2} |\Sigma_k|^{1/2}} \exp\left\{-\frac{1}{2}\left(\boldsymbol{x} - \boldsymbol{\mu}_k\right)^T \Sigma_k^{-1} \left(\boldsymbol{x} - \boldsymbol{\mu}_k \right)\right\} \pi_k
\end{equation*}\]</span>
for groups <span class="math inline">\(k=1,\dots,K\)</span>. We assign <span class="math inline">\(\boldsymbol{x}\)</span> to the group <span class="math inline">\(k\)</span> for which <span class="math inline">\(Q_k(\boldsymbol{x})\)</span> is largest.</p>
<p>Again, we can apply some algebraic manipulation, to show that this is equivalent to using
<span class="math display" id="eq:QDA2">\[\begin{equation}
Q_k(\boldsymbol{x}) = - \frac{1}{2} \boldsymbol{x}^T \Sigma_k^{-1} \boldsymbol{x} + \boldsymbol{\mu}_k^T \Sigma_k^{-1} \boldsymbol{x} - \frac{1}{2} \boldsymbol{\mu}_k^T \Sigma_k^{-1} \boldsymbol{\mu}_k - \frac{1}{2} \log |\Sigma_k| + \log \pi_k, \quad k=1,\ldots,K\tag{14.2}
\end{equation}\]</span>
which are <em>quadratic</em> in <span class="math inline">\(\boldsymbol{x}\)</span>. This is where the word <em>quadratic</em> in <em>quadratic discriminant analysis</em> comes from. This is simplest to see in the case when <span class="math inline">\(p=1\)</span> so that <span class="math inline">\(\boldsymbol{x}=x\)</span>, <span class="math inline">\(\boldsymbol{\mu}_k = \mu_k\)</span> and <span class="math inline">\(\Sigma_k = \sigma_k^2\)</span> are just real numbers. In this case <a href="#eq:QDA2">(14.2)</a> simplifies to
<span class="math display">\[\begin{align*}
Q_k(x) &amp;= - \frac{1}{2 \sigma_k^2} x^2 + \frac{\mu_k}{\sigma_k^2} x - \frac{\mu_k^2}{2 \sigma_k^2} - \frac{1}{2} \log \sigma_k^2 + \log \pi_k\\
       &amp;= a_k x^2 + b_k x + c_k,
\end{align*}\]</span>
for <span class="math inline">\(k=1,\ldots,K\)</span>, where <span class="math inline">\(a_k = -1 / (2 \sigma_k^2)\)</span>, <span class="math inline">\(b_k= \mu_k / \sigma_k^2\)</span> and <span class="math inline">\(c_k = \log \pi_k - \mu_k^2 / (2 \sigma_k^2) - (\log \sigma_k^2) / 2\)</span> are constants. This is just the equation of a quadratic curve.</p>
<p>The decision boundaries between the allocation regions <span class="math inline">\(R_k\)</span> and <span class="math inline">\(R_{\ell}\)</span> are, again, calculated by finding those values of <span class="math inline">\(\boldsymbol{x}\)</span> that satisfy <span class="math inline">\(Q_k(\boldsymbol{x}) = Q_\ell(\boldsymbol{x})\)</span>. For example, if we had <span class="math inline">\(K=2\)</span> groups and <span class="math inline">\(p=2\)</span>, we might get something which looks like the plot in Figure <a href="#fig:QDAillustration">14.5</a> where we see a quadratic boundary between the two allocation regions.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:QDAillustration"></span>
<img src="Weeks6to9new_files/figure-html/QDAillustration-1.png" alt="Illustrative example of allocation regions in QDA when $K=2$ and $p=2$." width="480" />
<p class="caption">
Figure 14.5: Illustrative example of allocation regions in QDA when <span class="math inline">\(K=2\)</span> and <span class="math inline">\(p=2\)</span>.
</p>
</div>
<!--
%SHOW DIFFERENCE IN ASSUMING EQUALITY OF GROUP MEANS WHEN ONE GROUP VERY UNDER-REPRESENTED
-->
</div>
<div id="fitting-the-model-1" class="section level2" number="14.5">
<h2><span class="header-section-number">14.5</span> Fitting the Model</h2>
<p>In practice, we generally do not know the group mean vectors <span class="math inline">\(\boldsymbol{\mu}_k\)</span> or the common covariance matrix <span class="math inline">\(\Sigma\)</span> in the conditional distributions
<span class="math display">\[\begin{equation*}
\boldsymbol{X} | Y = k \sim \mathrm{N}_p(\boldsymbol{\mu}_k, \Sigma)
\end{equation*}\]</span>
for LDA, or the group mean vectors <span class="math inline">\(\boldsymbol{\mu}_k\)</span> and group covariance matrices <span class="math inline">\(\Sigma_k\)</span> in the conditional distributions
<span class="math display">\[\begin{equation*}
\boldsymbol{X} | Y = k \sim \mathrm{N}_p(\boldsymbol{\mu}_k, \Sigma_k)
\end{equation*}\]</span>
for QDA. Similarly, we often do not know in advance the prior group membership probabilities <span class="math inline">\(\pi_k\)</span> in the Bayes classifiers for LDA or QDA. However, these parameters can often be estimated from data that has already been classified. Recall that we refer to this as a <em>training set</em>.</p>
<p>The details of this estimation procedure for the mean vectors and covariance matrices are beyond the scope of this course. For the group membership probabilities <span class="math inline">\(\pi_k\)</span>, one method of estimation is to use the sample proportions which we can estimate as follows. Suppose we have training data, <span class="math inline">\((\boldsymbol{x}_1, y_1),\ldots,(\boldsymbol{x}_n, y_n)\)</span> where <span class="math inline">\(\boldsymbol{x}_i\)</span> contains the observations on the <span class="math inline">\(p\)</span> predictor variables for individual / item <span class="math inline">\(i\)</span>. If <span class="math inline">\(n_k\)</span> of the observations are from group <span class="math inline">\(k\)</span> (i.e. have <span class="math inline">\(y_i=k\)</span>) then we can replace <span class="math inline">\(\pi_k\)</span> in our discriminant functions with estimates
<span class="math display">\[\begin{equation*}
\hat{\pi}_k = \frac{n_k}{n}, \quad k=1,\ldots,K.
\end{equation*}\]</span>
However, clearly this will not be appropriate if our training data do not form a representative sample of group labels. This might occur in a clinical trial, for instance, if we deliberately choose a sample of healthy patients and another sample of patients with a particular disease. In this case, the sample proportion of diseased individuals would be much higher than that in the wider population! In situations like these we must use estimates of the group membership probabilities that have been obtained by other means. For instance, in the clinical trial example, doctors may have some idea about the incidence of disease in the population as a whole.</p>
</div>
<div id="subsec:mbaadmissions" class="section level2" number="14.6">
<h2><span class="header-section-number">14.6</span> Example: MBA Admissions Data</h2>
<p>The MBA admissions data are available from the <code>durhamSLR</code> package as the <code>admission</code> data set. The data concern applicants to the Masters of Business Administration (MBA) programme of a US business graduate school. For each of a random sample of 85 applicants, the category to which the student was assigned by admissions tutors is recorded (admit, borderline or do not admit) along with the student’s grade point average (GPA) and graduate management admission test (GMAT) score. (The GPA is an average of the student’s results earned during their undergraduate degree and can range from 0.0 to 4.0. The GMAT is a standardised exam designed to assess skills deemed important for an MBA and scores range from 200 to 800.)</p>
<p>Loading the data into R and printing the first few rows yields</p>
<pre class="r"><code>## Load data into R
data(admission)
## Print the first few rows
head(admission)</code></pre>
<pre><code>##    GPA GMAT decision
## 1 2.96  596    admit
## 2 3.14  473    admit
## 3 3.22  482    admit
## 4 3.29  527    admit
## 5 3.69  505    admit
## 6 3.46  693    admit</code></pre>
<p>Suppose we want to use these data to generate a rule for automatic classification of applicants. In this case we have three groups (<span class="math inline">\(K=3\)</span>) and <span class="math inline">\(p=2\)</span> variables (<span class="math inline">\(X_1 = \text{GPA}\)</span> and <span class="math inline">\(X_2 = \text{GMAT}\)</span>).</p>
<p>We begin by plotting the data</p>
<pre class="r"><code>plot(admission$GPA, admission$GMAT, col=as.numeric(admission$decision)+1, 
     pch=as.numeric(admission$decision), xlab=&quot;GPA&quot;, ylab=&quot;GMAT&quot;, 
     ylim=c(200, 800), xlim=c(2,4))
legend(&quot;topleft&quot;, c(&quot;admit&quot;,&quot;borderline&quot;,&quot;not admit&quot;), col=2:4, pch=1:3)</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:admissionsdata"></span>
<img src="Weeks6to9new_files/figure-html/admissionsdata-1.png" alt="Scatterplot for the MBA admissions data." width="576" />
<p class="caption">
Figure 14.6: Scatterplot for the MBA admissions data.
</p>
</div>
<p>which generates the scatterplot in Figure <a href="#fig:admissionsdata">14.6</a>. An assumption of equal variance amongst the three groups does not seem unreasonable here and so we will generate the rule for automatic classification using the Bayes classifier for LDA.</p>
<p>We can estimate the parameters in our Bayes classifier for LDA using the <code>lda</code> function from the <code>MASS</code> package in R.</p>
<pre class="r"><code>## Load the MASS package
library(MASS)
## Perform LDA
(lda_fit = lda(decision ~ ., data=admission))</code></pre>
<pre><code>## Call:
## lda(decision ~ ., data = admission)
## 
## Prior probabilities of groups:
##     admit    border  notadmit 
## 0.3647059 0.3058824 0.3294118 
## 
## Group means:
##               GPA     GMAT
## admit    3.403871 561.2258
## border   2.992692 446.2308
## notadmit 2.482500 447.0714
## 
## Coefficients of linear discriminants:
##              LD1         LD2
## GPA  5.008766354  1.87668220
## GMAT 0.008568593 -0.01445106
## 
## Proportion of trace:
##    LD1    LD2 
## 0.9673 0.0327</code></pre>
<pre class="r"><code>## Is the output a list?
is.list(lda_fit)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<pre class="r"><code>## What are its components?
names(lda_fit)</code></pre>
<pre><code>##  [1] &quot;prior&quot;   &quot;counts&quot;  &quot;means&quot;   &quot;scaling&quot; &quot;lev&quot;     &quot;svd&quot;     &quot;N&quot;      
##  [8] &quot;call&quot;    &quot;terms&quot;   &quot;xlevels&quot;</code></pre>
<p>The output <code>lda_fit</code> is a list and the estimates of the prior group membership probabilities <span class="math inline">\(\hat{\pi}_k\)</span> for <span class="math inline">\(k=1,2,3\)</span> are stored in the component called <code>prior</code>, which we can extract as follows</p>
<pre class="r"><code>## Extract prior component of lda_fit object
lda_fit$prior</code></pre>
<pre><code>##     admit    border  notadmit 
## 0.3647059 0.3058824 0.3294118</code></pre>
<pre class="r"><code>## Compare to sample proportions
table(admission$decision) / nrow(admission)</code></pre>
<pre><code>## 
##     admit    border  notadmit 
## 0.3647059 0.3058824 0.3294118</code></pre>
<p>We therefore see that by default, R will use the sample proportions to estimate the prior group membership probabilities. If this is not appropriate, we can supply our own estimates of the prior group membership probabilities by passing an argument called <code>prior</code> to the <code>lda</code> function. We shall see an example of this in Section<br />
The estimates of the group specific mean vectors <span class="math inline">\(\boldsymbol{\hat{\mu}}_k\)</span> for <span class="math inline">\(k=1,2,3\)</span> are stored in the component called <code>means</code></p>
<pre class="r"><code>## Extract the group specific means
lda_fit$means</code></pre>
<pre><code>##               GPA     GMAT
## admit    3.403871 561.2258
## border   2.992692 446.2308
## notadmit 2.482500 447.0714</code></pre>
<p>As we saw in Figure <a href="#fig:admissionsdata">14.6</a>, the means for both test scores are the largest in the <code>admit</code> group. The mean GMAT scores are very similar in the <code>border</code> and <code>notadmit</code> groups but the mean GPA score is notably smaller in the <code>notadmit</code> group than the <code>border</code> group.</p>
<p>The so-called <code>Coefficients of linear discriminants</code> are stored in the component called <code>scaling</code></p>
<pre class="r"><code>## Extract the coefficients of linear discriminants
lda_fit$scaling</code></pre>
<pre><code>##              LD1         LD2
## GPA  5.008766354  1.87668220
## GMAT 0.008568593 -0.01445106</code></pre>
<p>It is tempting to think that the coefficients of linear discriminants might be coefficients in the discriminant functions. However, this is not the case. They actually arise through a slightly different way of formulating the problem addressed in linear discriminant analysis (called Fisher’s linear discriminant) which we do not cover in this module.</p>
If the coefficients of linear discriminants do not give the coefficients in the discriminant functions, where can this information be found? Unfortunately, the discriminant functions are not returned by functions in the <code>MASS</code> package or any of the other popular R packages for implementing LDA or QDA. Nevertheless, it is possible to write an R function to calculate them. Though the details are beyond the scope of this course, for illustration in this example, the discriminant functions for groups 1 (<code>admit</code>), 2 (<code>border</code>) and 3 (<code>notadmit</code>) can be calculated as
<span class="math display">\[\begin{align*}
Q_1(\boldsymbol{x}) &amp;= -241.380 + 106.250 x_1 + 0.212 x_2\\
Q_2(\boldsymbol{x}) &amp;= -178.500 + 92.670 x_1 + 0.173 x_2\\
Q_3(\boldsymbol{x}) &amp;= -135.009 + 78.086 x_1 +  0.165 x_2
\end{align*}\]</span>
By solving <span class="math inline">\(Q_j(\boldsymbol{x}) = Q_k(\boldsymbol{x})\)</span> for all pairs <span class="math inline">\(j \ne k\)</span> we can derive the boundaries between the allocation regions and hence display the allocation regions <span class="math inline">\(R_1, R_2, R_3\)</span> on a plot. This is illustrated in Figure <a href="#fig:admissionsdataallocation">14.7</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:admissionsdataallocation"></span>
<img src="Weeks6to9new_files/figure-html/admissionsdataallocation-1.png" alt="Scatterplot for the MBA admissions data with admissions regions." width="576" />
<p class="caption">
Figure 14.7: Scatterplot for the MBA admissions data with admissions regions.
</p>
</div>
<p>For example, suppose we would like to classify a new applicant with GPA 3.5 and GMAT score 500. We see immediately from Figure <a href="#fig:admissionsdataallocation">14.7</a> that we would assign the applicant to group 1, i.e. the admit group. We can also perform this classification in R by passing the object returned by the <code>lda</code> function to the <code>predict</code> function. The second argument should be a data frame of predictor values for which classification is required, with columns labelled as per the training data</p>
<pre class="r"><code>## Perform classification for a new applicant
(ynew = predict(lda_fit, data.frame(GPA=3.5, GMAT=500)))</code></pre>
<pre><code>## $class
## [1] admit
## Levels: admit border notadmit
## 
## $posterior
##       admit     border        notadmit
## 1 0.9841421 0.01585776 0.0000001669489
## 
## $x
##        LD1       LD2
## 1 2.730657 0.8190787</code></pre>
<p>The desired group label is contained in the component called <code>class</code></p>
<pre class="r"><code>## Extract group label
ynew$class</code></pre>
<pre><code>## [1] admit
## Levels: admit border notadmit</code></pre>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
